└── model (Qwen3VLModel)
    └── visual (Qwen3VLVisionModel)
        └── patch_embed (Qwen3VLVisionPatchEmbed)
            └── proj (Conv3d)
        └── pos_embed (Embedding)
        └── rotary_pos_emb (Qwen3VLVisionRotaryEmbedding)
        └── blocks (ModuleList)
            └── 0 (Qwen3VLVisionBlock)
                └── norm1 (LayerNorm)
                └── norm2 (LayerNorm)
                └── attn (Qwen3VLVisionAttention)
                    └── qkv (Linear)
                    └── proj (Linear)
                └── mlp (Qwen3VLVisionMLP)
                    └── linear_fc1 (Linear)
                    └── linear_fc2 (Linear)
                    └── act_fn (GELUTanh)
            └── 1 (Qwen3VLVisionBlock)
                └── norm1 (LayerNorm)
                └── norm2 (LayerNorm)
                └── attn (Qwen3VLVisionAttention)
                    └── qkv (Linear)
                    └── proj (Linear)
                └── mlp (Qwen3VLVisionMLP)
                    └── linear_fc1 (Linear)
                    └── linear_fc2 (Linear)
                    └── act_fn (GELUTanh)
            └── 2 (Qwen3VLVisionBlock)
                └── norm1 (LayerNorm)
                └── norm2 (LayerNorm)
                └── attn (Qwen3VLVisionAttention)
                    └── qkv (Linear)
                    └── proj (Linear)
                └── mlp (Qwen3VLVisionMLP)
                    └── linear_fc1 (Linear)
                    └── linear_fc2 (Linear)
                    └── act_fn (GELUTanh)
            └── 3 (Qwen3VLVisionBlock)
                └── norm1 (LayerNorm)
                └── norm2 (LayerNorm)
                └── attn (Qwen3VLVisionAttention)
                    └── qkv (Linear)
                    └── proj (Linear)
                └── mlp (Qwen3VLVisionMLP)
                    └── linear_fc1 (Linear)
                    └── linear_fc2 (Linear)
                    └── act_fn (GELUTanh)
            └── 4 (Qwen3VLVisionBlock)
                └── norm1 (LayerNorm)
                └── norm2 (LayerNorm)
                └── attn (Qwen3VLVisionAttention)
                    └── qkv (Linear)
                    └── proj (Linear)
                └── mlp (Qwen3VLVisionMLP)
                    └── linear_fc1 (Linear)
                    └── linear_fc2 (Linear)
                    └── act_fn (GELUTanh)
            └── 5 (Qwen3VLVisionBlock)
                └── norm1 (LayerNorm)
                └── norm2 (LayerNorm)
                └── attn (Qwen3VLVisionAttention)
                    └── qkv (Linear)
                    └── proj (Linear)
                └── mlp (Qwen3VLVisionMLP)
                    └── linear_fc1 (Linear)
                    └── linear_fc2 (Linear)
                    └── act_fn (GELUTanh)
            └── 6 (Qwen3VLVisionBlock)
                └── norm1 (LayerNorm)
                └── norm2 (LayerNorm)
                └── attn (Qwen3VLVisionAttention)
                    └── qkv (Linear)
                    └── proj (Linear)
                └── mlp (Qwen3VLVisionMLP)
                    └── linear_fc1 (Linear)
                    └── linear_fc2 (Linear)
                    └── act_fn (GELUTanh)
            └── 7 (Qwen3VLVisionBlock)
                └── norm1 (LayerNorm)
                └── norm2 (LayerNorm)
                └── attn (Qwen3VLVisionAttention)
                    └── qkv (Linear)
                    └── proj (Linear)
                └── mlp (Qwen3VLVisionMLP)
                    └── linear_fc1 (Linear)
                    └── linear_fc2 (Linear)
                    └── act_fn (GELUTanh)
            └── 8 (Qwen3VLVisionBlock)
                └── norm1 (LayerNorm)
                └── norm2 (LayerNorm)
                └── attn (Qwen3VLVisionAttention)
                    └── qkv (Linear)
                    └── proj (Linear)
                └── mlp (Qwen3VLVisionMLP)
                    └── linear_fc1 (Linear)
                    └── linear_fc2 (Linear)
                    └── act_fn (GELUTanh)
            └── 9 (Qwen3VLVisionBlock)
                └── norm1 (LayerNorm)
                └── norm2 (LayerNorm)
                └── attn (Qwen3VLVisionAttention)
                    └── qkv (Linear)
                    └── proj (Linear)
                └── mlp (Qwen3VLVisionMLP)
                    └── linear_fc1 (Linear)
                    └── linear_fc2 (Linear)
                    └── act_fn (GELUTanh)
            └── 10 (Qwen3VLVisionBlock)
                └── norm1 (LayerNorm)
                └── norm2 (LayerNorm)
                └── attn (Qwen3VLVisionAttention)
                    └── qkv (Linear)
                    └── proj (Linear)
                └── mlp (Qwen3VLVisionMLP)
                    └── linear_fc1 (Linear)
                    └── linear_fc2 (Linear)
                    └── act_fn (GELUTanh)
            └── 11 (Qwen3VLVisionBlock)
                └── norm1 (LayerNorm)
                └── norm2 (LayerNorm)
                └── attn (Qwen3VLVisionAttention)
                    └── qkv (Linear)
                    └── proj (Linear)
                └── mlp (Qwen3VLVisionMLP)
                    └── linear_fc1 (Linear)
                    └── linear_fc2 (Linear)
                    └── act_fn (GELUTanh)
            └── 12 (Qwen3VLVisionBlock)
                └── norm1 (LayerNorm)
                └── norm2 (LayerNorm)
                └── attn (Qwen3VLVisionAttention)
                    └── qkv (Linear)
                    └── proj (Linear)
                └── mlp (Qwen3VLVisionMLP)
                    └── linear_fc1 (Linear)
                    └── linear_fc2 (Linear)
                    └── act_fn (GELUTanh)
            └── 13 (Qwen3VLVisionBlock)
                └── norm1 (LayerNorm)
                └── norm2 (LayerNorm)
                └── attn (Qwen3VLVisionAttention)
                    └── qkv (Linear)
                    └── proj (Linear)
                └── mlp (Qwen3VLVisionMLP)
                    └── linear_fc1 (Linear)
                    └── linear_fc2 (Linear)
                    └── act_fn (GELUTanh)
            └── 14 (Qwen3VLVisionBlock)
                └── norm1 (LayerNorm)
                └── norm2 (LayerNorm)
                └── attn (Qwen3VLVisionAttention)
                    └── qkv (Linear)
                    └── proj (Linear)
                └── mlp (Qwen3VLVisionMLP)
                    └── linear_fc1 (Linear)
                    └── linear_fc2 (Linear)
                    └── act_fn (GELUTanh)
            └── 15 (Qwen3VLVisionBlock)
                └── norm1 (LayerNorm)
                └── norm2 (LayerNorm)
                └── attn (Qwen3VLVisionAttention)
                    └── qkv (Linear)
                    └── proj (Linear)
                └── mlp (Qwen3VLVisionMLP)
                    └── linear_fc1 (Linear)
                    └── linear_fc2 (Linear)
                    └── act_fn (GELUTanh)
            └── 16 (Qwen3VLVisionBlock)
                └── norm1 (LayerNorm)
                └── norm2 (LayerNorm)
                └── attn (Qwen3VLVisionAttention)
                    └── qkv (Linear)
                    └── proj (Linear)
                └── mlp (Qwen3VLVisionMLP)
                    └── linear_fc1 (Linear)
                    └── linear_fc2 (Linear)
                    └── act_fn (GELUTanh)
            └── 17 (Qwen3VLVisionBlock)
                └── norm1 (LayerNorm)
                └── norm2 (LayerNorm)
                └── attn (Qwen3VLVisionAttention)
                    └── qkv (Linear)
                    └── proj (Linear)
                └── mlp (Qwen3VLVisionMLP)
                    └── linear_fc1 (Linear)
                    └── linear_fc2 (Linear)
                    └── act_fn (GELUTanh)
            └── 18 (Qwen3VLVisionBlock)
                └── norm1 (LayerNorm)
                └── norm2 (LayerNorm)
                └── attn (Qwen3VLVisionAttention)
                    └── qkv (Linear)
                    └── proj (Linear)
                └── mlp (Qwen3VLVisionMLP)
                    └── linear_fc1 (Linear)
                    └── linear_fc2 (Linear)
                    └── act_fn (GELUTanh)
            └── 19 (Qwen3VLVisionBlock)
                └── norm1 (LayerNorm)
                └── norm2 (LayerNorm)
                └── attn (Qwen3VLVisionAttention)
                    └── qkv (Linear)
                    └── proj (Linear)
                └── mlp (Qwen3VLVisionMLP)
                    └── linear_fc1 (Linear)
                    └── linear_fc2 (Linear)
                    └── act_fn (GELUTanh)
            └── 20 (Qwen3VLVisionBlock)
                └── norm1 (LayerNorm)
                └── norm2 (LayerNorm)
                └── attn (Qwen3VLVisionAttention)
                    └── qkv (Linear)
                    └── proj (Linear)
                └── mlp (Qwen3VLVisionMLP)
                    └── linear_fc1 (Linear)
                    └── linear_fc2 (Linear)
                    └── act_fn (GELUTanh)
            └── 21 (Qwen3VLVisionBlock)
                └── norm1 (LayerNorm)
                └── norm2 (LayerNorm)
                └── attn (Qwen3VLVisionAttention)
                    └── qkv (Linear)
                    └── proj (Linear)
                └── mlp (Qwen3VLVisionMLP)
                    └── linear_fc1 (Linear)
                    └── linear_fc2 (Linear)
                    └── act_fn (GELUTanh)
            └── 22 (Qwen3VLVisionBlock)
                └── norm1 (LayerNorm)
                └── norm2 (LayerNorm)
                └── attn (Qwen3VLVisionAttention)
                    └── qkv (Linear)
                    └── proj (Linear)
                └── mlp (Qwen3VLVisionMLP)
                    └── linear_fc1 (Linear)
                    └── linear_fc2 (Linear)
                    └── act_fn (GELUTanh)
            └── 23 (Qwen3VLVisionBlock)
                └── norm1 (LayerNorm)
                └── norm2 (LayerNorm)
                └── attn (Qwen3VLVisionAttention)
                    └── qkv (Linear)
                    └── proj (Linear)
                └── mlp (Qwen3VLVisionMLP)
                    └── linear_fc1 (Linear)
                    └── linear_fc2 (Linear)
                    └── act_fn (GELUTanh)
            └── 24 (Qwen3VLVisionBlock)
                └── norm1 (LayerNorm)
                └── norm2 (LayerNorm)
                └── attn (Qwen3VLVisionAttention)
                    └── qkv (Linear)
                    └── proj (Linear)
                └── mlp (Qwen3VLVisionMLP)
                    └── linear_fc1 (Linear)
                    └── linear_fc2 (Linear)
                    └── act_fn (GELUTanh)
            └── 25 (Qwen3VLVisionBlock)
                └── norm1 (LayerNorm)
                └── norm2 (LayerNorm)
                └── attn (Qwen3VLVisionAttention)
                    └── qkv (Linear)
                    └── proj (Linear)
                └── mlp (Qwen3VLVisionMLP)
                    └── linear_fc1 (Linear)
                    └── linear_fc2 (Linear)
                    └── act_fn (GELUTanh)
            └── 26 (Qwen3VLVisionBlock)
                └── norm1 (LayerNorm)
                └── norm2 (LayerNorm)
                └── attn (Qwen3VLVisionAttention)
                    └── qkv (Linear)
                    └── proj (Linear)
                └── mlp (Qwen3VLVisionMLP)
                    └── linear_fc1 (Linear)
                    └── linear_fc2 (Linear)
                    └── act_fn (GELUTanh)
        └── merger (Qwen3VLVisionPatchMerger)
            └── norm (LayerNorm)
            └── linear_fc1 (Linear)
            └── act_fn (GELU)
            └── linear_fc2 (Linear)
        └── deepstack_merger_list (ModuleList)
            └── 0 (Qwen3VLVisionPatchMerger)
                └── norm (LayerNorm)
                └── linear_fc1 (Linear)
                └── act_fn (GELU)
                └── linear_fc2 (Linear)
            └── 1 (Qwen3VLVisionPatchMerger)
                └── norm (LayerNorm)
                └── linear_fc1 (Linear)
                └── act_fn (GELU)
                └── linear_fc2 (Linear)
            └── 2 (Qwen3VLVisionPatchMerger)
                └── norm (LayerNorm)
                └── linear_fc1 (Linear)
                └── act_fn (GELU)
                └── linear_fc2 (Linear)
    └── language_model (Qwen3VLTextModel)
        └── embed_tokens (Embedding)
        └── layers (ModuleList)
            └── 0 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 1 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 2 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 3 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 4 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 5 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 6 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 7 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 8 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 9 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 10 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 11 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 12 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 13 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 14 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 15 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 16 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 17 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 18 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 19 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 20 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 21 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 22 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 23 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 24 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 25 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 26 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 27 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 28 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 29 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 30 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 31 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 32 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 33 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 34 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
            └── 35 (Qwen3VLTextDecoderLayer)
                └── self_attn (Qwen3VLTextAttention)
                    └── q_proj (Linear)
                    └── k_proj (Linear)
                    └── v_proj (Linear)
                    └── o_proj (Linear)
                    └── q_norm (Qwen3VLTextRMSNorm)
                    └── k_norm (Qwen3VLTextRMSNorm)
                └── mlp (Qwen3VLTextMLP)
                    └── gate_proj (Linear)
                    └── up_proj (Linear)
                    └── down_proj (Linear)
                    └── act_fn (SiLUActivation)
                └── input_layernorm (Qwen3VLTextRMSNorm)
                └── post_attention_layernorm (Qwen3VLTextRMSNorm)
        └── norm (Qwen3VLTextRMSNorm)
        └── rotary_emb (Qwen3VLTextRotaryEmbedding)
└── lm_head (Linear)
