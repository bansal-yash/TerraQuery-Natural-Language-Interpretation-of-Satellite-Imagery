{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39e48dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"dataset_root\": \"/home/gaurav/Desktop/GeoAI/small_geo/check_data\", \n",
    "    # under this path expect folders:\n",
    "    #   sar_quads/\n",
    "    #   rgb_quads/\n",
    "    #   falsecolor_quads/\n",
    "\n",
    "    \"classes\": [\"sar\", \"rgb\", \"falsecolor\"],\n",
    "\n",
    "    \"epochs\": 2,\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 1e-5,\n",
    "\n",
    "    \"use_augmentation\": True,\n",
    "\n",
    "    # checkpoints\n",
    "    \"load_checkpoint\": False,\n",
    "    \"checkpoint_load_path\": \"\",\n",
    "    \"checkpoint_save_path\": \"/home/gaurav/scratch/interiit/gaurav/checkpoint/best_model_3classes_450_all_data.pt\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d5b65bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "231eb9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ThreeClassDataset(Dataset):\n",
    "    def __init__(self, root, classes, transform=None):\n",
    "        self.root = root\n",
    "        self.classes = classes\n",
    "        self.transform = transform\n",
    "\n",
    "        self.samples = []   # list of (filepath, label)\n",
    "        image_ext = (\".png\", \".jpg\", \".jpeg\", \".tif\", \".tiff\")\n",
    "\n",
    "        print(\"\\nScanning folders recursively...\\n\")\n",
    "\n",
    "        for label, cls in enumerate(classes):\n",
    "            folder = os.path.join(root, cls + \"_quads\")\n",
    "            if not os.path.isdir(folder):\n",
    "                raise ValueError(f\"Folder missing: {folder}\")\n",
    "\n",
    "            count = 0\n",
    "\n",
    "            # recursively walk inside this class folder\n",
    "            for r, d, files in os.walk(folder):\n",
    "                for f in files:\n",
    "                    if f.lower().endswith(image_ext):\n",
    "                        full_path = os.path.join(r, f)\n",
    "                        self.samples.append((full_path, label))\n",
    "                        count += 1\n",
    "\n",
    "            print(f\"-> {cls}: {count} images\")\n",
    "\n",
    "        print(f\"\\nTotal images loaded: {len(self.samples)}\")\n",
    "        print(f\"Classes: {classes}\\n\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, torch.tensor(label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5216b8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fada65da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scanning folders recursively...\n",
      "\n",
      "-> sar: 10322 images\n",
      "-> rgb: 11802 images\n",
      "-> falsecolor: 11802 images\n",
      "\n",
      "Total images loaded: 33926\n",
      "Classes: ['sar', 'rgb', 'falsecolor']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "full_ds = ThreeClassDataset(\n",
    "    root=CONFIG[\"dataset_root\"],\n",
    "    classes=CONFIG[\"classes\"],\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "\n",
    "val_size = int(0.2 * len(full_ds))\n",
    "test_size = int(0.1 * len(full_ds))\n",
    "train_size = len(full_ds) - val_size - test_size\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(\n",
    "    full_ds, [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# For val/test, override their transforms\n",
    "val_ds.dataset.transform = test_transform\n",
    "test_ds.dataset.transform = test_transform\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=CONFIG[\"batch_size\"], shuffle=True, num_workers=2)\n",
    "val_loader   = DataLoader(val_ds, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=2)\n",
    "test_loader  = DataLoader(test_ds, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563c9825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[ 0.2967,  0.2967,  0.2967,  ...,  0.7248,  0.7077,  0.7077],\n",
      "          [ 0.2796,  0.2967,  0.2967,  ...,  0.7077,  0.7077,  0.6906],\n",
      "          [ 0.2624,  0.2796,  0.3138,  ...,  0.7419,  0.7419,  0.7077],\n",
      "          ...,\n",
      "          [ 0.9303,  0.9646,  0.9988,  ..., -0.1486, -0.1486, -0.1486],\n",
      "          [ 1.0673,  1.0844,  1.1358,  ..., -0.1657, -0.1657, -0.1486],\n",
      "          [ 1.1358,  1.1529,  1.1872,  ..., -0.0972, -0.0972, -0.0801]],\n",
      "\n",
      "         [[ 0.3803,  0.3803,  0.3803,  ...,  0.7654,  0.7829,  0.7829],\n",
      "          [ 0.3627,  0.3803,  0.3803,  ...,  0.7479,  0.7654,  0.7654],\n",
      "          [ 0.3277,  0.3452,  0.3803,  ...,  0.7304,  0.7304,  0.7479],\n",
      "          ...,\n",
      "          [ 0.8004,  0.7829,  0.7654,  ..., -0.0574, -0.0399, -0.0399],\n",
      "          [ 0.7479,  0.7654,  0.7654,  ..., -0.0749, -0.0749, -0.0399],\n",
      "          [ 0.7479,  0.7479,  0.7304,  ..., -0.0049, -0.0049,  0.0301]],\n",
      "\n",
      "         [[ 0.5136,  0.5136,  0.5136,  ...,  0.9842,  0.9494,  0.9494],\n",
      "          [ 0.4962,  0.5136,  0.5136,  ...,  0.9668,  0.9319,  0.9319],\n",
      "          [ 0.4962,  0.5136,  0.5485,  ...,  0.9668,  0.9668,  0.9668],\n",
      "          ...,\n",
      "          [ 1.0714,  1.0365,  1.0191,  ...,  0.5485,  0.4962,  0.4614],\n",
      "          [ 1.0539,  1.0714,  1.0191,  ...,  0.5311,  0.5136,  0.4962],\n",
      "          [ 1.0714,  1.0539,  1.0191,  ...,  0.6008,  0.5834,  0.5659]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3481,  0.7248, -0.3369,  ...,  0.0056, -0.0116, -0.1486],\n",
      "          [-0.0287,  0.2111, -0.4226,  ...,  0.0398,  0.0227,  0.1939],\n",
      "          [-0.1828, -0.5767, -1.1418,  ..., -0.0458,  0.5022,  0.1768],\n",
      "          ...,\n",
      "          [ 0.1939, -0.1999,  0.0912,  ..., -0.1657, -0.6623, -0.3883],\n",
      "          [-0.0116, -0.1999,  0.0912,  ...,  0.3138, -0.2342, -0.3369],\n",
      "          [-0.4226, -0.5767, -0.1486,  ...,  0.4508, -0.0972,  0.0569]],\n",
      "\n",
      "         [[ 0.4853,  0.8704, -0.2150,  ...,  0.1352,  0.1176, -0.0224],\n",
      "          [ 0.1001,  0.3452, -0.3025,  ...,  0.1702,  0.1527,  0.3277],\n",
      "          [-0.0574, -0.4601, -1.0378,  ...,  0.0826,  0.6429,  0.3102],\n",
      "          ...,\n",
      "          [ 0.3277, -0.0749,  0.2227,  ..., -0.0399, -0.5476, -0.2675],\n",
      "          [ 0.1176, -0.0749,  0.2227,  ...,  0.4503, -0.1099, -0.2150],\n",
      "          [-0.3025, -0.4601, -0.0224,  ...,  0.5903,  0.0301,  0.1877]],\n",
      "\n",
      "         [[ 0.7054,  1.0888,  0.0082,  ...,  0.3568,  0.3393,  0.1999],\n",
      "          [ 0.3219,  0.5659, -0.0790,  ...,  0.3916,  0.3742,  0.5485],\n",
      "          [ 0.1651, -0.2358, -0.8110,  ...,  0.3045,  0.8622,  0.5311],\n",
      "          ...,\n",
      "          [ 0.5485,  0.1476,  0.4439,  ...,  0.1825, -0.3230, -0.0441],\n",
      "          [ 0.3393,  0.1476,  0.4439,  ...,  0.6705,  0.1128,  0.0082],\n",
      "          [-0.0790, -0.2358,  0.1999,  ...,  0.8099,  0.2522,  0.4091]]],\n",
      "\n",
      "\n",
      "        [[[-1.4672, -0.0972,  0.3309,  ..., -0.0801, -0.1828, -0.6109],\n",
      "          [-1.6042, -0.3198,  0.3309,  ..., -0.1999, -0.3198, -0.3369],\n",
      "          [-1.9295, -0.5596,  0.4679,  ..., -0.5767, -0.6281, -0.3712],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ...,  0.1426, -0.5424, -0.9877],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -0.4739, -0.5596, -1.0048],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -0.4226, -0.7650, -0.6109]],\n",
      "\n",
      "         [[-1.3704,  0.0301,  0.4678,  ...,  0.0476, -0.0574, -0.4951],\n",
      "          [-1.5105, -0.1975,  0.4678,  ..., -0.0749, -0.1975, -0.2150],\n",
      "          [-1.8431, -0.4426,  0.6078,  ..., -0.4601, -0.5126, -0.2500],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ...,  0.2752, -0.4251, -0.8803],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -0.3550, -0.4426, -0.8978],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -0.3025, -0.6527, -0.4951]],\n",
      "\n",
      "         [[-1.1421,  0.2522,  0.6879,  ...,  0.2696,  0.1651, -0.2707],\n",
      "          [-1.2816,  0.0256,  0.6879,  ...,  0.1476,  0.0256,  0.0082],\n",
      "          [-1.6127, -0.2184,  0.8274,  ..., -0.2358, -0.2881, -0.0267],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ...,  0.4962, -0.2010, -0.6541],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -0.1312, -0.2184, -0.6715],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -0.0790, -0.4275, -0.2707]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.8961,  0.9303,  0.9646,  ...,  0.2624, -0.0972, -0.5253],\n",
      "          [ 1.1358,  1.1358,  1.1187,  ...,  0.0569, -0.0629, -0.2684],\n",
      "          [ 1.5639,  1.4954,  1.3755,  ...,  0.0227, -0.0972, -0.1828],\n",
      "          ...,\n",
      "          [ 1.2214,  0.9817,  0.9132,  ...,  1.0502,  1.1872,  1.3242],\n",
      "          [ 1.2557,  0.9817,  0.8789,  ...,  1.0159,  0.9817,  0.9988],\n",
      "          [ 1.1015,  0.8276,  0.7077,  ...,  1.2043,  1.0331,  0.9303]],\n",
      "\n",
      "         [[-1.5805, -1.5805, -1.6155,  ...,  1.5532,  1.1681,  0.7304],\n",
      "          [-1.6331, -1.6331, -1.6506,  ...,  1.2381,  1.1155,  0.9230],\n",
      "          [-1.7731, -1.7731, -1.7556,  ...,  0.9755,  0.8880,  0.8354],\n",
      "          ...,\n",
      "          [-0.7052, -0.8978, -0.7752,  ..., -1.5455, -1.5980, -1.5280],\n",
      "          [-0.7927, -0.9328, -0.7752,  ..., -1.2304, -1.3704, -1.4055],\n",
      "          [-0.9853, -1.1078, -0.9503,  ..., -0.8277, -1.1078, -1.2479]],\n",
      "\n",
      "         [[-0.9156, -0.9504, -0.9678,  ...,  1.5594,  1.2457,  0.8099],\n",
      "          [-0.8807, -0.9156, -0.9678,  ...,  1.2805,  1.1934,  1.0017],\n",
      "          [-0.8633, -0.9156, -0.9504,  ...,  1.0539,  1.0017,  0.9319],\n",
      "          ...,\n",
      "          [-0.0964, -0.3055, -0.2532,  ..., -0.9504, -0.9330, -0.8807],\n",
      "          [-0.1487, -0.3230, -0.2532,  ..., -0.6890, -0.7936, -0.8458],\n",
      "          [-0.3230, -0.4973, -0.4275,  ..., -0.3230, -0.5670, -0.7238]]],\n",
      "\n",
      "\n",
      "        [[[-2.0837, -1.4500, -1.1932,  ..., -1.0733, -1.2959, -1.4500],\n",
      "          [-2.0837, -1.5185, -1.2103,  ..., -1.1247, -1.2959, -1.4672],\n",
      "          [-2.0837, -1.6042, -1.1760,  ..., -0.9363, -1.1932, -1.6042],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -1.3987, -1.5014, -1.4500],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -1.4158, -1.4843, -1.5014],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -1.4158, -1.5699, -1.5699]],\n",
      "\n",
      "         [[-1.9832, -1.2479, -0.9503,  ..., -0.8978, -1.1429, -1.3179],\n",
      "          [-2.0007, -1.3354, -0.9678,  ..., -0.9503, -1.1604, -1.3529],\n",
      "          [-2.0007, -1.4055, -0.9153,  ..., -0.7402, -1.0553, -1.5105],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -0.8803, -0.9853, -0.9503],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -0.9153, -0.9853, -1.0203],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -0.9153, -1.0903, -1.1078]],\n",
      "\n",
      "         [[-1.7522, -0.8807, -0.5147,  ..., -0.7587, -1.0027, -1.1944],\n",
      "          [-1.7522, -0.9678, -0.5321,  ..., -0.8110, -1.0027, -1.2119],\n",
      "          [-1.7696, -1.0550, -0.4798,  ..., -0.5670, -0.8807, -1.3687],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -0.5321, -0.6541, -0.6018],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -0.5495, -0.6367, -0.6541],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -0.5670, -0.7413, -0.7413]]],\n",
      "\n",
      "\n",
      "        [[[-0.4911, -0.6281, -0.4568,  ...,  1.1529,  1.1872,  1.3413],\n",
      "          [-0.4226, -0.5424, -0.4911,  ...,  1.4954,  1.5125,  1.5639],\n",
      "          [-0.3027, -0.1828,  0.0056,  ...,  1.6324,  1.3070,  1.4098],\n",
      "          ...,\n",
      "          [-2.0152, -1.9980, -2.0494,  ..., -0.6452, -0.3369, -0.0116],\n",
      "          [-2.0152, -2.0323, -2.0494,  ..., -0.3883, -0.2171, -0.0629],\n",
      "          [-1.9809, -1.9980, -1.9980,  ..., -0.8507, -0.1999, -0.1657]],\n",
      "\n",
      "         [[-1.9132, -1.9657, -1.8256,  ..., -2.0182, -1.9307, -1.8782],\n",
      "          [-1.9307, -2.0007, -1.9482,  ..., -1.8957, -1.8256, -1.7906],\n",
      "          [-1.9307, -1.8606, -1.7206,  ..., -1.8606, -1.9832, -1.8957],\n",
      "          ...,\n",
      "          [-1.6506, -1.6506, -1.6681,  ..., -0.8627, -0.7577, -0.6176],\n",
      "          [-1.6506, -1.6681, -1.6681,  ..., -0.7402, -0.7227, -0.6702],\n",
      "          [-1.6331, -1.6506, -1.6506,  ..., -1.0903, -0.7577, -0.7752]],\n",
      "\n",
      "         [[-1.6476, -1.7173, -1.5953,  ..., -1.7522, -1.6650, -1.5953],\n",
      "          [-1.6650, -1.7522, -1.6999,  ..., -1.5953, -1.5430, -1.4907],\n",
      "          [-1.6476, -1.5779, -1.4559,  ..., -1.5256, -1.6824, -1.5953],\n",
      "          ...,\n",
      "          [-1.3861, -1.3687, -1.4036,  ..., -0.4798, -0.3753, -0.2184],\n",
      "          [-1.3861, -1.4036, -1.4036,  ..., -0.3753, -0.3404, -0.2881],\n",
      "          [-1.3687, -1.3861, -1.3687,  ..., -0.7238, -0.3753, -0.3927]]]]), tensor([1, 0, 0, 2, 1, 0, 0, 1, 0, 1, 1, 2, 1, 2, 0, 2, 1, 1, 2, 1, 0, 1, 1, 1,\n",
      "        0, 1, 1, 1, 2, 0, 1, 0, 2, 2, 2, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 2, 0, 0,\n",
      "        1, 1, 1, 0, 2, 0, 1, 1, 0, 1, 2, 1, 0, 2, 1, 2])]\n",
      "Number of items in the batch: 2\n",
      "torch.Size([64, 3, 256, 256])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# print(next(iter(train_loader)))\n",
    "\n",
    "# # input_data, batch_labels = next(iter(train_loader))\n",
    "\n",
    "# # print(input_data.shape)\n",
    "# # print(batch_labels.shape)\n",
    "\n",
    "# # Grab the raw output batch\n",
    "# raw_batch = next(iter(train_loader))\n",
    "\n",
    "# # Check how many items are in the batch\n",
    "# print(f\"Number of items in the batch: {len(raw_batch)}\")\n",
    "\n",
    "# # Unpack into three variables\n",
    "# input_data, batch_labels = next(iter(train_loader))\n",
    "# # OR\n",
    "# input_data, batch_labels = next(iter(train_loader))\n",
    "\n",
    "# print(input_data.shape)\n",
    "# print(batch_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49f3beab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "def build_model(num_classes):\n",
    "    model = models.resnet50(weights=\"IMAGENET1K_V2\")\n",
    "    in_features = model.fc.in_features\n",
    "\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(in_features, 512),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(512, num_classes)\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2a3a5b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = build_model(num_classes=len(CONFIG[\"classes\"])).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=CONFIG[\"learning_rate\"],\n",
    "                       weight_decay=CONFIG[\"weight_decay\"])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scaler = GradScaler()\n",
    "best_val_acc = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "330d8992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 | Train Acc: 0.9694 | Val Acc: 0.9857\n",
      "  Saved best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 | Train Acc: 0.9868 | Val Acc: 0.9857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(CONFIG[\"epochs\"]):\n",
    "    model.train()\n",
    "    train_correct, train_total, train_loss = 0, 0, 0\n",
    "\n",
    "    # ---- TRAIN LOOP WITH TQDM ----\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{CONFIG['epochs']} - Train\", leave=False)\n",
    "\n",
    "    for imgs, labels in pbar:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast(\"cuda\"):\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item() * imgs.size(0)\n",
    "        train_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "        train_total += len(labels)\n",
    "\n",
    "        # live stats in progress bar\n",
    "        pbar.set_postfix({\n",
    "            \"loss\": f\"{train_loss/train_total:.4f}\",\n",
    "            \"acc\": f\"{train_correct/train_total:.4f}\"\n",
    "        })\n",
    "\n",
    "    # ---- VALIDATION LOOP WITH TQDM ----\n",
    "    model.eval()\n",
    "    val_correct, val_total, val_loss = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar_val = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{CONFIG['epochs']} - Val\", leave=False)\n",
    "\n",
    "        for imgs, labels in pbar_val:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * imgs.size(0)\n",
    "            val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "            val_total += len(labels)\n",
    "\n",
    "            pbar_val.set_postfix({\n",
    "                \"loss\": f\"{val_loss/val_total:.4f}\",\n",
    "                \"acc\": f\"{val_correct/val_total:.4f}\"\n",
    "            })\n",
    "\n",
    "    # ---- Epoch Summary ----\n",
    "    train_acc = train_correct / train_total\n",
    "    val_acc   = val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{CONFIG['epochs']} | \"\n",
    "          f\"Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), CONFIG[\"checkpoint_save_path\"])\n",
    "        print(\"  Saved best model!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "271c53c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_62347/3628695571.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(CONFIG[\"checkpoint_save_path\"]))\n",
      "Testing: 100%|██████████| 53/53 [00:05<00:00,  9.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.9823113207547169\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         sar       1.00      1.00      1.00      1040\n",
      "         rgb       0.97      0.98      0.97      1176\n",
      "  falsecolor       0.98      0.97      0.97      1176\n",
      "\n",
      "    accuracy                           0.98      3392\n",
      "   macro avg       0.98      0.98      0.98      3392\n",
      "weighted avg       0.98      0.98      0.98      3392\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1039    1    0]\n",
      " [   0 1156   20]\n",
      " [   0   39 1137]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "model.load_state_dict(torch.load(CONFIG[\"checkpoint_save_path\"]))\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        imgs = imgs.to(device)\n",
    "        outputs = model(imgs)\n",
    "        preds = outputs.argmax(1).cpu().numpy()\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels.numpy())\n",
    "\n",
    "y_pred = np.concatenate(all_preds)\n",
    "y_true = np.concatenate(all_labels)\n",
    "\n",
    "print(\"\\nTest Accuracy:\", (y_pred == y_true).mean())\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=CONFIG[\"classes\"]))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3153bf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1c4ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
