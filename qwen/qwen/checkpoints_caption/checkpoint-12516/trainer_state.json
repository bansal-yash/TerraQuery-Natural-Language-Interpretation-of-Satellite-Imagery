{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 12516,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0015981461504654601,
      "grad_norm": 8.25,
      "learning_rate": 0.000199856184084372,
      "loss": 7.3195,
      "step": 10
    },
    {
      "epoch": 0.0031962923009309203,
      "grad_norm": 5.5,
      "learning_rate": 0.00019969638862256313,
      "loss": 2.6733,
      "step": 20
    },
    {
      "epoch": 0.00479443845139638,
      "grad_norm": 4.4375,
      "learning_rate": 0.00019953659316075423,
      "loss": 2.4583,
      "step": 30
    },
    {
      "epoch": 0.0063925846018618405,
      "grad_norm": 3.375,
      "learning_rate": 0.00019937679769894536,
      "loss": 1.8708,
      "step": 40
    },
    {
      "epoch": 0.0079907307523273,
      "grad_norm": 12.25,
      "learning_rate": 0.0001992170022371365,
      "loss": 2.0698,
      "step": 50
    },
    {
      "epoch": 0.00958887690279276,
      "grad_norm": 2.75,
      "learning_rate": 0.0001990572067753276,
      "loss": 1.6353,
      "step": 60
    },
    {
      "epoch": 0.01118702305325822,
      "grad_norm": 4.5625,
      "learning_rate": 0.00019889741131351872,
      "loss": 1.6552,
      "step": 70
    },
    {
      "epoch": 0.012785169203723681,
      "grad_norm": 5.4375,
      "learning_rate": 0.00019873761585170982,
      "loss": 2.6854,
      "step": 80
    },
    {
      "epoch": 0.01438331535418914,
      "grad_norm": 3.25,
      "learning_rate": 0.00019857782038990094,
      "loss": 2.3467,
      "step": 90
    },
    {
      "epoch": 0.0159814615046546,
      "grad_norm": 2.578125,
      "learning_rate": 0.00019841802492809204,
      "loss": 1.8789,
      "step": 100
    },
    {
      "epoch": 0.01757960765512006,
      "grad_norm": 4.03125,
      "learning_rate": 0.00019825822946628317,
      "loss": 1.7116,
      "step": 110
    },
    {
      "epoch": 0.01917775380558552,
      "grad_norm": 2.078125,
      "learning_rate": 0.00019809843400447427,
      "loss": 1.8941,
      "step": 120
    },
    {
      "epoch": 0.020775899956050982,
      "grad_norm": 1.453125,
      "learning_rate": 0.0001979386385426654,
      "loss": 1.6487,
      "step": 130
    },
    {
      "epoch": 0.02237404610651644,
      "grad_norm": 3.234375,
      "learning_rate": 0.00019777884308085653,
      "loss": 2.1628,
      "step": 140
    },
    {
      "epoch": 0.0239721922569819,
      "grad_norm": 2.46875,
      "learning_rate": 0.00019761904761904763,
      "loss": 1.7166,
      "step": 150
    },
    {
      "epoch": 0.025570338407447362,
      "grad_norm": 1.796875,
      "learning_rate": 0.00019745925215723876,
      "loss": 1.7277,
      "step": 160
    },
    {
      "epoch": 0.02716848455791282,
      "grad_norm": 3.46875,
      "learning_rate": 0.00019729945669542986,
      "loss": 1.5093,
      "step": 170
    },
    {
      "epoch": 0.02876663070837828,
      "grad_norm": 3.46875,
      "learning_rate": 0.00019713966123362098,
      "loss": 2.0874,
      "step": 180
    },
    {
      "epoch": 0.030364776858843742,
      "grad_norm": 2.984375,
      "learning_rate": 0.00019697986577181208,
      "loss": 2.8651,
      "step": 190
    },
    {
      "epoch": 0.0319629230093092,
      "grad_norm": 32.5,
      "learning_rate": 0.0001968200703100032,
      "loss": 1.5343,
      "step": 200
    },
    {
      "epoch": 0.033561069159774665,
      "grad_norm": 3.515625,
      "learning_rate": 0.0001966602748481943,
      "loss": 2.2058,
      "step": 210
    },
    {
      "epoch": 0.03515921531024012,
      "grad_norm": 2.53125,
      "learning_rate": 0.00019650047938638544,
      "loss": 2.0422,
      "step": 220
    },
    {
      "epoch": 0.03675736146070558,
      "grad_norm": 1.4609375,
      "learning_rate": 0.00019634068392457654,
      "loss": 1.7033,
      "step": 230
    },
    {
      "epoch": 0.03835550761117104,
      "grad_norm": 33.5,
      "learning_rate": 0.00019618088846276767,
      "loss": 2.0487,
      "step": 240
    },
    {
      "epoch": 0.0399536537616365,
      "grad_norm": 1.96875,
      "learning_rate": 0.0001960210930009588,
      "loss": 1.9263,
      "step": 250
    },
    {
      "epoch": 0.041551799912101964,
      "grad_norm": 2.171875,
      "learning_rate": 0.0001958612975391499,
      "loss": 1.6311,
      "step": 260
    },
    {
      "epoch": 0.043149946062567425,
      "grad_norm": 1.8359375,
      "learning_rate": 0.00019570150207734102,
      "loss": 1.8442,
      "step": 270
    },
    {
      "epoch": 0.04474809221303288,
      "grad_norm": 3.34375,
      "learning_rate": 0.00019554170661553212,
      "loss": 1.763,
      "step": 280
    },
    {
      "epoch": 0.04634623836349834,
      "grad_norm": 3.890625,
      "learning_rate": 0.00019538191115372325,
      "loss": 1.2923,
      "step": 290
    },
    {
      "epoch": 0.0479443845139638,
      "grad_norm": 2.75,
      "learning_rate": 0.00019522211569191435,
      "loss": 1.7308,
      "step": 300
    },
    {
      "epoch": 0.04954253066442926,
      "grad_norm": 2.796875,
      "learning_rate": 0.00019506232023010548,
      "loss": 1.9412,
      "step": 310
    },
    {
      "epoch": 0.051140676814894724,
      "grad_norm": 8.8125,
      "learning_rate": 0.00019490252476829658,
      "loss": 1.7711,
      "step": 320
    },
    {
      "epoch": 0.052738822965360185,
      "grad_norm": 2.703125,
      "learning_rate": 0.0001947427293064877,
      "loss": 1.7282,
      "step": 330
    },
    {
      "epoch": 0.05433696911582564,
      "grad_norm": 6.5625,
      "learning_rate": 0.00019458293384467883,
      "loss": 1.649,
      "step": 340
    },
    {
      "epoch": 0.0559351152662911,
      "grad_norm": 2.65625,
      "learning_rate": 0.00019442313838286993,
      "loss": 1.871,
      "step": 350
    },
    {
      "epoch": 0.05753326141675656,
      "grad_norm": 2.265625,
      "learning_rate": 0.00019426334292106106,
      "loss": 1.9052,
      "step": 360
    },
    {
      "epoch": 0.05913140756722202,
      "grad_norm": 2.375,
      "learning_rate": 0.00019410354745925216,
      "loss": 1.6312,
      "step": 370
    },
    {
      "epoch": 0.060729553717687484,
      "grad_norm": 1.203125,
      "learning_rate": 0.0001939437519974433,
      "loss": 1.2334,
      "step": 380
    },
    {
      "epoch": 0.062327699868152946,
      "grad_norm": 6.40625,
      "learning_rate": 0.0001937839565356344,
      "loss": 1.876,
      "step": 390
    },
    {
      "epoch": 0.0639258460186184,
      "grad_norm": 2.34375,
      "learning_rate": 0.00019362416107382552,
      "loss": 1.8185,
      "step": 400
    },
    {
      "epoch": 0.06552399216908386,
      "grad_norm": 2.59375,
      "learning_rate": 0.00019346436561201662,
      "loss": 1.8169,
      "step": 410
    },
    {
      "epoch": 0.06712213831954933,
      "grad_norm": 3.28125,
      "learning_rate": 0.00019330457015020774,
      "loss": 2.2716,
      "step": 420
    },
    {
      "epoch": 0.06872028447001478,
      "grad_norm": 4.625,
      "learning_rate": 0.00019314477468839887,
      "loss": 2.2968,
      "step": 430
    },
    {
      "epoch": 0.07031843062048024,
      "grad_norm": 15.0,
      "learning_rate": 0.00019298497922658997,
      "loss": 1.5851,
      "step": 440
    },
    {
      "epoch": 0.0719165767709457,
      "grad_norm": 5.875,
      "learning_rate": 0.0001928251837647811,
      "loss": 2.1907,
      "step": 450
    },
    {
      "epoch": 0.07351472292141116,
      "grad_norm": 3.796875,
      "learning_rate": 0.0001926653883029722,
      "loss": 1.9154,
      "step": 460
    },
    {
      "epoch": 0.07511286907187663,
      "grad_norm": 1.4921875,
      "learning_rate": 0.00019250559284116333,
      "loss": 1.1607,
      "step": 470
    },
    {
      "epoch": 0.07671101522234208,
      "grad_norm": 2.0,
      "learning_rate": 0.00019234579737935443,
      "loss": 1.6796,
      "step": 480
    },
    {
      "epoch": 0.07830916137280754,
      "grad_norm": 5.78125,
      "learning_rate": 0.00019218600191754556,
      "loss": 1.7889,
      "step": 490
    },
    {
      "epoch": 0.079907307523273,
      "grad_norm": 2.171875,
      "learning_rate": 0.00019202620645573666,
      "loss": 2.0242,
      "step": 500
    },
    {
      "epoch": 0.08150545367373846,
      "grad_norm": 2.125,
      "learning_rate": 0.00019186641099392778,
      "loss": 1.6142,
      "step": 510
    },
    {
      "epoch": 0.08310359982420393,
      "grad_norm": 1.90625,
      "learning_rate": 0.0001917066155321189,
      "loss": 1.7703,
      "step": 520
    },
    {
      "epoch": 0.08470174597466938,
      "grad_norm": 3.84375,
      "learning_rate": 0.00019154682007031,
      "loss": 2.0758,
      "step": 530
    },
    {
      "epoch": 0.08629989212513485,
      "grad_norm": 3.40625,
      "learning_rate": 0.00019138702460850114,
      "loss": 1.8085,
      "step": 540
    },
    {
      "epoch": 0.0878980382756003,
      "grad_norm": 2.796875,
      "learning_rate": 0.00019122722914669224,
      "loss": 1.9337,
      "step": 550
    },
    {
      "epoch": 0.08949618442606576,
      "grad_norm": 2.984375,
      "learning_rate": 0.00019106743368488337,
      "loss": 1.7212,
      "step": 560
    },
    {
      "epoch": 0.09109433057653123,
      "grad_norm": 2.828125,
      "learning_rate": 0.00019090763822307447,
      "loss": 1.8517,
      "step": 570
    },
    {
      "epoch": 0.09269247672699668,
      "grad_norm": 5.0625,
      "learning_rate": 0.0001907478427612656,
      "loss": 1.8299,
      "step": 580
    },
    {
      "epoch": 0.09429062287746215,
      "grad_norm": 5.5,
      "learning_rate": 0.0001905880472994567,
      "loss": 1.6946,
      "step": 590
    },
    {
      "epoch": 0.0958887690279276,
      "grad_norm": 1.59375,
      "learning_rate": 0.00019042825183764782,
      "loss": 1.9492,
      "step": 600
    },
    {
      "epoch": 0.09748691517839306,
      "grad_norm": 2.5,
      "learning_rate": 0.00019026845637583895,
      "loss": 1.9168,
      "step": 610
    },
    {
      "epoch": 0.09908506132885853,
      "grad_norm": 4.875,
      "learning_rate": 0.00019010866091403005,
      "loss": 1.7695,
      "step": 620
    },
    {
      "epoch": 0.10068320747932398,
      "grad_norm": 2.46875,
      "learning_rate": 0.00018994886545222118,
      "loss": 1.8288,
      "step": 630
    },
    {
      "epoch": 0.10228135362978945,
      "grad_norm": 2.203125,
      "learning_rate": 0.00018978906999041228,
      "loss": 1.7267,
      "step": 640
    },
    {
      "epoch": 0.1038794997802549,
      "grad_norm": 1.2734375,
      "learning_rate": 0.0001896292745286034,
      "loss": 1.4296,
      "step": 650
    },
    {
      "epoch": 0.10547764593072037,
      "grad_norm": 3.140625,
      "learning_rate": 0.0001894694790667945,
      "loss": 2.1204,
      "step": 660
    },
    {
      "epoch": 0.10707579208118582,
      "grad_norm": 0.0279541015625,
      "learning_rate": 0.00018930968360498563,
      "loss": 1.5496,
      "step": 670
    },
    {
      "epoch": 0.10867393823165128,
      "grad_norm": 4.65625,
      "learning_rate": 0.00018914988814317673,
      "loss": 2.1328,
      "step": 680
    },
    {
      "epoch": 0.11027208438211675,
      "grad_norm": 4.71875,
      "learning_rate": 0.00018899009268136783,
      "loss": 1.9946,
      "step": 690
    },
    {
      "epoch": 0.1118702305325822,
      "grad_norm": 6.0625,
      "learning_rate": 0.000188830297219559,
      "loss": 1.7911,
      "step": 700
    },
    {
      "epoch": 0.11346837668304767,
      "grad_norm": 1.609375,
      "learning_rate": 0.0001886705017577501,
      "loss": 1.4082,
      "step": 710
    },
    {
      "epoch": 0.11506652283351312,
      "grad_norm": 12.0,
      "learning_rate": 0.00018851070629594122,
      "loss": 1.7319,
      "step": 720
    },
    {
      "epoch": 0.11666466898397858,
      "grad_norm": 1.90625,
      "learning_rate": 0.00018835091083413232,
      "loss": 1.5081,
      "step": 730
    },
    {
      "epoch": 0.11826281513444405,
      "grad_norm": 18.25,
      "learning_rate": 0.00018819111537232344,
      "loss": 5.867,
      "step": 740
    },
    {
      "epoch": 0.1198609612849095,
      "grad_norm": 3.828125,
      "learning_rate": 0.00018803131991051454,
      "loss": 4.0949,
      "step": 750
    },
    {
      "epoch": 0.12145910743537497,
      "grad_norm": 1.453125,
      "learning_rate": 0.00018787152444870567,
      "loss": 1.7143,
      "step": 760
    },
    {
      "epoch": 0.12305725358584042,
      "grad_norm": 1.4609375,
      "learning_rate": 0.00018771172898689677,
      "loss": 1.9178,
      "step": 770
    },
    {
      "epoch": 0.12465539973630589,
      "grad_norm": 2.03125,
      "learning_rate": 0.00018755193352508787,
      "loss": 2.1017,
      "step": 780
    },
    {
      "epoch": 0.12625354588677135,
      "grad_norm": 6.25,
      "learning_rate": 0.00018739213806327903,
      "loss": 1.9686,
      "step": 790
    },
    {
      "epoch": 0.1278516920372368,
      "grad_norm": 2.5,
      "learning_rate": 0.00018723234260147013,
      "loss": 2.1516,
      "step": 800
    },
    {
      "epoch": 0.12944983818770225,
      "grad_norm": 3.90625,
      "learning_rate": 0.00018707254713966126,
      "loss": 2.1021,
      "step": 810
    },
    {
      "epoch": 0.13104798433816772,
      "grad_norm": 0.0054931640625,
      "learning_rate": 0.00018691275167785236,
      "loss": 1.4975,
      "step": 820
    },
    {
      "epoch": 0.1326461304886332,
      "grad_norm": 2.015625,
      "learning_rate": 0.00018675295621604348,
      "loss": 1.8287,
      "step": 830
    },
    {
      "epoch": 0.13424427663909866,
      "grad_norm": 1.5859375,
      "learning_rate": 0.00018659316075423458,
      "loss": 1.74,
      "step": 840
    },
    {
      "epoch": 0.1358424227895641,
      "grad_norm": 1.859375,
      "learning_rate": 0.0001864333652924257,
      "loss": 1.4784,
      "step": 850
    },
    {
      "epoch": 0.13744056894002957,
      "grad_norm": 1.9765625,
      "learning_rate": 0.0001862735698306168,
      "loss": 1.8214,
      "step": 860
    },
    {
      "epoch": 0.13903871509049504,
      "grad_norm": 6.0625,
      "learning_rate": 0.0001861137743688079,
      "loss": 1.4959,
      "step": 870
    },
    {
      "epoch": 0.14063686124096048,
      "grad_norm": 2.1875,
      "learning_rate": 0.00018595397890699907,
      "loss": 1.7926,
      "step": 880
    },
    {
      "epoch": 0.14223500739142594,
      "grad_norm": 3.984375,
      "learning_rate": 0.00018579418344519017,
      "loss": 1.5478,
      "step": 890
    },
    {
      "epoch": 0.1438331535418914,
      "grad_norm": 6.46875,
      "learning_rate": 0.0001856343879833813,
      "loss": 2.4582,
      "step": 900
    },
    {
      "epoch": 0.14543129969235685,
      "grad_norm": 1.9375,
      "learning_rate": 0.0001854745925215724,
      "loss": 1.8707,
      "step": 910
    },
    {
      "epoch": 0.14702944584282232,
      "grad_norm": 3.75,
      "learning_rate": 0.00018531479705976352,
      "loss": 1.6345,
      "step": 920
    },
    {
      "epoch": 0.1486275919932878,
      "grad_norm": 2.890625,
      "learning_rate": 0.00018515500159795462,
      "loss": 1.6285,
      "step": 930
    },
    {
      "epoch": 0.15022573814375326,
      "grad_norm": 1.4921875,
      "learning_rate": 0.00018499520613614575,
      "loss": 1.1922,
      "step": 940
    },
    {
      "epoch": 0.1518238842942187,
      "grad_norm": 2.703125,
      "learning_rate": 0.00018483541067433685,
      "loss": 1.5982,
      "step": 950
    },
    {
      "epoch": 0.15342203044468417,
      "grad_norm": 1.9921875,
      "learning_rate": 0.00018467561521252795,
      "loss": 1.7951,
      "step": 960
    },
    {
      "epoch": 0.15502017659514963,
      "grad_norm": 2.46875,
      "learning_rate": 0.0001845158197507191,
      "loss": 1.5975,
      "step": 970
    },
    {
      "epoch": 0.15661832274561507,
      "grad_norm": 3.90625,
      "learning_rate": 0.0001843560242889102,
      "loss": 1.7471,
      "step": 980
    },
    {
      "epoch": 0.15821646889608054,
      "grad_norm": 6.46875,
      "learning_rate": 0.00018419622882710133,
      "loss": 1.6357,
      "step": 990
    },
    {
      "epoch": 0.159814615046546,
      "grad_norm": 4.75,
      "learning_rate": 0.00018403643336529243,
      "loss": 1.6798,
      "step": 1000
    },
    {
      "epoch": 0.16141276119701148,
      "grad_norm": 36.25,
      "learning_rate": 0.00018387663790348356,
      "loss": 2.1008,
      "step": 1010
    },
    {
      "epoch": 0.16301090734747692,
      "grad_norm": 0.2265625,
      "learning_rate": 0.00018371684244167466,
      "loss": 1.4058,
      "step": 1020
    },
    {
      "epoch": 0.1646090534979424,
      "grad_norm": 1.4296875,
      "learning_rate": 0.0001835570469798658,
      "loss": 1.5555,
      "step": 1030
    },
    {
      "epoch": 0.16620719964840786,
      "grad_norm": 1.9453125,
      "learning_rate": 0.0001833972515180569,
      "loss": 2.0397,
      "step": 1040
    },
    {
      "epoch": 0.1678053457988733,
      "grad_norm": 1.6484375,
      "learning_rate": 0.000183237456056248,
      "loss": 1.965,
      "step": 1050
    },
    {
      "epoch": 0.16940349194933876,
      "grad_norm": 12.1875,
      "learning_rate": 0.00018307766059443914,
      "loss": 2.0028,
      "step": 1060
    },
    {
      "epoch": 0.17100163809980423,
      "grad_norm": 0.6171875,
      "learning_rate": 0.00018291786513263025,
      "loss": 1.2509,
      "step": 1070
    },
    {
      "epoch": 0.1725997842502697,
      "grad_norm": 3.953125,
      "learning_rate": 0.00018275806967082137,
      "loss": 2.0118,
      "step": 1080
    },
    {
      "epoch": 0.17419793040073514,
      "grad_norm": 3.453125,
      "learning_rate": 0.00018259827420901247,
      "loss": 1.7555,
      "step": 1090
    },
    {
      "epoch": 0.1757960765512006,
      "grad_norm": 2.046875,
      "learning_rate": 0.0001824384787472036,
      "loss": 1.8351,
      "step": 1100
    },
    {
      "epoch": 0.17739422270166608,
      "grad_norm": 11.25,
      "learning_rate": 0.0001822786832853947,
      "loss": 1.6086,
      "step": 1110
    },
    {
      "epoch": 0.17899236885213152,
      "grad_norm": 2.8125,
      "learning_rate": 0.00018211888782358583,
      "loss": 1.6455,
      "step": 1120
    },
    {
      "epoch": 0.18059051500259699,
      "grad_norm": 1.9375,
      "learning_rate": 0.00018195909236177693,
      "loss": 2.0302,
      "step": 1130
    },
    {
      "epoch": 0.18218866115306245,
      "grad_norm": 2.734375,
      "learning_rate": 0.00018179929689996803,
      "loss": 1.7925,
      "step": 1140
    },
    {
      "epoch": 0.18378680730352792,
      "grad_norm": 3.59375,
      "learning_rate": 0.00018163950143815916,
      "loss": 2.0835,
      "step": 1150
    },
    {
      "epoch": 0.18538495345399336,
      "grad_norm": 3.484375,
      "learning_rate": 0.00018147970597635028,
      "loss": 1.7928,
      "step": 1160
    },
    {
      "epoch": 0.18698309960445883,
      "grad_norm": 1.359375,
      "learning_rate": 0.0001813199105145414,
      "loss": 1.889,
      "step": 1170
    },
    {
      "epoch": 0.1885812457549243,
      "grad_norm": 2.234375,
      "learning_rate": 0.0001811601150527325,
      "loss": 1.7635,
      "step": 1180
    },
    {
      "epoch": 0.19017939190538974,
      "grad_norm": 3.5625,
      "learning_rate": 0.00018100031959092364,
      "loss": 1.7697,
      "step": 1190
    },
    {
      "epoch": 0.1917775380558552,
      "grad_norm": 2.515625,
      "learning_rate": 0.00018084052412911474,
      "loss": 1.7236,
      "step": 1200
    },
    {
      "epoch": 0.19337568420632067,
      "grad_norm": 2.875,
      "learning_rate": 0.00018068072866730587,
      "loss": 1.5084,
      "step": 1210
    },
    {
      "epoch": 0.19497383035678612,
      "grad_norm": 1.0703125,
      "learning_rate": 0.00018052093320549697,
      "loss": 1.5451,
      "step": 1220
    },
    {
      "epoch": 0.19657197650725158,
      "grad_norm": 0.0791015625,
      "learning_rate": 0.00018036113774368807,
      "loss": 1.6946,
      "step": 1230
    },
    {
      "epoch": 0.19817012265771705,
      "grad_norm": 1.2578125,
      "learning_rate": 0.0001802013422818792,
      "loss": 1.4974,
      "step": 1240
    },
    {
      "epoch": 0.19976826880818252,
      "grad_norm": 0.255859375,
      "learning_rate": 0.00018004154682007032,
      "loss": 1.2296,
      "step": 1250
    },
    {
      "epoch": 0.20136641495864796,
      "grad_norm": 1.9453125,
      "learning_rate": 0.00017988175135826145,
      "loss": 1.5172,
      "step": 1260
    },
    {
      "epoch": 0.20296456110911343,
      "grad_norm": 5.46875,
      "learning_rate": 0.00017972195589645255,
      "loss": 2.2135,
      "step": 1270
    },
    {
      "epoch": 0.2045627072595789,
      "grad_norm": 1.8984375,
      "learning_rate": 0.00017956216043464368,
      "loss": 1.77,
      "step": 1280
    },
    {
      "epoch": 0.20616085341004434,
      "grad_norm": 2.140625,
      "learning_rate": 0.00017940236497283478,
      "loss": 1.6418,
      "step": 1290
    },
    {
      "epoch": 0.2077589995605098,
      "grad_norm": 2.609375,
      "learning_rate": 0.0001792425695110259,
      "loss": 1.6838,
      "step": 1300
    },
    {
      "epoch": 0.20935714571097527,
      "grad_norm": 2.46875,
      "learning_rate": 0.000179082774049217,
      "loss": 1.661,
      "step": 1310
    },
    {
      "epoch": 0.21095529186144074,
      "grad_norm": 4.6875,
      "learning_rate": 0.0001789229785874081,
      "loss": 2.0902,
      "step": 1320
    },
    {
      "epoch": 0.21255343801190618,
      "grad_norm": 0.9765625,
      "learning_rate": 0.00017876318312559923,
      "loss": 1.9011,
      "step": 1330
    },
    {
      "epoch": 0.21415158416237165,
      "grad_norm": 3.28125,
      "learning_rate": 0.00017860338766379036,
      "loss": 1.6703,
      "step": 1340
    },
    {
      "epoch": 0.21574973031283712,
      "grad_norm": 2.640625,
      "learning_rate": 0.0001784435922019815,
      "loss": 1.7631,
      "step": 1350
    },
    {
      "epoch": 0.21734787646330256,
      "grad_norm": 5.625,
      "learning_rate": 0.0001782837967401726,
      "loss": 2.1171,
      "step": 1360
    },
    {
      "epoch": 0.21894602261376803,
      "grad_norm": 4.9375,
      "learning_rate": 0.00017812400127836372,
      "loss": 2.1257,
      "step": 1370
    },
    {
      "epoch": 0.2205441687642335,
      "grad_norm": 0.00029754638671875,
      "learning_rate": 0.00017796420581655482,
      "loss": 1.6643,
      "step": 1380
    },
    {
      "epoch": 0.22214231491469896,
      "grad_norm": 2.765625,
      "learning_rate": 0.00017780441035474592,
      "loss": 2.0243,
      "step": 1390
    },
    {
      "epoch": 0.2237404610651644,
      "grad_norm": 1.1015625,
      "learning_rate": 0.00017764461489293705,
      "loss": 1.9462,
      "step": 1400
    },
    {
      "epoch": 0.22533860721562987,
      "grad_norm": 1.5546875,
      "learning_rate": 0.00017748481943112815,
      "loss": 1.6616,
      "step": 1410
    },
    {
      "epoch": 0.22693675336609534,
      "grad_norm": 2.34375,
      "learning_rate": 0.00017732502396931927,
      "loss": 1.5246,
      "step": 1420
    },
    {
      "epoch": 0.22853489951656078,
      "grad_norm": 0.16015625,
      "learning_rate": 0.0001771652285075104,
      "loss": 1.5889,
      "step": 1430
    },
    {
      "epoch": 0.23013304566702625,
      "grad_norm": 2.75,
      "learning_rate": 0.00017700543304570153,
      "loss": 2.1048,
      "step": 1440
    },
    {
      "epoch": 0.23173119181749172,
      "grad_norm": 2.796875,
      "learning_rate": 0.00017684563758389263,
      "loss": 1.6178,
      "step": 1450
    },
    {
      "epoch": 0.23332933796795716,
      "grad_norm": 2.59375,
      "learning_rate": 0.00017668584212208376,
      "loss": 1.6605,
      "step": 1460
    },
    {
      "epoch": 0.23492748411842262,
      "grad_norm": 4.75,
      "learning_rate": 0.00017652604666027486,
      "loss": 1.1268,
      "step": 1470
    },
    {
      "epoch": 0.2365256302688881,
      "grad_norm": 2.390625,
      "learning_rate": 0.00017636625119846596,
      "loss": 1.7714,
      "step": 1480
    },
    {
      "epoch": 0.23812377641935356,
      "grad_norm": 1.140625,
      "learning_rate": 0.00017620645573665708,
      "loss": 1.8997,
      "step": 1490
    },
    {
      "epoch": 0.239721922569819,
      "grad_norm": 1.875,
      "learning_rate": 0.00017604666027484819,
      "loss": 1.8324,
      "step": 1500
    },
    {
      "epoch": 0.24132006872028447,
      "grad_norm": 4.4375,
      "learning_rate": 0.0001758868648130393,
      "loss": 1.396,
      "step": 1510
    },
    {
      "epoch": 0.24291821487074994,
      "grad_norm": 1.953125,
      "learning_rate": 0.00017572706935123044,
      "loss": 1.368,
      "step": 1520
    },
    {
      "epoch": 0.24451636102121538,
      "grad_norm": 1.4296875,
      "learning_rate": 0.00017556727388942157,
      "loss": 1.6381,
      "step": 1530
    },
    {
      "epoch": 0.24611450717168085,
      "grad_norm": 2.796875,
      "learning_rate": 0.00017540747842761267,
      "loss": 1.8479,
      "step": 1540
    },
    {
      "epoch": 0.24771265332214631,
      "grad_norm": 2.484375,
      "learning_rate": 0.0001752476829658038,
      "loss": 1.2994,
      "step": 1550
    },
    {
      "epoch": 0.24931079947261178,
      "grad_norm": 1.65625,
      "learning_rate": 0.0001750878875039949,
      "loss": 1.716,
      "step": 1560
    },
    {
      "epoch": 0.25090894562307725,
      "grad_norm": 1.421875,
      "learning_rate": 0.000174928092042186,
      "loss": 1.3523,
      "step": 1570
    },
    {
      "epoch": 0.2525070917735427,
      "grad_norm": 1.765625,
      "learning_rate": 0.00017476829658037712,
      "loss": 1.607,
      "step": 1580
    },
    {
      "epoch": 0.25410523792400813,
      "grad_norm": 2.140625,
      "learning_rate": 0.00017460850111856822,
      "loss": 1.6201,
      "step": 1590
    },
    {
      "epoch": 0.2557033840744736,
      "grad_norm": 1.2109375,
      "learning_rate": 0.00017444870565675935,
      "loss": 1.514,
      "step": 1600
    },
    {
      "epoch": 0.25730153022493907,
      "grad_norm": 1.8046875,
      "learning_rate": 0.00017428891019495045,
      "loss": 1.328,
      "step": 1610
    },
    {
      "epoch": 0.2588996763754045,
      "grad_norm": 2.90625,
      "learning_rate": 0.0001741291147331416,
      "loss": 2.2164,
      "step": 1620
    },
    {
      "epoch": 0.26049782252587,
      "grad_norm": 1.96875,
      "learning_rate": 0.0001739693192713327,
      "loss": 1.9706,
      "step": 1630
    },
    {
      "epoch": 0.26209596867633544,
      "grad_norm": 109.0,
      "learning_rate": 0.00017380952380952383,
      "loss": 1.6617,
      "step": 1640
    },
    {
      "epoch": 0.2636941148268009,
      "grad_norm": 2.859375,
      "learning_rate": 0.00017364972834771493,
      "loss": 1.387,
      "step": 1650
    },
    {
      "epoch": 0.2652922609772664,
      "grad_norm": 1.265625,
      "learning_rate": 0.00017348993288590604,
      "loss": 1.1977,
      "step": 1660
    },
    {
      "epoch": 0.2668904071277318,
      "grad_norm": 1.921875,
      "learning_rate": 0.00017333013742409716,
      "loss": 1.9349,
      "step": 1670
    },
    {
      "epoch": 0.2684885532781973,
      "grad_norm": 1.5,
      "learning_rate": 0.00017317034196228826,
      "loss": 1.5932,
      "step": 1680
    },
    {
      "epoch": 0.27008669942866276,
      "grad_norm": 1.2109375,
      "learning_rate": 0.0001730105465004794,
      "loss": 1.5441,
      "step": 1690
    },
    {
      "epoch": 0.2716848455791282,
      "grad_norm": 1.28125,
      "learning_rate": 0.0001728507510386705,
      "loss": 1.505,
      "step": 1700
    },
    {
      "epoch": 0.2732829917295937,
      "grad_norm": 2.6875,
      "learning_rate": 0.00017269095557686165,
      "loss": 1.6968,
      "step": 1710
    },
    {
      "epoch": 0.27488113788005913,
      "grad_norm": 1.5234375,
      "learning_rate": 0.00017253116011505275,
      "loss": 1.4986,
      "step": 1720
    },
    {
      "epoch": 0.2764792840305246,
      "grad_norm": 2.203125,
      "learning_rate": 0.00017237136465324387,
      "loss": 1.1031,
      "step": 1730
    },
    {
      "epoch": 0.27807743018099007,
      "grad_norm": 1.7734375,
      "learning_rate": 0.00017221156919143497,
      "loss": 1.5012,
      "step": 1740
    },
    {
      "epoch": 0.2796755763314555,
      "grad_norm": 1.640625,
      "learning_rate": 0.00017205177372962607,
      "loss": 1.3378,
      "step": 1750
    },
    {
      "epoch": 0.28127372248192095,
      "grad_norm": 2.21875,
      "learning_rate": 0.0001718919782678172,
      "loss": 1.6335,
      "step": 1760
    },
    {
      "epoch": 0.28287186863238645,
      "grad_norm": 1.1953125,
      "learning_rate": 0.0001717321828060083,
      "loss": 1.7463,
      "step": 1770
    },
    {
      "epoch": 0.2844700147828519,
      "grad_norm": 3.4375,
      "learning_rate": 0.00017157238734419943,
      "loss": 1.3703,
      "step": 1780
    },
    {
      "epoch": 0.28606816093331733,
      "grad_norm": 3.6875,
      "learning_rate": 0.00017141259188239053,
      "loss": 1.6467,
      "step": 1790
    },
    {
      "epoch": 0.2876663070837828,
      "grad_norm": 3.0625,
      "learning_rate": 0.00017125279642058168,
      "loss": 2.1375,
      "step": 1800
    },
    {
      "epoch": 0.28926445323424826,
      "grad_norm": 1.53125,
      "learning_rate": 0.00017109300095877278,
      "loss": 1.4051,
      "step": 1810
    },
    {
      "epoch": 0.2908625993847137,
      "grad_norm": 2.953125,
      "learning_rate": 0.0001709332054969639,
      "loss": 1.9806,
      "step": 1820
    },
    {
      "epoch": 0.2924607455351792,
      "grad_norm": 2.15625,
      "learning_rate": 0.000170773410035155,
      "loss": 2.0099,
      "step": 1830
    },
    {
      "epoch": 0.29405889168564464,
      "grad_norm": 1.1328125,
      "learning_rate": 0.0001706136145733461,
      "loss": 1.9033,
      "step": 1840
    },
    {
      "epoch": 0.29565703783611014,
      "grad_norm": 3.796875,
      "learning_rate": 0.00017045381911153724,
      "loss": 1.4455,
      "step": 1850
    },
    {
      "epoch": 0.2972551839865756,
      "grad_norm": 1.0234375,
      "learning_rate": 0.00017029402364972834,
      "loss": 1.6005,
      "step": 1860
    },
    {
      "epoch": 0.298853330137041,
      "grad_norm": 2.125,
      "learning_rate": 0.00017013422818791947,
      "loss": 2.0389,
      "step": 1870
    },
    {
      "epoch": 0.3004514762875065,
      "grad_norm": 1.359375,
      "learning_rate": 0.00016997443272611057,
      "loss": 1.2241,
      "step": 1880
    },
    {
      "epoch": 0.30204962243797195,
      "grad_norm": 1.5390625,
      "learning_rate": 0.00016981463726430172,
      "loss": 1.8927,
      "step": 1890
    },
    {
      "epoch": 0.3036477685884374,
      "grad_norm": 1.9453125,
      "learning_rate": 0.00016965484180249282,
      "loss": 1.5213,
      "step": 1900
    },
    {
      "epoch": 0.3052459147389029,
      "grad_norm": 1.6875,
      "learning_rate": 0.00016949504634068395,
      "loss": 1.5428,
      "step": 1910
    },
    {
      "epoch": 0.30684406088936833,
      "grad_norm": 4.0625,
      "learning_rate": 0.00016933525087887505,
      "loss": 1.4468,
      "step": 1920
    },
    {
      "epoch": 0.30844220703983377,
      "grad_norm": 2.359375,
      "learning_rate": 0.00016917545541706615,
      "loss": 1.612,
      "step": 1930
    },
    {
      "epoch": 0.31004035319029927,
      "grad_norm": 2.53125,
      "learning_rate": 0.00016901565995525728,
      "loss": 1.0727,
      "step": 1940
    },
    {
      "epoch": 0.3116384993407647,
      "grad_norm": 2.703125,
      "learning_rate": 0.00016885586449344838,
      "loss": 1.8887,
      "step": 1950
    },
    {
      "epoch": 0.31323664549123015,
      "grad_norm": 1.0625,
      "learning_rate": 0.0001686960690316395,
      "loss": 1.5499,
      "step": 1960
    },
    {
      "epoch": 0.31483479164169564,
      "grad_norm": 3.671875,
      "learning_rate": 0.0001685362735698306,
      "loss": 1.964,
      "step": 1970
    },
    {
      "epoch": 0.3164329377921611,
      "grad_norm": 2.140625,
      "learning_rate": 0.00016837647810802176,
      "loss": 1.7408,
      "step": 1980
    },
    {
      "epoch": 0.3180310839426266,
      "grad_norm": 2.078125,
      "learning_rate": 0.00016821668264621286,
      "loss": 2.0446,
      "step": 1990
    },
    {
      "epoch": 0.319629230093092,
      "grad_norm": 2.625,
      "learning_rate": 0.000168056887184404,
      "loss": 1.7005,
      "step": 2000
    },
    {
      "epoch": 0.32122737624355746,
      "grad_norm": 2.0,
      "learning_rate": 0.0001678970917225951,
      "loss": 1.9176,
      "step": 2010
    },
    {
      "epoch": 0.32282552239402296,
      "grad_norm": 2.5,
      "learning_rate": 0.0001677372962607862,
      "loss": 1.8238,
      "step": 2020
    },
    {
      "epoch": 0.3244236685444884,
      "grad_norm": 2.109375,
      "learning_rate": 0.00016757750079897732,
      "loss": 1.732,
      "step": 2030
    },
    {
      "epoch": 0.32602181469495384,
      "grad_norm": 2.5,
      "learning_rate": 0.00016741770533716842,
      "loss": 1.8766,
      "step": 2040
    },
    {
      "epoch": 0.32761996084541933,
      "grad_norm": 0.75390625,
      "learning_rate": 0.00016725790987535955,
      "loss": 1.3787,
      "step": 2050
    },
    {
      "epoch": 0.3292181069958848,
      "grad_norm": 2.0,
      "learning_rate": 0.00016709811441355065,
      "loss": 1.3395,
      "step": 2060
    },
    {
      "epoch": 0.3308162531463502,
      "grad_norm": 1.484375,
      "learning_rate": 0.00016693831895174177,
      "loss": 1.5741,
      "step": 2070
    },
    {
      "epoch": 0.3324143992968157,
      "grad_norm": 0.0322265625,
      "learning_rate": 0.0001667785234899329,
      "loss": 1.6702,
      "step": 2080
    },
    {
      "epoch": 0.33401254544728115,
      "grad_norm": 0.126953125,
      "learning_rate": 0.00016661872802812403,
      "loss": 1.2356,
      "step": 2090
    },
    {
      "epoch": 0.3356106915977466,
      "grad_norm": 6.6875,
      "learning_rate": 0.00016645893256631513,
      "loss": 1.2828,
      "step": 2100
    },
    {
      "epoch": 0.3372088377482121,
      "grad_norm": 1.765625,
      "learning_rate": 0.00016629913710450623,
      "loss": 1.261,
      "step": 2110
    },
    {
      "epoch": 0.3388069838986775,
      "grad_norm": 1.9140625,
      "learning_rate": 0.00016613934164269736,
      "loss": 1.3731,
      "step": 2120
    },
    {
      "epoch": 0.34040513004914297,
      "grad_norm": 2.34375,
      "learning_rate": 0.00016597954618088846,
      "loss": 1.6501,
      "step": 2130
    },
    {
      "epoch": 0.34200327619960846,
      "grad_norm": 1.75,
      "learning_rate": 0.00016581975071907959,
      "loss": 1.7465,
      "step": 2140
    },
    {
      "epoch": 0.3436014223500739,
      "grad_norm": 3.375,
      "learning_rate": 0.00016565995525727069,
      "loss": 1.4603,
      "step": 2150
    },
    {
      "epoch": 0.3451995685005394,
      "grad_norm": 2.078125,
      "learning_rate": 0.0001655001597954618,
      "loss": 1.6318,
      "step": 2160
    },
    {
      "epoch": 0.34679771465100484,
      "grad_norm": 2.390625,
      "learning_rate": 0.00016534036433365294,
      "loss": 1.4717,
      "step": 2170
    },
    {
      "epoch": 0.3483958608014703,
      "grad_norm": 1.953125,
      "learning_rate": 0.00016518056887184407,
      "loss": 1.4684,
      "step": 2180
    },
    {
      "epoch": 0.3499940069519358,
      "grad_norm": 3.46875,
      "learning_rate": 0.00016502077341003517,
      "loss": 1.4361,
      "step": 2190
    },
    {
      "epoch": 0.3515921531024012,
      "grad_norm": 2.0625,
      "learning_rate": 0.00016486097794822627,
      "loss": 1.1149,
      "step": 2200
    },
    {
      "epoch": 0.35319029925286666,
      "grad_norm": 2.390625,
      "learning_rate": 0.0001647011824864174,
      "loss": 1.4269,
      "step": 2210
    },
    {
      "epoch": 0.35478844540333215,
      "grad_norm": 4.1875,
      "learning_rate": 0.0001645413870246085,
      "loss": 1.511,
      "step": 2220
    },
    {
      "epoch": 0.3563865915537976,
      "grad_norm": 7.5625,
      "learning_rate": 0.00016438159156279962,
      "loss": 1.4787,
      "step": 2230
    },
    {
      "epoch": 0.35798473770426303,
      "grad_norm": 2.40625,
      "learning_rate": 0.00016422179610099072,
      "loss": 1.6274,
      "step": 2240
    },
    {
      "epoch": 0.35958288385472853,
      "grad_norm": 3.4375,
      "learning_rate": 0.00016406200063918185,
      "loss": 1.5867,
      "step": 2250
    },
    {
      "epoch": 0.36118103000519397,
      "grad_norm": 1.578125,
      "learning_rate": 0.00016390220517737298,
      "loss": 1.1158,
      "step": 2260
    },
    {
      "epoch": 0.3627791761556594,
      "grad_norm": 1.140625,
      "learning_rate": 0.00016374240971556408,
      "loss": 2.03,
      "step": 2270
    },
    {
      "epoch": 0.3643773223061249,
      "grad_norm": 19.625,
      "learning_rate": 0.0001635826142537552,
      "loss": 1.8105,
      "step": 2280
    },
    {
      "epoch": 0.36597546845659035,
      "grad_norm": 1.6328125,
      "learning_rate": 0.0001634228187919463,
      "loss": 1.764,
      "step": 2290
    },
    {
      "epoch": 0.36757361460705584,
      "grad_norm": 3.5,
      "learning_rate": 0.00016326302333013744,
      "loss": 1.5101,
      "step": 2300
    },
    {
      "epoch": 0.3691717607575213,
      "grad_norm": 2.984375,
      "learning_rate": 0.00016310322786832854,
      "loss": 1.8845,
      "step": 2310
    },
    {
      "epoch": 0.3707699069079867,
      "grad_norm": 2.59375,
      "learning_rate": 0.00016294343240651966,
      "loss": 1.5059,
      "step": 2320
    },
    {
      "epoch": 0.3723680530584522,
      "grad_norm": 2.4375,
      "learning_rate": 0.00016278363694471076,
      "loss": 1.5114,
      "step": 2330
    },
    {
      "epoch": 0.37396619920891766,
      "grad_norm": 4.59375,
      "learning_rate": 0.0001626238414829019,
      "loss": 1.166,
      "step": 2340
    },
    {
      "epoch": 0.3755643453593831,
      "grad_norm": 2.5,
      "learning_rate": 0.00016246404602109302,
      "loss": 1.6926,
      "step": 2350
    },
    {
      "epoch": 0.3771624915098486,
      "grad_norm": 1.9453125,
      "learning_rate": 0.00016230425055928412,
      "loss": 1.5168,
      "step": 2360
    },
    {
      "epoch": 0.37876063766031404,
      "grad_norm": 1.0859375,
      "learning_rate": 0.00016214445509747525,
      "loss": 1.8462,
      "step": 2370
    },
    {
      "epoch": 0.3803587838107795,
      "grad_norm": 3.671875,
      "learning_rate": 0.00016198465963566635,
      "loss": 1.5626,
      "step": 2380
    },
    {
      "epoch": 0.381956929961245,
      "grad_norm": 2.359375,
      "learning_rate": 0.00016182486417385747,
      "loss": 1.2412,
      "step": 2390
    },
    {
      "epoch": 0.3835550761117104,
      "grad_norm": 2.828125,
      "learning_rate": 0.00016166506871204857,
      "loss": 1.3983,
      "step": 2400
    },
    {
      "epoch": 0.38515322226217585,
      "grad_norm": 2.53125,
      "learning_rate": 0.0001615052732502397,
      "loss": 1.6164,
      "step": 2410
    },
    {
      "epoch": 0.38675136841264135,
      "grad_norm": 1.0078125,
      "learning_rate": 0.0001613454777884308,
      "loss": 1.356,
      "step": 2420
    },
    {
      "epoch": 0.3883495145631068,
      "grad_norm": 1.5390625,
      "learning_rate": 0.00016118568232662193,
      "loss": 1.2627,
      "step": 2430
    },
    {
      "epoch": 0.38994766071357223,
      "grad_norm": 1.0859375,
      "learning_rate": 0.00016102588686481306,
      "loss": 1.5193,
      "step": 2440
    },
    {
      "epoch": 0.3915458068640377,
      "grad_norm": 2.390625,
      "learning_rate": 0.00016086609140300416,
      "loss": 1.6311,
      "step": 2450
    },
    {
      "epoch": 0.39314395301450317,
      "grad_norm": 3.015625,
      "learning_rate": 0.00016070629594119529,
      "loss": 1.4296,
      "step": 2460
    },
    {
      "epoch": 0.39474209916496866,
      "grad_norm": 1.59375,
      "learning_rate": 0.00016054650047938639,
      "loss": 1.4856,
      "step": 2470
    },
    {
      "epoch": 0.3963402453154341,
      "grad_norm": 2.484375,
      "learning_rate": 0.0001603867050175775,
      "loss": 1.4827,
      "step": 2480
    },
    {
      "epoch": 0.39793839146589954,
      "grad_norm": 2.046875,
      "learning_rate": 0.00016022690955576861,
      "loss": 1.3243,
      "step": 2490
    },
    {
      "epoch": 0.39953653761636504,
      "grad_norm": 1.328125,
      "learning_rate": 0.00016006711409395974,
      "loss": 1.3831,
      "step": 2500
    },
    {
      "epoch": 0.4011346837668305,
      "grad_norm": 2.828125,
      "learning_rate": 0.00015990731863215084,
      "loss": 1.5807,
      "step": 2510
    },
    {
      "epoch": 0.4027328299172959,
      "grad_norm": 3.28125,
      "learning_rate": 0.00015974752317034197,
      "loss": 1.1394,
      "step": 2520
    },
    {
      "epoch": 0.4043309760677614,
      "grad_norm": 4.96875,
      "learning_rate": 0.00015958772770853307,
      "loss": 1.8543,
      "step": 2530
    },
    {
      "epoch": 0.40592912221822686,
      "grad_norm": 2.015625,
      "learning_rate": 0.0001594279322467242,
      "loss": 1.6932,
      "step": 2540
    },
    {
      "epoch": 0.4075272683686923,
      "grad_norm": 3.28125,
      "learning_rate": 0.00015926813678491532,
      "loss": 1.6494,
      "step": 2550
    },
    {
      "epoch": 0.4091254145191578,
      "grad_norm": 1.578125,
      "learning_rate": 0.00015910834132310642,
      "loss": 1.93,
      "step": 2560
    },
    {
      "epoch": 0.41072356066962323,
      "grad_norm": 2.375,
      "learning_rate": 0.00015894854586129755,
      "loss": 1.9706,
      "step": 2570
    },
    {
      "epoch": 0.4123217068200887,
      "grad_norm": 2.5,
      "learning_rate": 0.00015878875039948865,
      "loss": 1.7186,
      "step": 2580
    },
    {
      "epoch": 0.41391985297055417,
      "grad_norm": 4.09375,
      "learning_rate": 0.00015862895493767978,
      "loss": 1.9415,
      "step": 2590
    },
    {
      "epoch": 0.4155179991210196,
      "grad_norm": 2.03125,
      "learning_rate": 0.00015846915947587088,
      "loss": 1.7564,
      "step": 2600
    },
    {
      "epoch": 0.41711614527148505,
      "grad_norm": 2.703125,
      "learning_rate": 0.000158309364014062,
      "loss": 1.6346,
      "step": 2610
    },
    {
      "epoch": 0.41871429142195055,
      "grad_norm": 3.78125,
      "learning_rate": 0.0001581495685522531,
      "loss": 1.6397,
      "step": 2620
    },
    {
      "epoch": 0.420312437572416,
      "grad_norm": 2.078125,
      "learning_rate": 0.00015798977309044424,
      "loss": 1.6272,
      "step": 2630
    },
    {
      "epoch": 0.4219105837228815,
      "grad_norm": 2.21875,
      "learning_rate": 0.00015782997762863536,
      "loss": 1.457,
      "step": 2640
    },
    {
      "epoch": 0.4235087298733469,
      "grad_norm": 1.6796875,
      "learning_rate": 0.00015767018216682646,
      "loss": 1.4534,
      "step": 2650
    },
    {
      "epoch": 0.42510687602381236,
      "grad_norm": 2.53125,
      "learning_rate": 0.0001575103867050176,
      "loss": 1.3918,
      "step": 2660
    },
    {
      "epoch": 0.42670502217427786,
      "grad_norm": 1.7734375,
      "learning_rate": 0.0001573505912432087,
      "loss": 1.8408,
      "step": 2670
    },
    {
      "epoch": 0.4283031683247433,
      "grad_norm": 2.578125,
      "learning_rate": 0.00015719079578139982,
      "loss": 1.4991,
      "step": 2680
    },
    {
      "epoch": 0.42990131447520874,
      "grad_norm": 1.9921875,
      "learning_rate": 0.00015703100031959092,
      "loss": 1.6836,
      "step": 2690
    },
    {
      "epoch": 0.43149946062567424,
      "grad_norm": 3.328125,
      "learning_rate": 0.00015687120485778205,
      "loss": 1.4508,
      "step": 2700
    },
    {
      "epoch": 0.4330976067761397,
      "grad_norm": 1.703125,
      "learning_rate": 0.00015671140939597315,
      "loss": 1.1034,
      "step": 2710
    },
    {
      "epoch": 0.4346957529266051,
      "grad_norm": 1.640625,
      "learning_rate": 0.00015655161393416427,
      "loss": 1.4901,
      "step": 2720
    },
    {
      "epoch": 0.4362938990770706,
      "grad_norm": 4.15625,
      "learning_rate": 0.0001563918184723554,
      "loss": 1.4284,
      "step": 2730
    },
    {
      "epoch": 0.43789204522753605,
      "grad_norm": 0.166015625,
      "learning_rate": 0.0001562320230105465,
      "loss": 1.5094,
      "step": 2740
    },
    {
      "epoch": 0.4394901913780015,
      "grad_norm": 2.484375,
      "learning_rate": 0.00015607222754873763,
      "loss": 1.3927,
      "step": 2750
    },
    {
      "epoch": 0.441088337528467,
      "grad_norm": 1.796875,
      "learning_rate": 0.00015591243208692873,
      "loss": 1.5376,
      "step": 2760
    },
    {
      "epoch": 0.44268648367893243,
      "grad_norm": 3.296875,
      "learning_rate": 0.00015575263662511986,
      "loss": 1.7772,
      "step": 2770
    },
    {
      "epoch": 0.4442846298293979,
      "grad_norm": 1.6484375,
      "learning_rate": 0.00015559284116331096,
      "loss": 1.5555,
      "step": 2780
    },
    {
      "epoch": 0.44588277597986337,
      "grad_norm": 1.640625,
      "learning_rate": 0.00015543304570150209,
      "loss": 1.4213,
      "step": 2790
    },
    {
      "epoch": 0.4474809221303288,
      "grad_norm": 1.2109375,
      "learning_rate": 0.00015527325023969319,
      "loss": 1.5532,
      "step": 2800
    },
    {
      "epoch": 0.4490790682807943,
      "grad_norm": 2.234375,
      "learning_rate": 0.00015511345477788431,
      "loss": 1.942,
      "step": 2810
    },
    {
      "epoch": 0.45067721443125974,
      "grad_norm": 0.0015869140625,
      "learning_rate": 0.00015495365931607544,
      "loss": 1.8719,
      "step": 2820
    },
    {
      "epoch": 0.4522753605817252,
      "grad_norm": 1.078125,
      "learning_rate": 0.00015479386385426654,
      "loss": 1.4412,
      "step": 2830
    },
    {
      "epoch": 0.4538735067321907,
      "grad_norm": 2.8125,
      "learning_rate": 0.00015463406839245767,
      "loss": 1.6924,
      "step": 2840
    },
    {
      "epoch": 0.4554716528826561,
      "grad_norm": 1.515625,
      "learning_rate": 0.00015447427293064877,
      "loss": 1.3947,
      "step": 2850
    },
    {
      "epoch": 0.45706979903312156,
      "grad_norm": 1.1328125,
      "learning_rate": 0.0001543144774688399,
      "loss": 1.5041,
      "step": 2860
    },
    {
      "epoch": 0.45866794518358706,
      "grad_norm": 1.640625,
      "learning_rate": 0.000154154682007031,
      "loss": 1.6133,
      "step": 2870
    },
    {
      "epoch": 0.4602660913340525,
      "grad_norm": 2.40625,
      "learning_rate": 0.00015399488654522212,
      "loss": 1.6308,
      "step": 2880
    },
    {
      "epoch": 0.46186423748451794,
      "grad_norm": 1.828125,
      "learning_rate": 0.00015383509108341323,
      "loss": 1.3761,
      "step": 2890
    },
    {
      "epoch": 0.46346238363498343,
      "grad_norm": 3.09375,
      "learning_rate": 0.00015367529562160435,
      "loss": 1.4932,
      "step": 2900
    },
    {
      "epoch": 0.4650605297854489,
      "grad_norm": 1.625,
      "learning_rate": 0.00015351550015979548,
      "loss": 1.355,
      "step": 2910
    },
    {
      "epoch": 0.4666586759359143,
      "grad_norm": 3.421875,
      "learning_rate": 0.00015335570469798658,
      "loss": 1.3305,
      "step": 2920
    },
    {
      "epoch": 0.4682568220863798,
      "grad_norm": 1.078125,
      "learning_rate": 0.0001531959092361777,
      "loss": 1.3031,
      "step": 2930
    },
    {
      "epoch": 0.46985496823684525,
      "grad_norm": 1.7421875,
      "learning_rate": 0.0001530361137743688,
      "loss": 1.3251,
      "step": 2940
    },
    {
      "epoch": 0.47145311438731075,
      "grad_norm": 2.390625,
      "learning_rate": 0.00015287631831255994,
      "loss": 1.2274,
      "step": 2950
    },
    {
      "epoch": 0.4730512605377762,
      "grad_norm": 1.40625,
      "learning_rate": 0.00015271652285075104,
      "loss": 1.4891,
      "step": 2960
    },
    {
      "epoch": 0.4746494066882416,
      "grad_norm": 1.8203125,
      "learning_rate": 0.00015255672738894216,
      "loss": 1.9524,
      "step": 2970
    },
    {
      "epoch": 0.4762475528387071,
      "grad_norm": 2.390625,
      "learning_rate": 0.00015239693192713326,
      "loss": 1.5629,
      "step": 2980
    },
    {
      "epoch": 0.47784569898917256,
      "grad_norm": 2.046875,
      "learning_rate": 0.0001522371364653244,
      "loss": 1.0223,
      "step": 2990
    },
    {
      "epoch": 0.479443845139638,
      "grad_norm": 3.015625,
      "learning_rate": 0.00015207734100351552,
      "loss": 1.6912,
      "step": 3000
    },
    {
      "epoch": 0.4810419912901035,
      "grad_norm": 0.00086212158203125,
      "learning_rate": 0.00015191754554170662,
      "loss": 1.0105,
      "step": 3010
    },
    {
      "epoch": 0.48264013744056894,
      "grad_norm": 3.953125,
      "learning_rate": 0.00015175775007989775,
      "loss": 1.5178,
      "step": 3020
    },
    {
      "epoch": 0.4842382835910344,
      "grad_norm": 3.96875,
      "learning_rate": 0.00015159795461808885,
      "loss": 1.8766,
      "step": 3030
    },
    {
      "epoch": 0.4858364297414999,
      "grad_norm": 1.0390625,
      "learning_rate": 0.00015143815915627997,
      "loss": 1.4223,
      "step": 3040
    },
    {
      "epoch": 0.4874345758919653,
      "grad_norm": 2.640625,
      "learning_rate": 0.00015127836369447108,
      "loss": 1.7105,
      "step": 3050
    },
    {
      "epoch": 0.48903272204243076,
      "grad_norm": 3.390625,
      "learning_rate": 0.0001511185682326622,
      "loss": 1.8077,
      "step": 3060
    },
    {
      "epoch": 0.49063086819289625,
      "grad_norm": 3.3125,
      "learning_rate": 0.0001509587727708533,
      "loss": 1.4634,
      "step": 3070
    },
    {
      "epoch": 0.4922290143433617,
      "grad_norm": 2.265625,
      "learning_rate": 0.00015079897730904443,
      "loss": 1.6833,
      "step": 3080
    },
    {
      "epoch": 0.49382716049382713,
      "grad_norm": 1.7734375,
      "learning_rate": 0.00015063918184723556,
      "loss": 1.7325,
      "step": 3090
    },
    {
      "epoch": 0.49542530664429263,
      "grad_norm": 1.0078125,
      "learning_rate": 0.00015047938638542666,
      "loss": 1.33,
      "step": 3100
    },
    {
      "epoch": 0.49702345279475807,
      "grad_norm": 2.046875,
      "learning_rate": 0.00015031959092361779,
      "loss": 1.8379,
      "step": 3110
    },
    {
      "epoch": 0.49862159894522357,
      "grad_norm": 3.296875,
      "learning_rate": 0.0001501597954618089,
      "loss": 1.457,
      "step": 3120
    },
    {
      "epoch": 0.500219745095689,
      "grad_norm": 7.375,
      "learning_rate": 0.00015000000000000001,
      "loss": 2.1272,
      "step": 3130
    },
    {
      "epoch": 0.5018178912461545,
      "grad_norm": 2.328125,
      "learning_rate": 0.00014984020453819111,
      "loss": 1.6602,
      "step": 3140
    },
    {
      "epoch": 0.5034160373966199,
      "grad_norm": 3.1875,
      "learning_rate": 0.00014968040907638224,
      "loss": 1.3684,
      "step": 3150
    },
    {
      "epoch": 0.5050141835470854,
      "grad_norm": 2.171875,
      "learning_rate": 0.00014952061361457334,
      "loss": 1.8264,
      "step": 3160
    },
    {
      "epoch": 0.5066123296975509,
      "grad_norm": 2.234375,
      "learning_rate": 0.00014936081815276447,
      "loss": 1.3049,
      "step": 3170
    },
    {
      "epoch": 0.5082104758480163,
      "grad_norm": 2.46875,
      "learning_rate": 0.0001492010226909556,
      "loss": 1.7029,
      "step": 3180
    },
    {
      "epoch": 0.5098086219984818,
      "grad_norm": 2.84375,
      "learning_rate": 0.0001490412272291467,
      "loss": 1.5195,
      "step": 3190
    },
    {
      "epoch": 0.5114067681489473,
      "grad_norm": 23.375,
      "learning_rate": 0.00014888143176733783,
      "loss": 1.7022,
      "step": 3200
    },
    {
      "epoch": 0.5130049142994126,
      "grad_norm": 0.00057220458984375,
      "learning_rate": 0.00014872163630552893,
      "loss": 1.4356,
      "step": 3210
    },
    {
      "epoch": 0.5146030604498781,
      "grad_norm": 2.390625,
      "learning_rate": 0.00014856184084372005,
      "loss": 1.6009,
      "step": 3220
    },
    {
      "epoch": 0.5162012066003436,
      "grad_norm": 1.0859375,
      "learning_rate": 0.00014840204538191115,
      "loss": 1.4249,
      "step": 3230
    },
    {
      "epoch": 0.517799352750809,
      "grad_norm": 0.0009307861328125,
      "learning_rate": 0.00014824224992010228,
      "loss": 1.3033,
      "step": 3240
    },
    {
      "epoch": 0.5193974989012745,
      "grad_norm": 1.03125,
      "learning_rate": 0.00014808245445829338,
      "loss": 1.712,
      "step": 3250
    },
    {
      "epoch": 0.52099564505174,
      "grad_norm": 2.421875,
      "learning_rate": 0.0001479226589964845,
      "loss": 1.5313,
      "step": 3260
    },
    {
      "epoch": 0.5225937912022054,
      "grad_norm": 3.78125,
      "learning_rate": 0.00014776286353467564,
      "loss": 1.806,
      "step": 3270
    },
    {
      "epoch": 0.5241919373526709,
      "grad_norm": 3.359375,
      "learning_rate": 0.00014760306807286674,
      "loss": 1.3634,
      "step": 3280
    },
    {
      "epoch": 0.5257900835031364,
      "grad_norm": 1.1171875,
      "learning_rate": 0.00014744327261105786,
      "loss": 1.261,
      "step": 3290
    },
    {
      "epoch": 0.5273882296536018,
      "grad_norm": 1.6953125,
      "learning_rate": 0.00014728347714924896,
      "loss": 1.4812,
      "step": 3300
    },
    {
      "epoch": 0.5289863758040673,
      "grad_norm": 2.890625,
      "learning_rate": 0.0001471236816874401,
      "loss": 1.2288,
      "step": 3310
    },
    {
      "epoch": 0.5305845219545328,
      "grad_norm": 2.875,
      "learning_rate": 0.0001469638862256312,
      "loss": 1.8548,
      "step": 3320
    },
    {
      "epoch": 0.5321826681049981,
      "grad_norm": 1.2890625,
      "learning_rate": 0.00014680409076382232,
      "loss": 1.1851,
      "step": 3330
    },
    {
      "epoch": 0.5337808142554636,
      "grad_norm": 2.421875,
      "learning_rate": 0.00014664429530201342,
      "loss": 1.893,
      "step": 3340
    },
    {
      "epoch": 0.5353789604059291,
      "grad_norm": 1.625,
      "learning_rate": 0.00014648449984020455,
      "loss": 1.4989,
      "step": 3350
    },
    {
      "epoch": 0.5369771065563946,
      "grad_norm": 3.46875,
      "learning_rate": 0.00014632470437839565,
      "loss": 1.7393,
      "step": 3360
    },
    {
      "epoch": 0.53857525270686,
      "grad_norm": 2.15625,
      "learning_rate": 0.00014616490891658678,
      "loss": 1.349,
      "step": 3370
    },
    {
      "epoch": 0.5401733988573255,
      "grad_norm": 6.28125,
      "learning_rate": 0.0001460051134547779,
      "loss": 1.5423,
      "step": 3380
    },
    {
      "epoch": 0.541771545007791,
      "grad_norm": 3.28125,
      "learning_rate": 0.000145845317992969,
      "loss": 1.2666,
      "step": 3390
    },
    {
      "epoch": 0.5433696911582564,
      "grad_norm": 2.5,
      "learning_rate": 0.00014568552253116013,
      "loss": 1.4583,
      "step": 3400
    },
    {
      "epoch": 0.5449678373087219,
      "grad_norm": 1.078125,
      "learning_rate": 0.00014552572706935123,
      "loss": 1.2522,
      "step": 3410
    },
    {
      "epoch": 0.5465659834591874,
      "grad_norm": 1.65625,
      "learning_rate": 0.00014536593160754236,
      "loss": 1.3403,
      "step": 3420
    },
    {
      "epoch": 0.5481641296096528,
      "grad_norm": 1.203125,
      "learning_rate": 0.00014520613614573346,
      "loss": 1.4802,
      "step": 3430
    },
    {
      "epoch": 0.5497622757601183,
      "grad_norm": 2.390625,
      "learning_rate": 0.0001450463406839246,
      "loss": 1.4811,
      "step": 3440
    },
    {
      "epoch": 0.5513604219105838,
      "grad_norm": 1.046875,
      "learning_rate": 0.0001448865452221157,
      "loss": 1.7016,
      "step": 3450
    },
    {
      "epoch": 0.5529585680610491,
      "grad_norm": 9.125,
      "learning_rate": 0.00014472674976030681,
      "loss": 1.8849,
      "step": 3460
    },
    {
      "epoch": 0.5545567142115146,
      "grad_norm": 1.953125,
      "learning_rate": 0.00014456695429849794,
      "loss": 1.6001,
      "step": 3470
    },
    {
      "epoch": 0.5561548603619801,
      "grad_norm": 2.6875,
      "learning_rate": 0.00014440715883668904,
      "loss": 1.9021,
      "step": 3480
    },
    {
      "epoch": 0.5577530065124455,
      "grad_norm": 3.953125,
      "learning_rate": 0.00014424736337488017,
      "loss": 1.633,
      "step": 3490
    },
    {
      "epoch": 0.559351152662911,
      "grad_norm": 1.953125,
      "learning_rate": 0.00014408756791307127,
      "loss": 1.6012,
      "step": 3500
    },
    {
      "epoch": 0.5609492988133765,
      "grad_norm": 3.75,
      "learning_rate": 0.0001439277724512624,
      "loss": 1.2181,
      "step": 3510
    },
    {
      "epoch": 0.5625474449638419,
      "grad_norm": 3.03125,
      "learning_rate": 0.0001437679769894535,
      "loss": 1.5653,
      "step": 3520
    },
    {
      "epoch": 0.5641455911143074,
      "grad_norm": 1.6796875,
      "learning_rate": 0.00014360818152764463,
      "loss": 1.8838,
      "step": 3530
    },
    {
      "epoch": 0.5657437372647729,
      "grad_norm": 1.1484375,
      "learning_rate": 0.00014344838606583573,
      "loss": 1.3047,
      "step": 3540
    },
    {
      "epoch": 0.5673418834152383,
      "grad_norm": 2.6875,
      "learning_rate": 0.00014328859060402685,
      "loss": 1.8701,
      "step": 3550
    },
    {
      "epoch": 0.5689400295657038,
      "grad_norm": 4.3125,
      "learning_rate": 0.00014312879514221798,
      "loss": 1.375,
      "step": 3560
    },
    {
      "epoch": 0.5705381757161693,
      "grad_norm": 0.37890625,
      "learning_rate": 0.00014296899968040908,
      "loss": 1.1556,
      "step": 3570
    },
    {
      "epoch": 0.5721363218666347,
      "grad_norm": 15.0,
      "learning_rate": 0.0001428092042186002,
      "loss": 1.2096,
      "step": 3580
    },
    {
      "epoch": 0.5737344680171002,
      "grad_norm": 1.328125,
      "learning_rate": 0.0001426494087567913,
      "loss": 1.9778,
      "step": 3590
    },
    {
      "epoch": 0.5753326141675656,
      "grad_norm": 1.9765625,
      "learning_rate": 0.00014248961329498244,
      "loss": 1.7072,
      "step": 3600
    },
    {
      "epoch": 0.576930760318031,
      "grad_norm": 1.84375,
      "learning_rate": 0.00014232981783317354,
      "loss": 1.5932,
      "step": 3610
    },
    {
      "epoch": 0.5785289064684965,
      "grad_norm": 1.6484375,
      "learning_rate": 0.00014217002237136466,
      "loss": 1.5572,
      "step": 3620
    },
    {
      "epoch": 0.580127052618962,
      "grad_norm": 1.1875,
      "learning_rate": 0.00014201022690955576,
      "loss": 0.9803,
      "step": 3630
    },
    {
      "epoch": 0.5817251987694274,
      "grad_norm": 7.96875,
      "learning_rate": 0.0001418504314477469,
      "loss": 1.7647,
      "step": 3640
    },
    {
      "epoch": 0.5833233449198929,
      "grad_norm": 1.7734375,
      "learning_rate": 0.00014169063598593802,
      "loss": 1.13,
      "step": 3650
    },
    {
      "epoch": 0.5849214910703584,
      "grad_norm": 2.140625,
      "learning_rate": 0.00014153084052412912,
      "loss": 1.3611,
      "step": 3660
    },
    {
      "epoch": 0.5865196372208239,
      "grad_norm": 2.421875,
      "learning_rate": 0.00014137104506232025,
      "loss": 1.218,
      "step": 3670
    },
    {
      "epoch": 0.5881177833712893,
      "grad_norm": 2.265625,
      "learning_rate": 0.00014121124960051135,
      "loss": 2.072,
      "step": 3680
    },
    {
      "epoch": 0.5897159295217548,
      "grad_norm": 1.96875,
      "learning_rate": 0.00014105145413870248,
      "loss": 1.5753,
      "step": 3690
    },
    {
      "epoch": 0.5913140756722203,
      "grad_norm": 1.9765625,
      "learning_rate": 0.00014089165867689358,
      "loss": 1.9954,
      "step": 3700
    },
    {
      "epoch": 0.5929122218226857,
      "grad_norm": 1.2109375,
      "learning_rate": 0.0001407318632150847,
      "loss": 1.046,
      "step": 3710
    },
    {
      "epoch": 0.5945103679731512,
      "grad_norm": 3.015625,
      "learning_rate": 0.0001405720677532758,
      "loss": 1.604,
      "step": 3720
    },
    {
      "epoch": 0.5961085141236167,
      "grad_norm": 1.8828125,
      "learning_rate": 0.00014041227229146693,
      "loss": 1.831,
      "step": 3730
    },
    {
      "epoch": 0.597706660274082,
      "grad_norm": 1.75,
      "learning_rate": 0.00014025247682965806,
      "loss": 1.4394,
      "step": 3740
    },
    {
      "epoch": 0.5993048064245475,
      "grad_norm": 6.34375,
      "learning_rate": 0.00014009268136784916,
      "loss": 1.3765,
      "step": 3750
    },
    {
      "epoch": 0.600902952575013,
      "grad_norm": 1.4140625,
      "learning_rate": 0.0001399328859060403,
      "loss": 1.5853,
      "step": 3760
    },
    {
      "epoch": 0.6025010987254784,
      "grad_norm": 0.73046875,
      "learning_rate": 0.0001397730904442314,
      "loss": 1.1573,
      "step": 3770
    },
    {
      "epoch": 0.6040992448759439,
      "grad_norm": 1.203125,
      "learning_rate": 0.00013961329498242251,
      "loss": 1.5976,
      "step": 3780
    },
    {
      "epoch": 0.6056973910264094,
      "grad_norm": 1.96875,
      "learning_rate": 0.00013945349952061362,
      "loss": 1.7512,
      "step": 3790
    },
    {
      "epoch": 0.6072955371768748,
      "grad_norm": 4.4375,
      "learning_rate": 0.00013929370405880474,
      "loss": 1.9683,
      "step": 3800
    },
    {
      "epoch": 0.6088936833273403,
      "grad_norm": 1.90625,
      "learning_rate": 0.00013913390859699584,
      "loss": 1.2043,
      "step": 3810
    },
    {
      "epoch": 0.6104918294778058,
      "grad_norm": 1.1640625,
      "learning_rate": 0.00013897411313518694,
      "loss": 1.4278,
      "step": 3820
    },
    {
      "epoch": 0.6120899756282712,
      "grad_norm": 2.703125,
      "learning_rate": 0.0001388143176733781,
      "loss": 1.299,
      "step": 3830
    },
    {
      "epoch": 0.6136881217787367,
      "grad_norm": 1.9375,
      "learning_rate": 0.0001386545222115692,
      "loss": 1.4345,
      "step": 3840
    },
    {
      "epoch": 0.6152862679292022,
      "grad_norm": 3.09375,
      "learning_rate": 0.00013849472674976033,
      "loss": 1.3257,
      "step": 3850
    },
    {
      "epoch": 0.6168844140796675,
      "grad_norm": 1.0546875,
      "learning_rate": 0.00013833493128795143,
      "loss": 1.2703,
      "step": 3860
    },
    {
      "epoch": 0.618482560230133,
      "grad_norm": 1.53125,
      "learning_rate": 0.00013817513582614255,
      "loss": 1.4669,
      "step": 3870
    },
    {
      "epoch": 0.6200807063805985,
      "grad_norm": 1.59375,
      "learning_rate": 0.00013801534036433365,
      "loss": 1.6759,
      "step": 3880
    },
    {
      "epoch": 0.6216788525310639,
      "grad_norm": 2.453125,
      "learning_rate": 0.00013785554490252478,
      "loss": 1.5458,
      "step": 3890
    },
    {
      "epoch": 0.6232769986815294,
      "grad_norm": 4.71875,
      "learning_rate": 0.00013769574944071588,
      "loss": 1.3871,
      "step": 3900
    },
    {
      "epoch": 0.6248751448319949,
      "grad_norm": 0.9296875,
      "learning_rate": 0.00013753595397890698,
      "loss": 1.6457,
      "step": 3910
    },
    {
      "epoch": 0.6264732909824603,
      "grad_norm": 2.046875,
      "learning_rate": 0.00013737615851709814,
      "loss": 1.6328,
      "step": 3920
    },
    {
      "epoch": 0.6280714371329258,
      "grad_norm": 1.453125,
      "learning_rate": 0.00013721636305528924,
      "loss": 1.4085,
      "step": 3930
    },
    {
      "epoch": 0.6296695832833913,
      "grad_norm": 0.0001354217529296875,
      "learning_rate": 0.00013705656759348036,
      "loss": 1.5949,
      "step": 3940
    },
    {
      "epoch": 0.6312677294338567,
      "grad_norm": 0.000156402587890625,
      "learning_rate": 0.00013689677213167147,
      "loss": 1.3437,
      "step": 3950
    },
    {
      "epoch": 0.6328658755843222,
      "grad_norm": 2.453125,
      "learning_rate": 0.0001367369766698626,
      "loss": 1.9979,
      "step": 3960
    },
    {
      "epoch": 0.6344640217347877,
      "grad_norm": 1.953125,
      "learning_rate": 0.0001365771812080537,
      "loss": 1.3413,
      "step": 3970
    },
    {
      "epoch": 0.6360621678852532,
      "grad_norm": 1.734375,
      "learning_rate": 0.00013641738574624482,
      "loss": 1.415,
      "step": 3980
    },
    {
      "epoch": 0.6376603140357185,
      "grad_norm": 2.453125,
      "learning_rate": 0.00013625759028443592,
      "loss": 1.3927,
      "step": 3990
    },
    {
      "epoch": 0.639258460186184,
      "grad_norm": 3.46875,
      "learning_rate": 0.00013609779482262702,
      "loss": 1.3965,
      "step": 4000
    },
    {
      "epoch": 0.6408566063366495,
      "grad_norm": 2.03125,
      "learning_rate": 0.00013593799936081818,
      "loss": 1.5693,
      "step": 4010
    },
    {
      "epoch": 0.6424547524871149,
      "grad_norm": 2.3125,
      "learning_rate": 0.00013577820389900928,
      "loss": 1.9686,
      "step": 4020
    },
    {
      "epoch": 0.6440528986375804,
      "grad_norm": 1.921875,
      "learning_rate": 0.0001356184084372004,
      "loss": 1.2836,
      "step": 4030
    },
    {
      "epoch": 0.6456510447880459,
      "grad_norm": 2.390625,
      "learning_rate": 0.0001354586129753915,
      "loss": 1.5027,
      "step": 4040
    },
    {
      "epoch": 0.6472491909385113,
      "grad_norm": 1.5390625,
      "learning_rate": 0.00013529881751358263,
      "loss": 1.232,
      "step": 4050
    },
    {
      "epoch": 0.6488473370889768,
      "grad_norm": 3.109375,
      "learning_rate": 0.00013513902205177373,
      "loss": 1.608,
      "step": 4060
    },
    {
      "epoch": 0.6504454832394423,
      "grad_norm": 3.03125,
      "learning_rate": 0.00013497922658996486,
      "loss": 1.7837,
      "step": 4070
    },
    {
      "epoch": 0.6520436293899077,
      "grad_norm": 3.6875,
      "learning_rate": 0.00013481943112815596,
      "loss": 1.2332,
      "step": 4080
    },
    {
      "epoch": 0.6536417755403732,
      "grad_norm": 4.4375,
      "learning_rate": 0.00013465963566634706,
      "loss": 1.7761,
      "step": 4090
    },
    {
      "epoch": 0.6552399216908387,
      "grad_norm": 1.484375,
      "learning_rate": 0.00013449984020453821,
      "loss": 1.4564,
      "step": 4100
    },
    {
      "epoch": 0.656838067841304,
      "grad_norm": 3.765625,
      "learning_rate": 0.00013434004474272932,
      "loss": 1.6869,
      "step": 4110
    },
    {
      "epoch": 0.6584362139917695,
      "grad_norm": 1.6015625,
      "learning_rate": 0.00013418024928092044,
      "loss": 1.6799,
      "step": 4120
    },
    {
      "epoch": 0.660034360142235,
      "grad_norm": 1.828125,
      "learning_rate": 0.00013402045381911154,
      "loss": 1.1441,
      "step": 4130
    },
    {
      "epoch": 0.6616325062927004,
      "grad_norm": 1.96875,
      "learning_rate": 0.00013386065835730267,
      "loss": 1.4481,
      "step": 4140
    },
    {
      "epoch": 0.6632306524431659,
      "grad_norm": 2.984375,
      "learning_rate": 0.00013370086289549377,
      "loss": 1.8917,
      "step": 4150
    },
    {
      "epoch": 0.6648287985936314,
      "grad_norm": 2.890625,
      "learning_rate": 0.0001335410674336849,
      "loss": 1.2737,
      "step": 4160
    },
    {
      "epoch": 0.6664269447440968,
      "grad_norm": 1.8125,
      "learning_rate": 0.000133381271971876,
      "loss": 1.9844,
      "step": 4170
    },
    {
      "epoch": 0.6680250908945623,
      "grad_norm": 2.546875,
      "learning_rate": 0.0001332214765100671,
      "loss": 1.6527,
      "step": 4180
    },
    {
      "epoch": 0.6696232370450278,
      "grad_norm": 4.65625,
      "learning_rate": 0.00013306168104825825,
      "loss": 2.0327,
      "step": 4190
    },
    {
      "epoch": 0.6712213831954932,
      "grad_norm": 1.5234375,
      "learning_rate": 0.00013290188558644935,
      "loss": 1.7536,
      "step": 4200
    },
    {
      "epoch": 0.6728195293459587,
      "grad_norm": 1.609375,
      "learning_rate": 0.00013274209012464048,
      "loss": 1.6662,
      "step": 4210
    },
    {
      "epoch": 0.6744176754964242,
      "grad_norm": 1.703125,
      "learning_rate": 0.00013258229466283158,
      "loss": 1.3883,
      "step": 4220
    },
    {
      "epoch": 0.6760158216468896,
      "grad_norm": 1.6953125,
      "learning_rate": 0.0001324224992010227,
      "loss": 1.6496,
      "step": 4230
    },
    {
      "epoch": 0.677613967797355,
      "grad_norm": 1.8203125,
      "learning_rate": 0.0001322627037392138,
      "loss": 1.7442,
      "step": 4240
    },
    {
      "epoch": 0.6792121139478206,
      "grad_norm": 1.703125,
      "learning_rate": 0.00013210290827740494,
      "loss": 1.3212,
      "step": 4250
    },
    {
      "epoch": 0.6808102600982859,
      "grad_norm": 1.0859375,
      "learning_rate": 0.00013194311281559604,
      "loss": 1.1668,
      "step": 4260
    },
    {
      "epoch": 0.6824084062487514,
      "grad_norm": 2.515625,
      "learning_rate": 0.00013178331735378714,
      "loss": 1.2506,
      "step": 4270
    },
    {
      "epoch": 0.6840065523992169,
      "grad_norm": 2.96875,
      "learning_rate": 0.00013162352189197827,
      "loss": 1.6964,
      "step": 4280
    },
    {
      "epoch": 0.6856046985496824,
      "grad_norm": 7.96875,
      "learning_rate": 0.0001314637264301694,
      "loss": 0.664,
      "step": 4290
    },
    {
      "epoch": 0.6872028447001478,
      "grad_norm": 2.84375,
      "learning_rate": 0.00013130393096836052,
      "loss": 1.2828,
      "step": 4300
    },
    {
      "epoch": 0.6888009908506133,
      "grad_norm": 5.75,
      "learning_rate": 0.00013114413550655162,
      "loss": 1.6472,
      "step": 4310
    },
    {
      "epoch": 0.6903991370010788,
      "grad_norm": 2.390625,
      "learning_rate": 0.00013098434004474275,
      "loss": 1.6291,
      "step": 4320
    },
    {
      "epoch": 0.6919972831515442,
      "grad_norm": 4.3125,
      "learning_rate": 0.00013082454458293385,
      "loss": 1.5601,
      "step": 4330
    },
    {
      "epoch": 0.6935954293020097,
      "grad_norm": 1.546875,
      "learning_rate": 0.00013066474912112498,
      "loss": 1.2043,
      "step": 4340
    },
    {
      "epoch": 0.6951935754524752,
      "grad_norm": 3.484375,
      "learning_rate": 0.00013050495365931608,
      "loss": 1.4194,
      "step": 4350
    },
    {
      "epoch": 0.6967917216029406,
      "grad_norm": 2.09375,
      "learning_rate": 0.00013034515819750718,
      "loss": 1.3049,
      "step": 4360
    },
    {
      "epoch": 0.6983898677534061,
      "grad_norm": 2.59375,
      "learning_rate": 0.0001301853627356983,
      "loss": 1.4863,
      "step": 4370
    },
    {
      "epoch": 0.6999880139038716,
      "grad_norm": 0.92578125,
      "learning_rate": 0.00013002556727388943,
      "loss": 1.3398,
      "step": 4380
    },
    {
      "epoch": 0.7015861600543369,
      "grad_norm": 672.0,
      "learning_rate": 0.00012986577181208056,
      "loss": 1.824,
      "step": 4390
    },
    {
      "epoch": 0.7031843062048024,
      "grad_norm": 1.2265625,
      "learning_rate": 0.00012970597635027166,
      "loss": 1.3207,
      "step": 4400
    },
    {
      "epoch": 0.7047824523552679,
      "grad_norm": 1.984375,
      "learning_rate": 0.0001295461808884628,
      "loss": 1.3019,
      "step": 4410
    },
    {
      "epoch": 0.7063805985057333,
      "grad_norm": 2.109375,
      "learning_rate": 0.0001293863854266539,
      "loss": 1.42,
      "step": 4420
    },
    {
      "epoch": 0.7079787446561988,
      "grad_norm": 0.021728515625,
      "learning_rate": 0.00012922658996484502,
      "loss": 1.5618,
      "step": 4430
    },
    {
      "epoch": 0.7095768908066643,
      "grad_norm": 1.640625,
      "learning_rate": 0.00012906679450303612,
      "loss": 1.5242,
      "step": 4440
    },
    {
      "epoch": 0.7111750369571297,
      "grad_norm": 2.046875,
      "learning_rate": 0.00012890699904122722,
      "loss": 1.0428,
      "step": 4450
    },
    {
      "epoch": 0.7127731831075952,
      "grad_norm": 1.1328125,
      "learning_rate": 0.00012874720357941834,
      "loss": 1.0626,
      "step": 4460
    },
    {
      "epoch": 0.7143713292580607,
      "grad_norm": 2.8125,
      "learning_rate": 0.00012858740811760947,
      "loss": 1.1138,
      "step": 4470
    },
    {
      "epoch": 0.7159694754085261,
      "grad_norm": 0.98046875,
      "learning_rate": 0.0001284276126558006,
      "loss": 1.0556,
      "step": 4480
    },
    {
      "epoch": 0.7175676215589916,
      "grad_norm": 1.671875,
      "learning_rate": 0.0001282678171939917,
      "loss": 1.3262,
      "step": 4490
    },
    {
      "epoch": 0.7191657677094571,
      "grad_norm": 1.328125,
      "learning_rate": 0.00012810802173218283,
      "loss": 1.6226,
      "step": 4500
    },
    {
      "epoch": 0.7207639138599224,
      "grad_norm": 1.65625,
      "learning_rate": 0.00012794822627037393,
      "loss": 1.2599,
      "step": 4510
    },
    {
      "epoch": 0.7223620600103879,
      "grad_norm": 2.375,
      "learning_rate": 0.00012778843080856503,
      "loss": 1.476,
      "step": 4520
    },
    {
      "epoch": 0.7239602061608534,
      "grad_norm": 3.0625,
      "learning_rate": 0.00012762863534675615,
      "loss": 1.9827,
      "step": 4530
    },
    {
      "epoch": 0.7255583523113188,
      "grad_norm": 2.71875,
      "learning_rate": 0.00012746883988494726,
      "loss": 2.0634,
      "step": 4540
    },
    {
      "epoch": 0.7271564984617843,
      "grad_norm": 7.03125,
      "learning_rate": 0.00012730904442313838,
      "loss": 1.4454,
      "step": 4550
    },
    {
      "epoch": 0.7287546446122498,
      "grad_norm": 0.392578125,
      "learning_rate": 0.0001271492489613295,
      "loss": 1.3793,
      "step": 4560
    },
    {
      "epoch": 0.7303527907627152,
      "grad_norm": 1.1484375,
      "learning_rate": 0.00012698945349952064,
      "loss": 1.7318,
      "step": 4570
    },
    {
      "epoch": 0.7319509369131807,
      "grad_norm": 3.171875,
      "learning_rate": 0.00012682965803771174,
      "loss": 1.5784,
      "step": 4580
    },
    {
      "epoch": 0.7335490830636462,
      "grad_norm": 1.1796875,
      "learning_rate": 0.00012666986257590287,
      "loss": 1.419,
      "step": 4590
    },
    {
      "epoch": 0.7351472292141117,
      "grad_norm": 3.0,
      "learning_rate": 0.00012651006711409397,
      "loss": 1.5938,
      "step": 4600
    },
    {
      "epoch": 0.7367453753645771,
      "grad_norm": 1.9609375,
      "learning_rate": 0.00012635027165228507,
      "loss": 1.2981,
      "step": 4610
    },
    {
      "epoch": 0.7383435215150426,
      "grad_norm": 2.4375,
      "learning_rate": 0.0001261904761904762,
      "loss": 1.5438,
      "step": 4620
    },
    {
      "epoch": 0.7399416676655081,
      "grad_norm": 3.46875,
      "learning_rate": 0.0001260306807286673,
      "loss": 1.9769,
      "step": 4630
    },
    {
      "epoch": 0.7415398138159734,
      "grad_norm": 1.625,
      "learning_rate": 0.00012587088526685842,
      "loss": 1.4729,
      "step": 4640
    },
    {
      "epoch": 0.7431379599664389,
      "grad_norm": 1.1875,
      "learning_rate": 0.00012571108980504955,
      "loss": 1.7972,
      "step": 4650
    },
    {
      "epoch": 0.7447361061169044,
      "grad_norm": 1.8125,
      "learning_rate": 0.00012555129434324068,
      "loss": 1.2325,
      "step": 4660
    },
    {
      "epoch": 0.7463342522673698,
      "grad_norm": 1.609375,
      "learning_rate": 0.00012539149888143178,
      "loss": 1.1109,
      "step": 4670
    },
    {
      "epoch": 0.7479323984178353,
      "grad_norm": 2.1875,
      "learning_rate": 0.0001252317034196229,
      "loss": 1.5065,
      "step": 4680
    },
    {
      "epoch": 0.7495305445683008,
      "grad_norm": 2.859375,
      "learning_rate": 0.000125071907957814,
      "loss": 1.7146,
      "step": 4690
    },
    {
      "epoch": 0.7511286907187662,
      "grad_norm": 1.984375,
      "learning_rate": 0.0001249121124960051,
      "loss": 1.538,
      "step": 4700
    },
    {
      "epoch": 0.7527268368692317,
      "grad_norm": 3.671875,
      "learning_rate": 0.00012475231703419623,
      "loss": 1.8271,
      "step": 4710
    },
    {
      "epoch": 0.7543249830196972,
      "grad_norm": 2.296875,
      "learning_rate": 0.00012459252157238733,
      "loss": 1.9377,
      "step": 4720
    },
    {
      "epoch": 0.7559231291701626,
      "grad_norm": 3.40625,
      "learning_rate": 0.00012443272611057846,
      "loss": 1.6766,
      "step": 4730
    },
    {
      "epoch": 0.7575212753206281,
      "grad_norm": 1.2421875,
      "learning_rate": 0.00012427293064876956,
      "loss": 1.5908,
      "step": 4740
    },
    {
      "epoch": 0.7591194214710936,
      "grad_norm": 2.84375,
      "learning_rate": 0.00012411313518696072,
      "loss": 1.6598,
      "step": 4750
    },
    {
      "epoch": 0.760717567621559,
      "grad_norm": 10.4375,
      "learning_rate": 0.00012395333972515182,
      "loss": 1.6035,
      "step": 4760
    },
    {
      "epoch": 0.7623157137720245,
      "grad_norm": 2.78125,
      "learning_rate": 0.00012379354426334294,
      "loss": 1.3636,
      "step": 4770
    },
    {
      "epoch": 0.76391385992249,
      "grad_norm": 1.46875,
      "learning_rate": 0.00012363374880153404,
      "loss": 1.4192,
      "step": 4780
    },
    {
      "epoch": 0.7655120060729553,
      "grad_norm": 2.515625,
      "learning_rate": 0.00012347395333972514,
      "loss": 1.2988,
      "step": 4790
    },
    {
      "epoch": 0.7671101522234208,
      "grad_norm": 3.109375,
      "learning_rate": 0.00012331415787791627,
      "loss": 1.1843,
      "step": 4800
    },
    {
      "epoch": 0.7687082983738863,
      "grad_norm": 1.703125,
      "learning_rate": 0.00012315436241610737,
      "loss": 1.3928,
      "step": 4810
    },
    {
      "epoch": 0.7703064445243517,
      "grad_norm": 1.5546875,
      "learning_rate": 0.0001229945669542985,
      "loss": 1.446,
      "step": 4820
    },
    {
      "epoch": 0.7719045906748172,
      "grad_norm": 1.5625,
      "learning_rate": 0.0001228347714924896,
      "loss": 1.3326,
      "step": 4830
    },
    {
      "epoch": 0.7735027368252827,
      "grad_norm": 1.9296875,
      "learning_rate": 0.00012267497603068075,
      "loss": 1.8096,
      "step": 4840
    },
    {
      "epoch": 0.7751008829757481,
      "grad_norm": 23.125,
      "learning_rate": 0.00012251518056887185,
      "loss": 1.8195,
      "step": 4850
    },
    {
      "epoch": 0.7766990291262136,
      "grad_norm": 2.90625,
      "learning_rate": 0.00012235538510706298,
      "loss": 1.8272,
      "step": 4860
    },
    {
      "epoch": 0.7782971752766791,
      "grad_norm": 1.7890625,
      "learning_rate": 0.00012219558964525408,
      "loss": 1.6988,
      "step": 4870
    },
    {
      "epoch": 0.7798953214271445,
      "grad_norm": 1.1171875,
      "learning_rate": 0.0001220357941834452,
      "loss": 1.5043,
      "step": 4880
    },
    {
      "epoch": 0.78149346757761,
      "grad_norm": 2.21875,
      "learning_rate": 0.00012187599872163631,
      "loss": 1.5254,
      "step": 4890
    },
    {
      "epoch": 0.7830916137280755,
      "grad_norm": 2.34375,
      "learning_rate": 0.00012171620325982742,
      "loss": 1.7296,
      "step": 4900
    },
    {
      "epoch": 0.7846897598785408,
      "grad_norm": 1.7890625,
      "learning_rate": 0.00012155640779801854,
      "loss": 1.7833,
      "step": 4910
    },
    {
      "epoch": 0.7862879060290063,
      "grad_norm": 1.6015625,
      "learning_rate": 0.00012139661233620965,
      "loss": 1.5578,
      "step": 4920
    },
    {
      "epoch": 0.7878860521794718,
      "grad_norm": 1.015625,
      "learning_rate": 0.00012123681687440078,
      "loss": 1.5405,
      "step": 4930
    },
    {
      "epoch": 0.7894841983299373,
      "grad_norm": 2.3125,
      "learning_rate": 0.0001210770214125919,
      "loss": 1.0062,
      "step": 4940
    },
    {
      "epoch": 0.7910823444804027,
      "grad_norm": 3.265625,
      "learning_rate": 0.00012091722595078301,
      "loss": 1.3676,
      "step": 4950
    },
    {
      "epoch": 0.7926804906308682,
      "grad_norm": 1.359375,
      "learning_rate": 0.00012075743048897412,
      "loss": 1.1948,
      "step": 4960
    },
    {
      "epoch": 0.7942786367813337,
      "grad_norm": 1.46875,
      "learning_rate": 0.00012059763502716524,
      "loss": 1.7108,
      "step": 4970
    },
    {
      "epoch": 0.7958767829317991,
      "grad_norm": 1.8671875,
      "learning_rate": 0.00012043783956535635,
      "loss": 1.1449,
      "step": 4980
    },
    {
      "epoch": 0.7974749290822646,
      "grad_norm": 3.234375,
      "learning_rate": 0.00012027804410354746,
      "loss": 1.4438,
      "step": 4990
    },
    {
      "epoch": 0.7990730752327301,
      "grad_norm": 7.96875,
      "learning_rate": 0.00012011824864173858,
      "loss": 1.3813,
      "step": 5000
    },
    {
      "epoch": 0.8006712213831955,
      "grad_norm": 2.484375,
      "learning_rate": 0.00011995845317992969,
      "loss": 1.3798,
      "step": 5010
    },
    {
      "epoch": 0.802269367533661,
      "grad_norm": 2.46875,
      "learning_rate": 0.00011979865771812082,
      "loss": 1.2435,
      "step": 5020
    },
    {
      "epoch": 0.8038675136841265,
      "grad_norm": 2.375,
      "learning_rate": 0.00011963886225631193,
      "loss": 1.5493,
      "step": 5030
    },
    {
      "epoch": 0.8054656598345918,
      "grad_norm": 2.375,
      "learning_rate": 0.00011947906679450305,
      "loss": 0.9773,
      "step": 5040
    },
    {
      "epoch": 0.8070638059850573,
      "grad_norm": 3.234375,
      "learning_rate": 0.00011931927133269416,
      "loss": 1.5062,
      "step": 5050
    },
    {
      "epoch": 0.8086619521355228,
      "grad_norm": 1.671875,
      "learning_rate": 0.00011915947587088527,
      "loss": 1.8877,
      "step": 5060
    },
    {
      "epoch": 0.8102600982859882,
      "grad_norm": 1.0078125,
      "learning_rate": 0.00011899968040907639,
      "loss": 1.3217,
      "step": 5070
    },
    {
      "epoch": 0.8118582444364537,
      "grad_norm": 1.0859375,
      "learning_rate": 0.0001188398849472675,
      "loss": 1.1959,
      "step": 5080
    },
    {
      "epoch": 0.8134563905869192,
      "grad_norm": 1.0703125,
      "learning_rate": 0.00011868008948545862,
      "loss": 1.3268,
      "step": 5090
    },
    {
      "epoch": 0.8150545367373846,
      "grad_norm": 1.921875,
      "learning_rate": 0.00011852029402364973,
      "loss": 1.1635,
      "step": 5100
    },
    {
      "epoch": 0.8166526828878501,
      "grad_norm": 1.0234375,
      "learning_rate": 0.00011836049856184086,
      "loss": 1.3232,
      "step": 5110
    },
    {
      "epoch": 0.8182508290383156,
      "grad_norm": 2.171875,
      "learning_rate": 0.00011820070310003197,
      "loss": 1.4617,
      "step": 5120
    },
    {
      "epoch": 0.819848975188781,
      "grad_norm": 3.546875,
      "learning_rate": 0.00011804090763822309,
      "loss": 1.5192,
      "step": 5130
    },
    {
      "epoch": 0.8214471213392465,
      "grad_norm": 1.8671875,
      "learning_rate": 0.0001178811121764142,
      "loss": 1.5183,
      "step": 5140
    },
    {
      "epoch": 0.823045267489712,
      "grad_norm": 2.71875,
      "learning_rate": 0.00011772131671460531,
      "loss": 1.5918,
      "step": 5150
    },
    {
      "epoch": 0.8246434136401773,
      "grad_norm": 0.000881195068359375,
      "learning_rate": 0.00011756152125279643,
      "loss": 1.352,
      "step": 5160
    },
    {
      "epoch": 0.8262415597906428,
      "grad_norm": 1.0390625,
      "learning_rate": 0.00011740172579098754,
      "loss": 1.232,
      "step": 5170
    },
    {
      "epoch": 0.8278397059411083,
      "grad_norm": 0.00185394287109375,
      "learning_rate": 0.00011724193032917866,
      "loss": 0.9922,
      "step": 5180
    },
    {
      "epoch": 0.8294378520915737,
      "grad_norm": 3.015625,
      "learning_rate": 0.00011708213486736977,
      "loss": 1.401,
      "step": 5190
    },
    {
      "epoch": 0.8310359982420392,
      "grad_norm": 2.203125,
      "learning_rate": 0.00011692233940556088,
      "loss": 1.7923,
      "step": 5200
    },
    {
      "epoch": 0.8326341443925047,
      "grad_norm": 1.859375,
      "learning_rate": 0.00011676254394375201,
      "loss": 1.0899,
      "step": 5210
    },
    {
      "epoch": 0.8342322905429701,
      "grad_norm": 5.0,
      "learning_rate": 0.00011660274848194312,
      "loss": 1.2044,
      "step": 5220
    },
    {
      "epoch": 0.8358304366934356,
      "grad_norm": 1.4609375,
      "learning_rate": 0.00011644295302013424,
      "loss": 1.2164,
      "step": 5230
    },
    {
      "epoch": 0.8374285828439011,
      "grad_norm": 1.8515625,
      "learning_rate": 0.00011628315755832535,
      "loss": 1.2551,
      "step": 5240
    },
    {
      "epoch": 0.8390267289943666,
      "grad_norm": 2.65625,
      "learning_rate": 0.00011612336209651647,
      "loss": 1.5781,
      "step": 5250
    },
    {
      "epoch": 0.840624875144832,
      "grad_norm": 2.046875,
      "learning_rate": 0.00011596356663470758,
      "loss": 1.4346,
      "step": 5260
    },
    {
      "epoch": 0.8422230212952975,
      "grad_norm": 3.703125,
      "learning_rate": 0.0001158037711728987,
      "loss": 1.5694,
      "step": 5270
    },
    {
      "epoch": 0.843821167445763,
      "grad_norm": 2.203125,
      "learning_rate": 0.00011564397571108981,
      "loss": 1.501,
      "step": 5280
    },
    {
      "epoch": 0.8454193135962284,
      "grad_norm": 1.6015625,
      "learning_rate": 0.00011548418024928092,
      "loss": 1.2769,
      "step": 5290
    },
    {
      "epoch": 0.8470174597466938,
      "grad_norm": 1.2265625,
      "learning_rate": 0.00011532438478747205,
      "loss": 1.8254,
      "step": 5300
    },
    {
      "epoch": 0.8486156058971593,
      "grad_norm": 6.0625,
      "learning_rate": 0.00011516458932566316,
      "loss": 1.8784,
      "step": 5310
    },
    {
      "epoch": 0.8502137520476247,
      "grad_norm": 2.203125,
      "learning_rate": 0.00011500479386385428,
      "loss": 1.5576,
      "step": 5320
    },
    {
      "epoch": 0.8518118981980902,
      "grad_norm": 5.71875,
      "learning_rate": 0.00011484499840204539,
      "loss": 1.977,
      "step": 5330
    },
    {
      "epoch": 0.8534100443485557,
      "grad_norm": 1.0234375,
      "learning_rate": 0.0001146852029402365,
      "loss": 1.5083,
      "step": 5340
    },
    {
      "epoch": 0.8550081904990211,
      "grad_norm": 0.05859375,
      "learning_rate": 0.00011452540747842762,
      "loss": 1.0917,
      "step": 5350
    },
    {
      "epoch": 0.8566063366494866,
      "grad_norm": 2.328125,
      "learning_rate": 0.00011436561201661873,
      "loss": 1.4354,
      "step": 5360
    },
    {
      "epoch": 0.8582044827999521,
      "grad_norm": 2.21875,
      "learning_rate": 0.00011420581655480985,
      "loss": 1.6553,
      "step": 5370
    },
    {
      "epoch": 0.8598026289504175,
      "grad_norm": 1.125,
      "learning_rate": 0.00011404602109300095,
      "loss": 1.5416,
      "step": 5380
    },
    {
      "epoch": 0.861400775100883,
      "grad_norm": 3.8125,
      "learning_rate": 0.00011388622563119209,
      "loss": 1.3303,
      "step": 5390
    },
    {
      "epoch": 0.8629989212513485,
      "grad_norm": 3.484375,
      "learning_rate": 0.0001137264301693832,
      "loss": 1.5003,
      "step": 5400
    },
    {
      "epoch": 0.8645970674018139,
      "grad_norm": 1.53125,
      "learning_rate": 0.00011356663470757432,
      "loss": 1.5362,
      "step": 5410
    },
    {
      "epoch": 0.8661952135522794,
      "grad_norm": 1.9375,
      "learning_rate": 0.00011340683924576543,
      "loss": 1.3759,
      "step": 5420
    },
    {
      "epoch": 0.8677933597027448,
      "grad_norm": 2.84375,
      "learning_rate": 0.00011324704378395654,
      "loss": 1.6402,
      "step": 5430
    },
    {
      "epoch": 0.8693915058532102,
      "grad_norm": 3.171875,
      "learning_rate": 0.00011308724832214766,
      "loss": 1.6609,
      "step": 5440
    },
    {
      "epoch": 0.8709896520036757,
      "grad_norm": 1.78125,
      "learning_rate": 0.00011292745286033877,
      "loss": 1.4421,
      "step": 5450
    },
    {
      "epoch": 0.8725877981541412,
      "grad_norm": 2.84375,
      "learning_rate": 0.00011276765739852989,
      "loss": 1.6068,
      "step": 5460
    },
    {
      "epoch": 0.8741859443046066,
      "grad_norm": 1.796875,
      "learning_rate": 0.00011260786193672099,
      "loss": 1.5577,
      "step": 5470
    },
    {
      "epoch": 0.8757840904550721,
      "grad_norm": 3.5,
      "learning_rate": 0.00011244806647491213,
      "loss": 1.3699,
      "step": 5480
    },
    {
      "epoch": 0.8773822366055376,
      "grad_norm": 0.0003070831298828125,
      "learning_rate": 0.00011228827101310324,
      "loss": 1.5801,
      "step": 5490
    },
    {
      "epoch": 0.878980382756003,
      "grad_norm": 3.34375,
      "learning_rate": 0.00011212847555129436,
      "loss": 1.5669,
      "step": 5500
    },
    {
      "epoch": 0.8805785289064685,
      "grad_norm": 2.65625,
      "learning_rate": 0.00011196868008948547,
      "loss": 1.2644,
      "step": 5510
    },
    {
      "epoch": 0.882176675056934,
      "grad_norm": 3.546875,
      "learning_rate": 0.00011180888462767658,
      "loss": 1.5351,
      "step": 5520
    },
    {
      "epoch": 0.8837748212073994,
      "grad_norm": 1.875,
      "learning_rate": 0.0001116490891658677,
      "loss": 1.2356,
      "step": 5530
    },
    {
      "epoch": 0.8853729673578649,
      "grad_norm": 1.21875,
      "learning_rate": 0.00011148929370405881,
      "loss": 1.3899,
      "step": 5540
    },
    {
      "epoch": 0.8869711135083304,
      "grad_norm": 3.515625,
      "learning_rate": 0.00011132949824224993,
      "loss": 1.5581,
      "step": 5550
    },
    {
      "epoch": 0.8885692596587959,
      "grad_norm": 2.5625,
      "learning_rate": 0.00011116970278044103,
      "loss": 1.8836,
      "step": 5560
    },
    {
      "epoch": 0.8901674058092612,
      "grad_norm": 1.46875,
      "learning_rate": 0.00011100990731863217,
      "loss": 1.1437,
      "step": 5570
    },
    {
      "epoch": 0.8917655519597267,
      "grad_norm": 1.8359375,
      "learning_rate": 0.00011085011185682328,
      "loss": 1.2299,
      "step": 5580
    },
    {
      "epoch": 0.8933636981101922,
      "grad_norm": 1.0859375,
      "learning_rate": 0.0001106903163950144,
      "loss": 1.67,
      "step": 5590
    },
    {
      "epoch": 0.8949618442606576,
      "grad_norm": 3.390625,
      "learning_rate": 0.00011053052093320551,
      "loss": 1.5316,
      "step": 5600
    },
    {
      "epoch": 0.8965599904111231,
      "grad_norm": 3.0,
      "learning_rate": 0.00011037072547139662,
      "loss": 1.9649,
      "step": 5610
    },
    {
      "epoch": 0.8981581365615886,
      "grad_norm": 1.9921875,
      "learning_rate": 0.00011021093000958774,
      "loss": 1.5401,
      "step": 5620
    },
    {
      "epoch": 0.899756282712054,
      "grad_norm": 1.515625,
      "learning_rate": 0.00011005113454777885,
      "loss": 1.5572,
      "step": 5630
    },
    {
      "epoch": 0.9013544288625195,
      "grad_norm": 1.9140625,
      "learning_rate": 0.00010989133908596996,
      "loss": 1.4964,
      "step": 5640
    },
    {
      "epoch": 0.902952575012985,
      "grad_norm": 1.03125,
      "learning_rate": 0.00010973154362416106,
      "loss": 1.1754,
      "step": 5650
    },
    {
      "epoch": 0.9045507211634504,
      "grad_norm": 16.75,
      "learning_rate": 0.00010957174816235218,
      "loss": 1.7586,
      "step": 5660
    },
    {
      "epoch": 0.9061488673139159,
      "grad_norm": 0.14453125,
      "learning_rate": 0.00010941195270054332,
      "loss": 0.9621,
      "step": 5670
    },
    {
      "epoch": 0.9077470134643814,
      "grad_norm": 1.65625,
      "learning_rate": 0.00010925215723873443,
      "loss": 1.3445,
      "step": 5680
    },
    {
      "epoch": 0.9093451596148467,
      "grad_norm": 2.90625,
      "learning_rate": 0.00010909236177692555,
      "loss": 1.6665,
      "step": 5690
    },
    {
      "epoch": 0.9109433057653122,
      "grad_norm": 2.21875,
      "learning_rate": 0.00010893256631511666,
      "loss": 1.3462,
      "step": 5700
    },
    {
      "epoch": 0.9125414519157777,
      "grad_norm": 1.875,
      "learning_rate": 0.00010877277085330778,
      "loss": 1.6971,
      "step": 5710
    },
    {
      "epoch": 0.9141395980662431,
      "grad_norm": 2.171875,
      "learning_rate": 0.00010861297539149889,
      "loss": 1.5141,
      "step": 5720
    },
    {
      "epoch": 0.9157377442167086,
      "grad_norm": 0.00153350830078125,
      "learning_rate": 0.00010845317992969,
      "loss": 1.3323,
      "step": 5730
    },
    {
      "epoch": 0.9173358903671741,
      "grad_norm": 2.078125,
      "learning_rate": 0.0001082933844678811,
      "loss": 1.5783,
      "step": 5740
    },
    {
      "epoch": 0.9189340365176395,
      "grad_norm": 3.78125,
      "learning_rate": 0.00010813358900607222,
      "loss": 1.4857,
      "step": 5750
    },
    {
      "epoch": 0.920532182668105,
      "grad_norm": 3.734375,
      "learning_rate": 0.00010797379354426336,
      "loss": 1.4941,
      "step": 5760
    },
    {
      "epoch": 0.9221303288185705,
      "grad_norm": 3.0625,
      "learning_rate": 0.00010781399808245447,
      "loss": 1.7601,
      "step": 5770
    },
    {
      "epoch": 0.9237284749690359,
      "grad_norm": 3.1875,
      "learning_rate": 0.00010765420262064559,
      "loss": 1.6808,
      "step": 5780
    },
    {
      "epoch": 0.9253266211195014,
      "grad_norm": 2.421875,
      "learning_rate": 0.0001074944071588367,
      "loss": 1.3033,
      "step": 5790
    },
    {
      "epoch": 0.9269247672699669,
      "grad_norm": 2.375,
      "learning_rate": 0.00010733461169702781,
      "loss": 1.2613,
      "step": 5800
    },
    {
      "epoch": 0.9285229134204322,
      "grad_norm": 4.21875,
      "learning_rate": 0.00010717481623521893,
      "loss": 1.6517,
      "step": 5810
    },
    {
      "epoch": 0.9301210595708977,
      "grad_norm": 3.25,
      "learning_rate": 0.00010701502077341003,
      "loss": 1.5464,
      "step": 5820
    },
    {
      "epoch": 0.9317192057213632,
      "grad_norm": 3.5625,
      "learning_rate": 0.00010685522531160114,
      "loss": 1.6995,
      "step": 5830
    },
    {
      "epoch": 0.9333173518718286,
      "grad_norm": 10.1875,
      "learning_rate": 0.00010669542984979226,
      "loss": 1.4418,
      "step": 5840
    },
    {
      "epoch": 0.9349154980222941,
      "grad_norm": 1.9765625,
      "learning_rate": 0.0001065356343879834,
      "loss": 1.2491,
      "step": 5850
    },
    {
      "epoch": 0.9365136441727596,
      "grad_norm": 0.2060546875,
      "learning_rate": 0.00010637583892617451,
      "loss": 0.9494,
      "step": 5860
    },
    {
      "epoch": 0.9381117903232251,
      "grad_norm": 2.421875,
      "learning_rate": 0.00010621604346436563,
      "loss": 1.642,
      "step": 5870
    },
    {
      "epoch": 0.9397099364736905,
      "grad_norm": 1.4921875,
      "learning_rate": 0.00010605624800255674,
      "loss": 1.1339,
      "step": 5880
    },
    {
      "epoch": 0.941308082624156,
      "grad_norm": 3.3125,
      "learning_rate": 0.00010589645254074785,
      "loss": 1.4896,
      "step": 5890
    },
    {
      "epoch": 0.9429062287746215,
      "grad_norm": 18.75,
      "learning_rate": 0.00010573665707893897,
      "loss": 1.8275,
      "step": 5900
    },
    {
      "epoch": 0.9445043749250869,
      "grad_norm": 1.7578125,
      "learning_rate": 0.00010557686161713007,
      "loss": 1.523,
      "step": 5910
    },
    {
      "epoch": 0.9461025210755524,
      "grad_norm": 0.13671875,
      "learning_rate": 0.00010541706615532118,
      "loss": 1.6601,
      "step": 5920
    },
    {
      "epoch": 0.9477006672260179,
      "grad_norm": 2.96875,
      "learning_rate": 0.0001052572706935123,
      "loss": 1.4274,
      "step": 5930
    },
    {
      "epoch": 0.9492988133764833,
      "grad_norm": 2.796875,
      "learning_rate": 0.00010509747523170344,
      "loss": 1.5969,
      "step": 5940
    },
    {
      "epoch": 0.9508969595269487,
      "grad_norm": 2.5,
      "learning_rate": 0.00010493767976989455,
      "loss": 1.3339,
      "step": 5950
    },
    {
      "epoch": 0.9524951056774142,
      "grad_norm": 3.15625,
      "learning_rate": 0.00010477788430808566,
      "loss": 1.6478,
      "step": 5960
    },
    {
      "epoch": 0.9540932518278796,
      "grad_norm": 2.59375,
      "learning_rate": 0.00010461808884627678,
      "loss": 1.6863,
      "step": 5970
    },
    {
      "epoch": 0.9556913979783451,
      "grad_norm": 3.9375,
      "learning_rate": 0.00010445829338446789,
      "loss": 1.1477,
      "step": 5980
    },
    {
      "epoch": 0.9572895441288106,
      "grad_norm": 3.5625,
      "learning_rate": 0.000104298497922659,
      "loss": 1.5769,
      "step": 5990
    },
    {
      "epoch": 0.958887690279276,
      "grad_norm": 3.8125,
      "learning_rate": 0.0001041387024608501,
      "loss": 1.5443,
      "step": 6000
    },
    {
      "epoch": 0.9604858364297415,
      "grad_norm": 1.4921875,
      "learning_rate": 0.00010397890699904122,
      "loss": 1.5871,
      "step": 6010
    },
    {
      "epoch": 0.962083982580207,
      "grad_norm": 2.65625,
      "learning_rate": 0.00010381911153723233,
      "loss": 1.4833,
      "step": 6020
    },
    {
      "epoch": 0.9636821287306724,
      "grad_norm": 0.125,
      "learning_rate": 0.00010365931607542348,
      "loss": 1.0961,
      "step": 6030
    },
    {
      "epoch": 0.9652802748811379,
      "grad_norm": 0.00372314453125,
      "learning_rate": 0.00010349952061361459,
      "loss": 1.3242,
      "step": 6040
    },
    {
      "epoch": 0.9668784210316034,
      "grad_norm": 1.78125,
      "learning_rate": 0.0001033397251518057,
      "loss": 0.9694,
      "step": 6050
    },
    {
      "epoch": 0.9684765671820688,
      "grad_norm": 0.7890625,
      "learning_rate": 0.00010317992968999682,
      "loss": 1.5447,
      "step": 6060
    },
    {
      "epoch": 0.9700747133325343,
      "grad_norm": 1.625,
      "learning_rate": 0.00010302013422818793,
      "loss": 1.3291,
      "step": 6070
    },
    {
      "epoch": 0.9716728594829998,
      "grad_norm": 2.15625,
      "learning_rate": 0.00010286033876637905,
      "loss": 1.4896,
      "step": 6080
    },
    {
      "epoch": 0.9732710056334651,
      "grad_norm": 1.875,
      "learning_rate": 0.00010270054330457015,
      "loss": 0.8849,
      "step": 6090
    },
    {
      "epoch": 0.9748691517839306,
      "grad_norm": 1.03125,
      "learning_rate": 0.00010254074784276126,
      "loss": 1.4613,
      "step": 6100
    },
    {
      "epoch": 0.9764672979343961,
      "grad_norm": 1.2421875,
      "learning_rate": 0.00010238095238095237,
      "loss": 1.2944,
      "step": 6110
    },
    {
      "epoch": 0.9780654440848615,
      "grad_norm": 2.5,
      "learning_rate": 0.00010222115691914349,
      "loss": 1.5242,
      "step": 6120
    },
    {
      "epoch": 0.979663590235327,
      "grad_norm": 2.078125,
      "learning_rate": 0.00010206136145733463,
      "loss": 1.7403,
      "step": 6130
    },
    {
      "epoch": 0.9812617363857925,
      "grad_norm": 4.25,
      "learning_rate": 0.00010190156599552574,
      "loss": 1.6468,
      "step": 6140
    },
    {
      "epoch": 0.9828598825362579,
      "grad_norm": 2.125,
      "learning_rate": 0.00010174177053371686,
      "loss": 1.4473,
      "step": 6150
    },
    {
      "epoch": 0.9844580286867234,
      "grad_norm": 3.265625,
      "learning_rate": 0.00010158197507190797,
      "loss": 1.5976,
      "step": 6160
    },
    {
      "epoch": 0.9860561748371889,
      "grad_norm": 1.984375,
      "learning_rate": 0.00010142217961009908,
      "loss": 1.4491,
      "step": 6170
    },
    {
      "epoch": 0.9876543209876543,
      "grad_norm": 1.1328125,
      "learning_rate": 0.00010126238414829018,
      "loss": 1.6335,
      "step": 6180
    },
    {
      "epoch": 0.9892524671381198,
      "grad_norm": 1.3515625,
      "learning_rate": 0.0001011025886864813,
      "loss": 1.2116,
      "step": 6190
    },
    {
      "epoch": 0.9908506132885853,
      "grad_norm": 2.375,
      "learning_rate": 0.00010094279322467241,
      "loss": 1.5976,
      "step": 6200
    },
    {
      "epoch": 0.9924487594390508,
      "grad_norm": 2.546875,
      "learning_rate": 0.00010078299776286353,
      "loss": 1.5307,
      "step": 6210
    },
    {
      "epoch": 0.9940469055895161,
      "grad_norm": 2.46875,
      "learning_rate": 0.00010062320230105467,
      "loss": 1.6556,
      "step": 6220
    },
    {
      "epoch": 0.9956450517399816,
      "grad_norm": 3.15625,
      "learning_rate": 0.00010046340683924578,
      "loss": 1.6878,
      "step": 6230
    },
    {
      "epoch": 0.9972431978904471,
      "grad_norm": 4.90625,
      "learning_rate": 0.0001003036113774369,
      "loss": 1.0976,
      "step": 6240
    },
    {
      "epoch": 0.9988413440409125,
      "grad_norm": 2.9375,
      "learning_rate": 0.00010014381591562801,
      "loss": 1.2016,
      "step": 6250
    },
    {
      "epoch": 1.000319629230093,
      "grad_norm": 3.4375,
      "learning_rate": 9.998402045381911e-05,
      "loss": 1.0428,
      "step": 6260
    },
    {
      "epoch": 1.0019177753805586,
      "grad_norm": 4.4375,
      "learning_rate": 9.982422499201022e-05,
      "loss": 1.4095,
      "step": 6270
    },
    {
      "epoch": 1.003515921531024,
      "grad_norm": 2.4375,
      "learning_rate": 9.966442953020134e-05,
      "loss": 1.1845,
      "step": 6280
    },
    {
      "epoch": 1.0051140676814894,
      "grad_norm": 1.15625,
      "learning_rate": 9.950463406839246e-05,
      "loss": 0.9412,
      "step": 6290
    },
    {
      "epoch": 1.0067122138319549,
      "grad_norm": 2.140625,
      "learning_rate": 9.934483860658358e-05,
      "loss": 1.502,
      "step": 6300
    },
    {
      "epoch": 1.0083103599824204,
      "grad_norm": 4.03125,
      "learning_rate": 9.918504314477469e-05,
      "loss": 1.4477,
      "step": 6310
    },
    {
      "epoch": 1.0099085061328859,
      "grad_norm": 2.234375,
      "learning_rate": 9.90252476829658e-05,
      "loss": 1.5916,
      "step": 6320
    },
    {
      "epoch": 1.0115066522833513,
      "grad_norm": 1.640625,
      "learning_rate": 9.886545222115693e-05,
      "loss": 1.2089,
      "step": 6330
    },
    {
      "epoch": 1.0131047984338168,
      "grad_norm": 2.421875,
      "learning_rate": 9.870565675934805e-05,
      "loss": 1.0452,
      "step": 6340
    },
    {
      "epoch": 1.0147029445842823,
      "grad_norm": 3.21875,
      "learning_rate": 9.854586129753915e-05,
      "loss": 1.5188,
      "step": 6350
    },
    {
      "epoch": 1.0163010907347476,
      "grad_norm": 1.4453125,
      "learning_rate": 9.838606583573026e-05,
      "loss": 1.568,
      "step": 6360
    },
    {
      "epoch": 1.017899236885213,
      "grad_norm": 1.0546875,
      "learning_rate": 9.822627037392138e-05,
      "loss": 0.8099,
      "step": 6370
    },
    {
      "epoch": 1.0194973830356786,
      "grad_norm": 2.296875,
      "learning_rate": 9.80664749121125e-05,
      "loss": 1.2359,
      "step": 6380
    },
    {
      "epoch": 1.021095529186144,
      "grad_norm": 2.890625,
      "learning_rate": 9.790667945030362e-05,
      "loss": 1.1586,
      "step": 6390
    },
    {
      "epoch": 1.0226936753366096,
      "grad_norm": 3.984375,
      "learning_rate": 9.774688398849473e-05,
      "loss": 1.136,
      "step": 6400
    },
    {
      "epoch": 1.024291821487075,
      "grad_norm": 2.53125,
      "learning_rate": 9.758708852668585e-05,
      "loss": 1.3081,
      "step": 6410
    },
    {
      "epoch": 1.0258899676375404,
      "grad_norm": 1.703125,
      "learning_rate": 9.742729306487696e-05,
      "loss": 1.3909,
      "step": 6420
    },
    {
      "epoch": 1.0274881137880059,
      "grad_norm": 0.09326171875,
      "learning_rate": 9.726749760306809e-05,
      "loss": 1.1628,
      "step": 6430
    },
    {
      "epoch": 1.0290862599384714,
      "grad_norm": 0.10498046875,
      "learning_rate": 9.710770214125919e-05,
      "loss": 0.9837,
      "step": 6440
    },
    {
      "epoch": 1.0306844060889369,
      "grad_norm": 2.921875,
      "learning_rate": 9.69479066794503e-05,
      "loss": 1.4634,
      "step": 6450
    },
    {
      "epoch": 1.0322825522394024,
      "grad_norm": 2.0625,
      "learning_rate": 9.678811121764142e-05,
      "loss": 1.5153,
      "step": 6460
    },
    {
      "epoch": 1.0338806983898678,
      "grad_norm": 1.84375,
      "learning_rate": 9.662831575583254e-05,
      "loss": 1.3126,
      "step": 6470
    },
    {
      "epoch": 1.0354788445403331,
      "grad_norm": 2.96875,
      "learning_rate": 9.646852029402366e-05,
      "loss": 1.0196,
      "step": 6480
    },
    {
      "epoch": 1.0370769906907986,
      "grad_norm": 2.171875,
      "learning_rate": 9.630872483221477e-05,
      "loss": 1.4788,
      "step": 6490
    },
    {
      "epoch": 1.0386751368412641,
      "grad_norm": 3.578125,
      "learning_rate": 9.614892937040588e-05,
      "loss": 1.385,
      "step": 6500
    },
    {
      "epoch": 1.0402732829917296,
      "grad_norm": 2.953125,
      "learning_rate": 9.5989133908597e-05,
      "loss": 1.2562,
      "step": 6510
    },
    {
      "epoch": 1.041871429142195,
      "grad_norm": 1.609375,
      "learning_rate": 9.582933844678813e-05,
      "loss": 1.3927,
      "step": 6520
    },
    {
      "epoch": 1.0434695752926606,
      "grad_norm": 1.2265625,
      "learning_rate": 9.566954298497923e-05,
      "loss": 0.9894,
      "step": 6530
    },
    {
      "epoch": 1.0450677214431259,
      "grad_norm": 2.578125,
      "learning_rate": 9.550974752317034e-05,
      "loss": 1.443,
      "step": 6540
    },
    {
      "epoch": 1.0466658675935914,
      "grad_norm": 2.46875,
      "learning_rate": 9.534995206136145e-05,
      "loss": 1.2854,
      "step": 6550
    },
    {
      "epoch": 1.0482640137440569,
      "grad_norm": 1.546875,
      "learning_rate": 9.519015659955258e-05,
      "loss": 1.0103,
      "step": 6560
    },
    {
      "epoch": 1.0498621598945224,
      "grad_norm": 1.9375,
      "learning_rate": 9.50303611377437e-05,
      "loss": 1.4612,
      "step": 6570
    },
    {
      "epoch": 1.0514603060449879,
      "grad_norm": 2.78125,
      "learning_rate": 9.487056567593481e-05,
      "loss": 1.2422,
      "step": 6580
    },
    {
      "epoch": 1.0530584521954534,
      "grad_norm": 1.9453125,
      "learning_rate": 9.471077021412592e-05,
      "loss": 1.4486,
      "step": 6590
    },
    {
      "epoch": 1.0546565983459186,
      "grad_norm": 1.0859375,
      "learning_rate": 9.455097475231704e-05,
      "loss": 1.289,
      "step": 6600
    },
    {
      "epoch": 1.0562547444963841,
      "grad_norm": 1.7734375,
      "learning_rate": 9.439117929050816e-05,
      "loss": 1.3763,
      "step": 6610
    },
    {
      "epoch": 1.0578528906468496,
      "grad_norm": 1.3515625,
      "learning_rate": 9.423138382869927e-05,
      "loss": 0.97,
      "step": 6620
    },
    {
      "epoch": 1.0594510367973151,
      "grad_norm": 1.3125,
      "learning_rate": 9.407158836689038e-05,
      "loss": 1.0954,
      "step": 6630
    },
    {
      "epoch": 1.0610491829477806,
      "grad_norm": 2.625,
      "learning_rate": 9.391179290508149e-05,
      "loss": 1.4184,
      "step": 6640
    },
    {
      "epoch": 1.062647329098246,
      "grad_norm": 3.09375,
      "learning_rate": 9.375199744327261e-05,
      "loss": 1.1409,
      "step": 6650
    },
    {
      "epoch": 1.0642454752487116,
      "grad_norm": 2.71875,
      "learning_rate": 9.359220198146373e-05,
      "loss": 1.7435,
      "step": 6660
    },
    {
      "epoch": 1.0658436213991769,
      "grad_norm": 2.484375,
      "learning_rate": 9.343240651965485e-05,
      "loss": 1.2011,
      "step": 6670
    },
    {
      "epoch": 1.0674417675496424,
      "grad_norm": 2.140625,
      "learning_rate": 9.327261105784596e-05,
      "loss": 1.3947,
      "step": 6680
    },
    {
      "epoch": 1.0690399137001079,
      "grad_norm": 2.703125,
      "learning_rate": 9.311281559603708e-05,
      "loss": 0.9812,
      "step": 6690
    },
    {
      "epoch": 1.0706380598505734,
      "grad_norm": 2.265625,
      "learning_rate": 9.295302013422819e-05,
      "loss": 1.1756,
      "step": 6700
    },
    {
      "epoch": 1.0722362060010389,
      "grad_norm": 2.75,
      "learning_rate": 9.27932246724193e-05,
      "loss": 0.9331,
      "step": 6710
    },
    {
      "epoch": 1.0738343521515044,
      "grad_norm": 3.515625,
      "learning_rate": 9.263342921061042e-05,
      "loss": 1.371,
      "step": 6720
    },
    {
      "epoch": 1.0754324983019696,
      "grad_norm": 1.8984375,
      "learning_rate": 9.247363374880153e-05,
      "loss": 1.2426,
      "step": 6730
    },
    {
      "epoch": 1.0770306444524351,
      "grad_norm": 2.390625,
      "learning_rate": 9.231383828699265e-05,
      "loss": 1.0543,
      "step": 6740
    },
    {
      "epoch": 1.0786287906029006,
      "grad_norm": 2.015625,
      "learning_rate": 9.215404282518377e-05,
      "loss": 1.4474,
      "step": 6750
    },
    {
      "epoch": 1.0802269367533661,
      "grad_norm": 1.078125,
      "learning_rate": 9.199424736337489e-05,
      "loss": 1.2993,
      "step": 6760
    },
    {
      "epoch": 1.0818250829038316,
      "grad_norm": 3.109375,
      "learning_rate": 9.1834451901566e-05,
      "loss": 1.1997,
      "step": 6770
    },
    {
      "epoch": 1.083423229054297,
      "grad_norm": 2.828125,
      "learning_rate": 9.167465643975712e-05,
      "loss": 1.0596,
      "step": 6780
    },
    {
      "epoch": 1.0850213752047624,
      "grad_norm": 1.875,
      "learning_rate": 9.151486097794823e-05,
      "loss": 1.2849,
      "step": 6790
    },
    {
      "epoch": 1.0866195213552279,
      "grad_norm": 1.6484375,
      "learning_rate": 9.135506551613934e-05,
      "loss": 1.2189,
      "step": 6800
    },
    {
      "epoch": 1.0882176675056934,
      "grad_norm": 1.125,
      "learning_rate": 9.119527005433046e-05,
      "loss": 1.2288,
      "step": 6810
    },
    {
      "epoch": 1.0898158136561589,
      "grad_norm": 2.53125,
      "learning_rate": 9.103547459252157e-05,
      "loss": 1.5247,
      "step": 6820
    },
    {
      "epoch": 1.0914139598066244,
      "grad_norm": 2.84375,
      "learning_rate": 9.087567913071269e-05,
      "loss": 1.1501,
      "step": 6830
    },
    {
      "epoch": 1.0930121059570899,
      "grad_norm": 1.5625,
      "learning_rate": 9.071588366890381e-05,
      "loss": 1.3186,
      "step": 6840
    },
    {
      "epoch": 1.0946102521075551,
      "grad_norm": 2.265625,
      "learning_rate": 9.055608820709493e-05,
      "loss": 0.9743,
      "step": 6850
    },
    {
      "epoch": 1.0962083982580206,
      "grad_norm": 2.609375,
      "learning_rate": 9.039629274528604e-05,
      "loss": 1.2056,
      "step": 6860
    },
    {
      "epoch": 1.0978065444084861,
      "grad_norm": 2.28125,
      "learning_rate": 9.023649728347715e-05,
      "loss": 1.4844,
      "step": 6870
    },
    {
      "epoch": 1.0994046905589516,
      "grad_norm": 1.2109375,
      "learning_rate": 9.007670182166827e-05,
      "loss": 1.4316,
      "step": 6880
    },
    {
      "epoch": 1.1010028367094171,
      "grad_norm": 2.0625,
      "learning_rate": 8.991690635985938e-05,
      "loss": 1.3526,
      "step": 6890
    },
    {
      "epoch": 1.1026009828598826,
      "grad_norm": 0.00185394287109375,
      "learning_rate": 8.97571108980505e-05,
      "loss": 1.4948,
      "step": 6900
    },
    {
      "epoch": 1.1041991290103481,
      "grad_norm": 2.109375,
      "learning_rate": 8.959731543624161e-05,
      "loss": 1.4009,
      "step": 6910
    },
    {
      "epoch": 1.1057972751608134,
      "grad_norm": 1.6875,
      "learning_rate": 8.943751997443272e-05,
      "loss": 1.1044,
      "step": 6920
    },
    {
      "epoch": 1.1073954213112789,
      "grad_norm": 3.875,
      "learning_rate": 8.927772451262385e-05,
      "loss": 1.1998,
      "step": 6930
    },
    {
      "epoch": 1.1089935674617444,
      "grad_norm": 2.859375,
      "learning_rate": 8.911792905081497e-05,
      "loss": 1.0874,
      "step": 6940
    },
    {
      "epoch": 1.1105917136122099,
      "grad_norm": 1.6953125,
      "learning_rate": 8.895813358900608e-05,
      "loss": 1.1844,
      "step": 6950
    },
    {
      "epoch": 1.1121898597626754,
      "grad_norm": 1.6796875,
      "learning_rate": 8.87983381271972e-05,
      "loss": 1.0835,
      "step": 6960
    },
    {
      "epoch": 1.1137880059131406,
      "grad_norm": 2.703125,
      "learning_rate": 8.863854266538831e-05,
      "loss": 1.1228,
      "step": 6970
    },
    {
      "epoch": 1.1153861520636061,
      "grad_norm": 1.25,
      "learning_rate": 8.847874720357942e-05,
      "loss": 0.9774,
      "step": 6980
    },
    {
      "epoch": 1.1169842982140716,
      "grad_norm": 2.890625,
      "learning_rate": 8.831895174177054e-05,
      "loss": 1.3596,
      "step": 6990
    },
    {
      "epoch": 1.1185824443645371,
      "grad_norm": 2.109375,
      "learning_rate": 8.815915627996165e-05,
      "loss": 1.6855,
      "step": 7000
    },
    {
      "epoch": 1.1201805905150026,
      "grad_norm": 4.75,
      "learning_rate": 8.799936081815276e-05,
      "loss": 1.0523,
      "step": 7010
    },
    {
      "epoch": 1.1217787366654681,
      "grad_norm": 1.6796875,
      "learning_rate": 8.783956535634389e-05,
      "loss": 1.1462,
      "step": 7020
    },
    {
      "epoch": 1.1233768828159336,
      "grad_norm": 2.28125,
      "learning_rate": 8.7679769894535e-05,
      "loss": 1.3923,
      "step": 7030
    },
    {
      "epoch": 1.124975028966399,
      "grad_norm": 0.00144195556640625,
      "learning_rate": 8.751997443272612e-05,
      "loss": 1.1858,
      "step": 7040
    },
    {
      "epoch": 1.1265731751168644,
      "grad_norm": 2.234375,
      "learning_rate": 8.736017897091723e-05,
      "loss": 1.2666,
      "step": 7050
    },
    {
      "epoch": 1.1281713212673299,
      "grad_norm": 2.03125,
      "learning_rate": 8.720038350910835e-05,
      "loss": 1.3521,
      "step": 7060
    },
    {
      "epoch": 1.1297694674177954,
      "grad_norm": 2.984375,
      "learning_rate": 8.704058804729946e-05,
      "loss": 1.5753,
      "step": 7070
    },
    {
      "epoch": 1.1313676135682609,
      "grad_norm": 2.234375,
      "learning_rate": 8.688079258549057e-05,
      "loss": 1.0536,
      "step": 7080
    },
    {
      "epoch": 1.1329657597187264,
      "grad_norm": 5.03125,
      "learning_rate": 8.672099712368169e-05,
      "loss": 1.2028,
      "step": 7090
    },
    {
      "epoch": 1.1345639058691916,
      "grad_norm": 1.8671875,
      "learning_rate": 8.65612016618728e-05,
      "loss": 1.253,
      "step": 7100
    },
    {
      "epoch": 1.1361620520196571,
      "grad_norm": 6.34375,
      "learning_rate": 8.640140620006392e-05,
      "loss": 1.0818,
      "step": 7110
    },
    {
      "epoch": 1.1377601981701226,
      "grad_norm": 2.046875,
      "learning_rate": 8.624161073825504e-05,
      "loss": 1.1303,
      "step": 7120
    },
    {
      "epoch": 1.1393583443205881,
      "grad_norm": 2.109375,
      "learning_rate": 8.608181527644616e-05,
      "loss": 0.9156,
      "step": 7130
    },
    {
      "epoch": 1.1409564904710536,
      "grad_norm": 2.421875,
      "learning_rate": 8.592201981463727e-05,
      "loss": 1.4555,
      "step": 7140
    },
    {
      "epoch": 1.1425546366215191,
      "grad_norm": 2.421875,
      "learning_rate": 8.576222435282839e-05,
      "loss": 1.4006,
      "step": 7150
    },
    {
      "epoch": 1.1441527827719846,
      "grad_norm": 3.046875,
      "learning_rate": 8.56024288910195e-05,
      "loss": 1.5401,
      "step": 7160
    },
    {
      "epoch": 1.14575092892245,
      "grad_norm": 1.734375,
      "learning_rate": 8.544263342921061e-05,
      "loss": 1.201,
      "step": 7170
    },
    {
      "epoch": 1.1473490750729154,
      "grad_norm": 3.453125,
      "learning_rate": 8.528283796740173e-05,
      "loss": 0.6643,
      "step": 7180
    },
    {
      "epoch": 1.1489472212233809,
      "grad_norm": 2.015625,
      "learning_rate": 8.512304250559284e-05,
      "loss": 1.1965,
      "step": 7190
    },
    {
      "epoch": 1.1505453673738464,
      "grad_norm": 2.6875,
      "learning_rate": 8.496324704378395e-05,
      "loss": 1.2937,
      "step": 7200
    },
    {
      "epoch": 1.1521435135243119,
      "grad_norm": 4.0625,
      "learning_rate": 8.480345158197508e-05,
      "loss": 1.1774,
      "step": 7210
    },
    {
      "epoch": 1.1537416596747772,
      "grad_norm": 0.11474609375,
      "learning_rate": 8.46436561201662e-05,
      "loss": 1.3489,
      "step": 7220
    },
    {
      "epoch": 1.1553398058252426,
      "grad_norm": 1.5234375,
      "learning_rate": 8.448386065835731e-05,
      "loss": 1.5116,
      "step": 7230
    },
    {
      "epoch": 1.1569379519757081,
      "grad_norm": 1.1953125,
      "learning_rate": 8.432406519654842e-05,
      "loss": 1.4609,
      "step": 7240
    },
    {
      "epoch": 1.1585360981261736,
      "grad_norm": 1.609375,
      "learning_rate": 8.416426973473954e-05,
      "loss": 1.304,
      "step": 7250
    },
    {
      "epoch": 1.1601342442766391,
      "grad_norm": 2.578125,
      "learning_rate": 8.400447427293065e-05,
      "loss": 1.234,
      "step": 7260
    },
    {
      "epoch": 1.1617323904271046,
      "grad_norm": 2.171875,
      "learning_rate": 8.384467881112177e-05,
      "loss": 1.101,
      "step": 7270
    },
    {
      "epoch": 1.1633305365775701,
      "grad_norm": 1.4296875,
      "learning_rate": 8.368488334931288e-05,
      "loss": 1.6649,
      "step": 7280
    },
    {
      "epoch": 1.1649286827280354,
      "grad_norm": 1.1484375,
      "learning_rate": 8.3525087887504e-05,
      "loss": 0.9614,
      "step": 7290
    },
    {
      "epoch": 1.166526828878501,
      "grad_norm": 2.921875,
      "learning_rate": 8.336529242569512e-05,
      "loss": 1.5858,
      "step": 7300
    },
    {
      "epoch": 1.1681249750289664,
      "grad_norm": 2.03125,
      "learning_rate": 8.320549696388624e-05,
      "loss": 1.306,
      "step": 7310
    },
    {
      "epoch": 1.169723121179432,
      "grad_norm": 1.7890625,
      "learning_rate": 8.304570150207735e-05,
      "loss": 1.358,
      "step": 7320
    },
    {
      "epoch": 1.1713212673298974,
      "grad_norm": 3.046875,
      "learning_rate": 8.288590604026846e-05,
      "loss": 1.0893,
      "step": 7330
    },
    {
      "epoch": 1.1729194134803627,
      "grad_norm": 4.21875,
      "learning_rate": 8.272611057845956e-05,
      "loss": 0.8627,
      "step": 7340
    },
    {
      "epoch": 1.1745175596308282,
      "grad_norm": 1.625,
      "learning_rate": 8.256631511665069e-05,
      "loss": 1.2179,
      "step": 7350
    },
    {
      "epoch": 1.1761157057812937,
      "grad_norm": 1.203125,
      "learning_rate": 8.24065196548418e-05,
      "loss": 0.8319,
      "step": 7360
    },
    {
      "epoch": 1.1777138519317591,
      "grad_norm": 2.921875,
      "learning_rate": 8.224672419303292e-05,
      "loss": 1.4011,
      "step": 7370
    },
    {
      "epoch": 1.1793119980822246,
      "grad_norm": 1.1640625,
      "learning_rate": 8.208692873122403e-05,
      "loss": 1.6621,
      "step": 7380
    },
    {
      "epoch": 1.1809101442326901,
      "grad_norm": 1.75,
      "learning_rate": 8.192713326941516e-05,
      "loss": 1.1913,
      "step": 7390
    },
    {
      "epoch": 1.1825082903831556,
      "grad_norm": 2.9375,
      "learning_rate": 8.176733780760627e-05,
      "loss": 1.5208,
      "step": 7400
    },
    {
      "epoch": 1.184106436533621,
      "grad_norm": 2.328125,
      "learning_rate": 8.160754234579739e-05,
      "loss": 1.0622,
      "step": 7410
    },
    {
      "epoch": 1.1857045826840864,
      "grad_norm": 1.90625,
      "learning_rate": 8.14477468839885e-05,
      "loss": 1.4092,
      "step": 7420
    },
    {
      "epoch": 1.187302728834552,
      "grad_norm": 2.328125,
      "learning_rate": 8.12879514221796e-05,
      "loss": 1.3802,
      "step": 7430
    },
    {
      "epoch": 1.1889008749850174,
      "grad_norm": 2.140625,
      "learning_rate": 8.112815596037073e-05,
      "loss": 0.9937,
      "step": 7440
    },
    {
      "epoch": 1.190499021135483,
      "grad_norm": 1.3984375,
      "learning_rate": 8.096836049856184e-05,
      "loss": 1.2346,
      "step": 7450
    },
    {
      "epoch": 1.1920971672859484,
      "grad_norm": 2.515625,
      "learning_rate": 8.080856503675296e-05,
      "loss": 1.2616,
      "step": 7460
    },
    {
      "epoch": 1.1936953134364137,
      "grad_norm": 4.78125,
      "learning_rate": 8.064876957494407e-05,
      "loss": 1.3863,
      "step": 7470
    },
    {
      "epoch": 1.1952934595868792,
      "grad_norm": 1.671875,
      "learning_rate": 8.04889741131352e-05,
      "loss": 1.3782,
      "step": 7480
    },
    {
      "epoch": 1.1968916057373447,
      "grad_norm": 1.2890625,
      "learning_rate": 8.032917865132631e-05,
      "loss": 0.9829,
      "step": 7490
    },
    {
      "epoch": 1.1984897518878102,
      "grad_norm": 2.484375,
      "learning_rate": 8.016938318951743e-05,
      "loss": 1.5247,
      "step": 7500
    },
    {
      "epoch": 1.2000878980382756,
      "grad_norm": 1.3046875,
      "learning_rate": 8.000958772770854e-05,
      "loss": 1.0882,
      "step": 7510
    },
    {
      "epoch": 1.2016860441887411,
      "grad_norm": 2.765625,
      "learning_rate": 7.984979226589964e-05,
      "loss": 1.4244,
      "step": 7520
    },
    {
      "epoch": 1.2032841903392066,
      "grad_norm": 1.0859375,
      "learning_rate": 7.968999680409077e-05,
      "loss": 1.1471,
      "step": 7530
    },
    {
      "epoch": 1.204882336489672,
      "grad_norm": 0.21875,
      "learning_rate": 7.953020134228188e-05,
      "loss": 1.2337,
      "step": 7540
    },
    {
      "epoch": 1.2064804826401374,
      "grad_norm": 1.1953125,
      "learning_rate": 7.9370405880473e-05,
      "loss": 1.1174,
      "step": 7550
    },
    {
      "epoch": 1.208078628790603,
      "grad_norm": 12.4375,
      "learning_rate": 7.921061041866411e-05,
      "loss": 1.1709,
      "step": 7560
    },
    {
      "epoch": 1.2096767749410684,
      "grad_norm": 1.7265625,
      "learning_rate": 7.905081495685522e-05,
      "loss": 1.0587,
      "step": 7570
    },
    {
      "epoch": 1.211274921091534,
      "grad_norm": 2.96875,
      "learning_rate": 7.889101949504635e-05,
      "loss": 1.226,
      "step": 7580
    },
    {
      "epoch": 1.2128730672419992,
      "grad_norm": 6.1875,
      "learning_rate": 7.873122403323747e-05,
      "loss": 1.1929,
      "step": 7590
    },
    {
      "epoch": 1.2144712133924647,
      "grad_norm": 2.078125,
      "learning_rate": 7.857142857142858e-05,
      "loss": 1.3902,
      "step": 7600
    },
    {
      "epoch": 1.2160693595429302,
      "grad_norm": 3.015625,
      "learning_rate": 7.841163310961968e-05,
      "loss": 1.2682,
      "step": 7610
    },
    {
      "epoch": 1.2176675056933957,
      "grad_norm": 3.234375,
      "learning_rate": 7.825183764781081e-05,
      "loss": 1.1715,
      "step": 7620
    },
    {
      "epoch": 1.2192656518438612,
      "grad_norm": 2.15625,
      "learning_rate": 7.809204218600192e-05,
      "loss": 1.5257,
      "step": 7630
    },
    {
      "epoch": 1.2208637979943266,
      "grad_norm": 4.6875,
      "learning_rate": 7.793224672419304e-05,
      "loss": 1.2867,
      "step": 7640
    },
    {
      "epoch": 1.2224619441447921,
      "grad_norm": 2.34375,
      "learning_rate": 7.777245126238415e-05,
      "loss": 1.5418,
      "step": 7650
    },
    {
      "epoch": 1.2240600902952574,
      "grad_norm": 1.890625,
      "learning_rate": 7.761265580057526e-05,
      "loss": 1.0104,
      "step": 7660
    },
    {
      "epoch": 1.225658236445723,
      "grad_norm": 0.064453125,
      "learning_rate": 7.745286033876639e-05,
      "loss": 1.2123,
      "step": 7670
    },
    {
      "epoch": 1.2272563825961884,
      "grad_norm": 2.90625,
      "learning_rate": 7.72930648769575e-05,
      "loss": 1.0761,
      "step": 7680
    },
    {
      "epoch": 1.228854528746654,
      "grad_norm": 2.96875,
      "learning_rate": 7.713326941514862e-05,
      "loss": 1.2233,
      "step": 7690
    },
    {
      "epoch": 1.2304526748971194,
      "grad_norm": 1.7421875,
      "learning_rate": 7.697347395333972e-05,
      "loss": 1.4547,
      "step": 7700
    },
    {
      "epoch": 1.232050821047585,
      "grad_norm": 2.609375,
      "learning_rate": 7.681367849153085e-05,
      "loss": 1.5465,
      "step": 7710
    },
    {
      "epoch": 1.2336489671980502,
      "grad_norm": 1.734375,
      "learning_rate": 7.665388302972196e-05,
      "loss": 0.9945,
      "step": 7720
    },
    {
      "epoch": 1.2352471133485157,
      "grad_norm": 1.4609375,
      "learning_rate": 7.649408756791307e-05,
      "loss": 1.1977,
      "step": 7730
    },
    {
      "epoch": 1.2368452594989812,
      "grad_norm": 3.1875,
      "learning_rate": 7.633429210610419e-05,
      "loss": 1.462,
      "step": 7740
    },
    {
      "epoch": 1.2384434056494467,
      "grad_norm": 1.8828125,
      "learning_rate": 7.61744966442953e-05,
      "loss": 1.6924,
      "step": 7750
    },
    {
      "epoch": 1.2400415517999122,
      "grad_norm": 2.6875,
      "learning_rate": 7.601470118248643e-05,
      "loss": 1.2333,
      "step": 7760
    },
    {
      "epoch": 1.2416396979503777,
      "grad_norm": 1.0234375,
      "learning_rate": 7.585490572067754e-05,
      "loss": 1.4084,
      "step": 7770
    },
    {
      "epoch": 1.243237844100843,
      "grad_norm": 1.6640625,
      "learning_rate": 7.569511025886864e-05,
      "loss": 1.0073,
      "step": 7780
    },
    {
      "epoch": 1.2448359902513084,
      "grad_norm": 1.6484375,
      "learning_rate": 7.553531479705976e-05,
      "loss": 1.0902,
      "step": 7790
    },
    {
      "epoch": 1.246434136401774,
      "grad_norm": 2.125,
      "learning_rate": 7.537551933525087e-05,
      "loss": 1.4619,
      "step": 7800
    },
    {
      "epoch": 1.2480322825522394,
      "grad_norm": 1.875,
      "learning_rate": 7.5215723873442e-05,
      "loss": 1.2855,
      "step": 7810
    },
    {
      "epoch": 1.249630428702705,
      "grad_norm": 1.8984375,
      "learning_rate": 7.505592841163311e-05,
      "loss": 1.4977,
      "step": 7820
    },
    {
      "epoch": 1.2512285748531704,
      "grad_norm": 1.40625,
      "learning_rate": 7.489613294982423e-05,
      "loss": 1.3362,
      "step": 7830
    },
    {
      "epoch": 1.2528267210036357,
      "grad_norm": 2.65625,
      "learning_rate": 7.473633748801534e-05,
      "loss": 1.6144,
      "step": 7840
    },
    {
      "epoch": 1.2544248671541012,
      "grad_norm": 2.203125,
      "learning_rate": 7.457654202620647e-05,
      "loss": 1.5213,
      "step": 7850
    },
    {
      "epoch": 1.2560230133045667,
      "grad_norm": 1.140625,
      "learning_rate": 7.441674656439758e-05,
      "loss": 1.0046,
      "step": 7860
    },
    {
      "epoch": 1.2576211594550322,
      "grad_norm": 1.8203125,
      "learning_rate": 7.425695110258868e-05,
      "loss": 0.9834,
      "step": 7870
    },
    {
      "epoch": 1.2592193056054977,
      "grad_norm": 4.53125,
      "learning_rate": 7.40971556407798e-05,
      "loss": 1.2591,
      "step": 7880
    },
    {
      "epoch": 1.2608174517559632,
      "grad_norm": 2.25,
      "learning_rate": 7.393736017897091e-05,
      "loss": 1.0559,
      "step": 7890
    },
    {
      "epoch": 1.2624155979064287,
      "grad_norm": 2.296875,
      "learning_rate": 7.377756471716204e-05,
      "loss": 1.6419,
      "step": 7900
    },
    {
      "epoch": 1.264013744056894,
      "grad_norm": 2.828125,
      "learning_rate": 7.361776925535315e-05,
      "loss": 1.0809,
      "step": 7910
    },
    {
      "epoch": 1.2656118902073594,
      "grad_norm": 0.068359375,
      "learning_rate": 7.345797379354427e-05,
      "loss": 0.9129,
      "step": 7920
    },
    {
      "epoch": 1.267210036357825,
      "grad_norm": 3.203125,
      "learning_rate": 7.329817833173538e-05,
      "loss": 1.1233,
      "step": 7930
    },
    {
      "epoch": 1.2688081825082904,
      "grad_norm": 2.53125,
      "learning_rate": 7.313838286992651e-05,
      "loss": 1.3133,
      "step": 7940
    },
    {
      "epoch": 1.270406328658756,
      "grad_norm": 2.9375,
      "learning_rate": 7.297858740811762e-05,
      "loss": 1.5478,
      "step": 7950
    },
    {
      "epoch": 1.2720044748092212,
      "grad_norm": 3.921875,
      "learning_rate": 7.281879194630872e-05,
      "loss": 1.3163,
      "step": 7960
    },
    {
      "epoch": 1.2736026209596867,
      "grad_norm": 3.90625,
      "learning_rate": 7.265899648449984e-05,
      "loss": 1.4662,
      "step": 7970
    },
    {
      "epoch": 1.2752007671101522,
      "grad_norm": 3.375,
      "learning_rate": 7.249920102269095e-05,
      "loss": 1.5148,
      "step": 7980
    },
    {
      "epoch": 1.2767989132606177,
      "grad_norm": 1.046875,
      "learning_rate": 7.233940556088208e-05,
      "loss": 1.1848,
      "step": 7990
    },
    {
      "epoch": 1.2783970594110832,
      "grad_norm": 3.296875,
      "learning_rate": 7.217961009907319e-05,
      "loss": 1.7168,
      "step": 8000
    },
    {
      "epoch": 1.2799952055615487,
      "grad_norm": 1.875,
      "learning_rate": 7.20198146372643e-05,
      "loss": 1.2315,
      "step": 8010
    },
    {
      "epoch": 1.2815933517120142,
      "grad_norm": 2.109375,
      "learning_rate": 7.186001917545542e-05,
      "loss": 1.2219,
      "step": 8020
    },
    {
      "epoch": 1.2831914978624797,
      "grad_norm": 2.171875,
      "learning_rate": 7.170022371364653e-05,
      "loss": 1.4537,
      "step": 8030
    },
    {
      "epoch": 1.284789644012945,
      "grad_norm": 2.640625,
      "learning_rate": 7.154042825183766e-05,
      "loss": 1.3865,
      "step": 8040
    },
    {
      "epoch": 1.2863877901634104,
      "grad_norm": 4.59375,
      "learning_rate": 7.138063279002876e-05,
      "loss": 1.1366,
      "step": 8050
    },
    {
      "epoch": 1.287985936313876,
      "grad_norm": 2.296875,
      "learning_rate": 7.122083732821988e-05,
      "loss": 1.3268,
      "step": 8060
    },
    {
      "epoch": 1.2895840824643414,
      "grad_norm": 1.2890625,
      "learning_rate": 7.106104186641099e-05,
      "loss": 1.1809,
      "step": 8070
    },
    {
      "epoch": 1.2911822286148067,
      "grad_norm": 2.28125,
      "learning_rate": 7.090124640460212e-05,
      "loss": 1.2662,
      "step": 8080
    },
    {
      "epoch": 1.2927803747652722,
      "grad_norm": 2.15625,
      "learning_rate": 7.074145094279323e-05,
      "loss": 1.3442,
      "step": 8090
    },
    {
      "epoch": 1.2943785209157377,
      "grad_norm": 2.625,
      "learning_rate": 7.058165548098434e-05,
      "loss": 1.0362,
      "step": 8100
    },
    {
      "epoch": 1.2959766670662032,
      "grad_norm": 2.125,
      "learning_rate": 7.042186001917546e-05,
      "loss": 1.1545,
      "step": 8110
    },
    {
      "epoch": 1.2975748132166687,
      "grad_norm": 2.453125,
      "learning_rate": 7.026206455736657e-05,
      "loss": 1.0197,
      "step": 8120
    },
    {
      "epoch": 1.2991729593671342,
      "grad_norm": 1.71875,
      "learning_rate": 7.01022690955577e-05,
      "loss": 1.3033,
      "step": 8130
    },
    {
      "epoch": 1.3007711055175997,
      "grad_norm": 2.28125,
      "learning_rate": 6.99424736337488e-05,
      "loss": 1.4499,
      "step": 8140
    },
    {
      "epoch": 1.3023692516680652,
      "grad_norm": 3.828125,
      "learning_rate": 6.978267817193991e-05,
      "loss": 1.5835,
      "step": 8150
    },
    {
      "epoch": 1.3039673978185304,
      "grad_norm": 3.078125,
      "learning_rate": 6.962288271013103e-05,
      "loss": 1.5112,
      "step": 8160
    },
    {
      "epoch": 1.305565543968996,
      "grad_norm": 2.015625,
      "learning_rate": 6.946308724832216e-05,
      "loss": 1.0859,
      "step": 8170
    },
    {
      "epoch": 1.3071636901194614,
      "grad_norm": 5.78125,
      "learning_rate": 6.930329178651327e-05,
      "loss": 1.3289,
      "step": 8180
    },
    {
      "epoch": 1.308761836269927,
      "grad_norm": 3.515625,
      "learning_rate": 6.914349632470438e-05,
      "loss": 1.4297,
      "step": 8190
    },
    {
      "epoch": 1.3103599824203924,
      "grad_norm": 2.140625,
      "learning_rate": 6.89837008628955e-05,
      "loss": 1.3851,
      "step": 8200
    },
    {
      "epoch": 1.3119581285708577,
      "grad_norm": 3.03125,
      "learning_rate": 6.882390540108661e-05,
      "loss": 1.4739,
      "step": 8210
    },
    {
      "epoch": 1.3135562747213232,
      "grad_norm": 1.9140625,
      "learning_rate": 6.866410993927773e-05,
      "loss": 1.1481,
      "step": 8220
    },
    {
      "epoch": 1.3151544208717887,
      "grad_norm": 1.734375,
      "learning_rate": 6.850431447746884e-05,
      "loss": 1.2989,
      "step": 8230
    },
    {
      "epoch": 1.3167525670222542,
      "grad_norm": 3.890625,
      "learning_rate": 6.834451901565995e-05,
      "loss": 1.223,
      "step": 8240
    },
    {
      "epoch": 1.3183507131727197,
      "grad_norm": 1.2578125,
      "learning_rate": 6.818472355385107e-05,
      "loss": 1.1305,
      "step": 8250
    },
    {
      "epoch": 1.3199488593231852,
      "grad_norm": 1.2578125,
      "learning_rate": 6.802492809204218e-05,
      "loss": 1.2043,
      "step": 8260
    },
    {
      "epoch": 1.3215470054736507,
      "grad_norm": 1.78125,
      "learning_rate": 6.786513263023331e-05,
      "loss": 1.2079,
      "step": 8270
    },
    {
      "epoch": 1.323145151624116,
      "grad_norm": 1.6484375,
      "learning_rate": 6.770533716842442e-05,
      "loss": 1.283,
      "step": 8280
    },
    {
      "epoch": 1.3247432977745814,
      "grad_norm": 3.1875,
      "learning_rate": 6.754554170661554e-05,
      "loss": 1.5028,
      "step": 8290
    },
    {
      "epoch": 1.326341443925047,
      "grad_norm": 3.4375,
      "learning_rate": 6.738574624480665e-05,
      "loss": 1.427,
      "step": 8300
    },
    {
      "epoch": 1.3279395900755124,
      "grad_norm": 3.203125,
      "learning_rate": 6.722595078299776e-05,
      "loss": 1.6174,
      "step": 8310
    },
    {
      "epoch": 1.329537736225978,
      "grad_norm": 2.75,
      "learning_rate": 6.706615532118888e-05,
      "loss": 1.1464,
      "step": 8320
    },
    {
      "epoch": 1.3311358823764432,
      "grad_norm": 4.09375,
      "learning_rate": 6.690635985937999e-05,
      "loss": 0.9925,
      "step": 8330
    },
    {
      "epoch": 1.3327340285269087,
      "grad_norm": 3.984375,
      "learning_rate": 6.67465643975711e-05,
      "loss": 1.0061,
      "step": 8340
    },
    {
      "epoch": 1.3343321746773742,
      "grad_norm": 2.59375,
      "learning_rate": 6.658676893576222e-05,
      "loss": 0.9483,
      "step": 8350
    },
    {
      "epoch": 1.3359303208278397,
      "grad_norm": 3.515625,
      "learning_rate": 6.642697347395335e-05,
      "loss": 1.2965,
      "step": 8360
    },
    {
      "epoch": 1.3375284669783052,
      "grad_norm": 3.046875,
      "learning_rate": 6.626717801214446e-05,
      "loss": 1.3505,
      "step": 8370
    },
    {
      "epoch": 1.3391266131287707,
      "grad_norm": 2.765625,
      "learning_rate": 6.610738255033558e-05,
      "loss": 0.7775,
      "step": 8380
    },
    {
      "epoch": 1.3407247592792362,
      "grad_norm": 2.75,
      "learning_rate": 6.594758708852669e-05,
      "loss": 1.3879,
      "step": 8390
    },
    {
      "epoch": 1.3423229054297017,
      "grad_norm": 1.6171875,
      "learning_rate": 6.57877916267178e-05,
      "loss": 1.2186,
      "step": 8400
    },
    {
      "epoch": 1.343921051580167,
      "grad_norm": 1.6875,
      "learning_rate": 6.562799616490892e-05,
      "loss": 1.2522,
      "step": 8410
    },
    {
      "epoch": 1.3455191977306324,
      "grad_norm": 3.875,
      "learning_rate": 6.546820070310003e-05,
      "loss": 1.1778,
      "step": 8420
    },
    {
      "epoch": 1.347117343881098,
      "grad_norm": 2.234375,
      "learning_rate": 6.530840524129115e-05,
      "loss": 1.4042,
      "step": 8430
    },
    {
      "epoch": 1.3487154900315634,
      "grad_norm": 1.1484375,
      "learning_rate": 6.514860977948226e-05,
      "loss": 1.32,
      "step": 8440
    },
    {
      "epoch": 1.3503136361820287,
      "grad_norm": 2.734375,
      "learning_rate": 6.498881431767339e-05,
      "loss": 1.1737,
      "step": 8450
    },
    {
      "epoch": 1.3519117823324942,
      "grad_norm": 2.265625,
      "learning_rate": 6.48290188558645e-05,
      "loss": 1.3465,
      "step": 8460
    },
    {
      "epoch": 1.3535099284829597,
      "grad_norm": 1.8046875,
      "learning_rate": 6.466922339405561e-05,
      "loss": 1.4015,
      "step": 8470
    },
    {
      "epoch": 1.3551080746334252,
      "grad_norm": 2.1875,
      "learning_rate": 6.450942793224673e-05,
      "loss": 1.5526,
      "step": 8480
    },
    {
      "epoch": 1.3567062207838907,
      "grad_norm": 4.25,
      "learning_rate": 6.434963247043784e-05,
      "loss": 1.2821,
      "step": 8490
    },
    {
      "epoch": 1.3583043669343562,
      "grad_norm": 1.703125,
      "learning_rate": 6.418983700862896e-05,
      "loss": 1.1489,
      "step": 8500
    },
    {
      "epoch": 1.3599025130848217,
      "grad_norm": 2.28125,
      "learning_rate": 6.403004154682007e-05,
      "loss": 1.3265,
      "step": 8510
    },
    {
      "epoch": 1.3615006592352872,
      "grad_norm": 1.1015625,
      "learning_rate": 6.387024608501118e-05,
      "loss": 1.3733,
      "step": 8520
    },
    {
      "epoch": 1.3630988053857525,
      "grad_norm": 2.0625,
      "learning_rate": 6.37104506232023e-05,
      "loss": 1.2282,
      "step": 8530
    },
    {
      "epoch": 1.364696951536218,
      "grad_norm": 1.4296875,
      "learning_rate": 6.355065516139343e-05,
      "loss": 1.2592,
      "step": 8540
    },
    {
      "epoch": 1.3662950976866834,
      "grad_norm": 1.9765625,
      "learning_rate": 6.339085969958454e-05,
      "loss": 1.5243,
      "step": 8550
    },
    {
      "epoch": 1.367893243837149,
      "grad_norm": 3.78125,
      "learning_rate": 6.323106423777565e-05,
      "loss": 1.3593,
      "step": 8560
    },
    {
      "epoch": 1.3694913899876144,
      "grad_norm": 3.0,
      "learning_rate": 6.307126877596677e-05,
      "loss": 1.4182,
      "step": 8570
    },
    {
      "epoch": 1.3710895361380797,
      "grad_norm": 4.887580871582031e-05,
      "learning_rate": 6.291147331415788e-05,
      "loss": 1.0922,
      "step": 8580
    },
    {
      "epoch": 1.3726876822885452,
      "grad_norm": 1.203125,
      "learning_rate": 6.2751677852349e-05,
      "loss": 1.0762,
      "step": 8590
    },
    {
      "epoch": 1.3742858284390107,
      "grad_norm": 2.84375,
      "learning_rate": 6.259188239054011e-05,
      "loss": 1.2413,
      "step": 8600
    },
    {
      "epoch": 1.3758839745894762,
      "grad_norm": 2.5,
      "learning_rate": 6.243208692873122e-05,
      "loss": 1.3011,
      "step": 8610
    },
    {
      "epoch": 1.3774821207399417,
      "grad_norm": 2.34375,
      "learning_rate": 6.227229146692234e-05,
      "loss": 0.8076,
      "step": 8620
    },
    {
      "epoch": 1.3790802668904072,
      "grad_norm": 1.2578125,
      "learning_rate": 6.211249600511346e-05,
      "loss": 1.1134,
      "step": 8630
    },
    {
      "epoch": 1.3806784130408727,
      "grad_norm": 3.4375,
      "learning_rate": 6.195270054330458e-05,
      "loss": 1.0335,
      "step": 8640
    },
    {
      "epoch": 1.3822765591913382,
      "grad_norm": 2.015625,
      "learning_rate": 6.179290508149569e-05,
      "loss": 0.97,
      "step": 8650
    },
    {
      "epoch": 1.3838747053418035,
      "grad_norm": 1.4453125,
      "learning_rate": 6.16331096196868e-05,
      "loss": 1.0404,
      "step": 8660
    },
    {
      "epoch": 1.385472851492269,
      "grad_norm": 3.421875,
      "learning_rate": 6.147331415787792e-05,
      "loss": 1.2049,
      "step": 8670
    },
    {
      "epoch": 1.3870709976427344,
      "grad_norm": 2.828125,
      "learning_rate": 6.131351869606903e-05,
      "loss": 1.1714,
      "step": 8680
    },
    {
      "epoch": 1.3886691437932,
      "grad_norm": 2.859375,
      "learning_rate": 6.115372323426015e-05,
      "loss": 1.1553,
      "step": 8690
    },
    {
      "epoch": 1.3902672899436652,
      "grad_norm": 5.96875,
      "learning_rate": 6.099392777245126e-05,
      "loss": 1.2308,
      "step": 8700
    },
    {
      "epoch": 1.3918654360941307,
      "grad_norm": 4.9375,
      "learning_rate": 6.0834132310642376e-05,
      "loss": 1.5274,
      "step": 8710
    },
    {
      "epoch": 1.3934635822445962,
      "grad_norm": 1.984375,
      "learning_rate": 6.067433684883349e-05,
      "loss": 1.4277,
      "step": 8720
    },
    {
      "epoch": 1.3950617283950617,
      "grad_norm": 3.375,
      "learning_rate": 6.051454138702462e-05,
      "loss": 1.3721,
      "step": 8730
    },
    {
      "epoch": 1.3966598745455272,
      "grad_norm": 5.125,
      "learning_rate": 6.0354745925215725e-05,
      "loss": 1.6183,
      "step": 8740
    },
    {
      "epoch": 1.3982580206959927,
      "grad_norm": 2.578125,
      "learning_rate": 6.019495046340684e-05,
      "loss": 1.0686,
      "step": 8750
    },
    {
      "epoch": 1.3998561668464582,
      "grad_norm": 1.578125,
      "learning_rate": 6.003515500159795e-05,
      "loss": 1.0668,
      "step": 8760
    },
    {
      "epoch": 1.4014543129969237,
      "grad_norm": 1.34375,
      "learning_rate": 5.987535953978908e-05,
      "loss": 1.5246,
      "step": 8770
    },
    {
      "epoch": 1.403052459147389,
      "grad_norm": 1.9921875,
      "learning_rate": 5.9715564077980194e-05,
      "loss": 1.3125,
      "step": 8780
    },
    {
      "epoch": 1.4046506052978545,
      "grad_norm": 1.5546875,
      "learning_rate": 5.95557686161713e-05,
      "loss": 1.2121,
      "step": 8790
    },
    {
      "epoch": 1.40624875144832,
      "grad_norm": 3.15625,
      "learning_rate": 5.9395973154362415e-05,
      "loss": 1.4791,
      "step": 8800
    },
    {
      "epoch": 1.4078468975987855,
      "grad_norm": 1.75,
      "learning_rate": 5.923617769255353e-05,
      "loss": 1.0068,
      "step": 8810
    },
    {
      "epoch": 1.409445043749251,
      "grad_norm": 2.140625,
      "learning_rate": 5.9076382230744656e-05,
      "loss": 1.4605,
      "step": 8820
    },
    {
      "epoch": 1.4110431898997162,
      "grad_norm": 2.484375,
      "learning_rate": 5.8916586768935763e-05,
      "loss": 1.3218,
      "step": 8830
    },
    {
      "epoch": 1.4126413360501817,
      "grad_norm": 3.5,
      "learning_rate": 5.875679130712688e-05,
      "loss": 1.0187,
      "step": 8840
    },
    {
      "epoch": 1.4142394822006472,
      "grad_norm": 2.234375,
      "learning_rate": 5.859699584531799e-05,
      "loss": 1.1878,
      "step": 8850
    },
    {
      "epoch": 1.4158376283511127,
      "grad_norm": 1.9375,
      "learning_rate": 5.843720038350912e-05,
      "loss": 1.432,
      "step": 8860
    },
    {
      "epoch": 1.4174357745015782,
      "grad_norm": 3.421875,
      "learning_rate": 5.8277404921700226e-05,
      "loss": 1.4107,
      "step": 8870
    },
    {
      "epoch": 1.4190339206520437,
      "grad_norm": 1.859375,
      "learning_rate": 5.811760945989134e-05,
      "loss": 0.9862,
      "step": 8880
    },
    {
      "epoch": 1.4206320668025092,
      "grad_norm": 3.921875,
      "learning_rate": 5.7957813998082454e-05,
      "loss": 1.2986,
      "step": 8890
    },
    {
      "epoch": 1.4222302129529745,
      "grad_norm": 2.09375,
      "learning_rate": 5.779801853627357e-05,
      "loss": 1.375,
      "step": 8900
    },
    {
      "epoch": 1.42382835910344,
      "grad_norm": 2.890625,
      "learning_rate": 5.7638223074464695e-05,
      "loss": 1.2155,
      "step": 8910
    },
    {
      "epoch": 1.4254265052539055,
      "grad_norm": 3.0625,
      "learning_rate": 5.74784276126558e-05,
      "loss": 1.2482,
      "step": 8920
    },
    {
      "epoch": 1.427024651404371,
      "grad_norm": 3.375,
      "learning_rate": 5.7318632150846916e-05,
      "loss": 1.5133,
      "step": 8930
    },
    {
      "epoch": 1.4286227975548365,
      "grad_norm": 1.65625,
      "learning_rate": 5.715883668903803e-05,
      "loss": 0.8098,
      "step": 8940
    },
    {
      "epoch": 1.4302209437053017,
      "grad_norm": 2.328125,
      "learning_rate": 5.6999041227229144e-05,
      "loss": 1.4342,
      "step": 8950
    },
    {
      "epoch": 1.4318190898557672,
      "grad_norm": 2.109375,
      "learning_rate": 5.6839245765420265e-05,
      "loss": 1.3222,
      "step": 8960
    },
    {
      "epoch": 1.4334172360062327,
      "grad_norm": 2.171875,
      "learning_rate": 5.667945030361138e-05,
      "loss": 1.4424,
      "step": 8970
    },
    {
      "epoch": 1.4350153821566982,
      "grad_norm": 4.71875,
      "learning_rate": 5.651965484180249e-05,
      "loss": 1.1893,
      "step": 8980
    },
    {
      "epoch": 1.4366135283071637,
      "grad_norm": 2.0,
      "learning_rate": 5.635985937999361e-05,
      "loss": 1.4835,
      "step": 8990
    },
    {
      "epoch": 1.4382116744576292,
      "grad_norm": 3.90625,
      "learning_rate": 5.6200063918184734e-05,
      "loss": 1.5623,
      "step": 9000
    },
    {
      "epoch": 1.4398098206080947,
      "grad_norm": 2.171875,
      "learning_rate": 5.604026845637584e-05,
      "loss": 1.1996,
      "step": 9010
    },
    {
      "epoch": 1.4414079667585602,
      "grad_norm": 3.125,
      "learning_rate": 5.5880472994566955e-05,
      "loss": 0.9482,
      "step": 9020
    },
    {
      "epoch": 1.4430061129090255,
      "grad_norm": 4.5625,
      "learning_rate": 5.572067753275807e-05,
      "loss": 1.5581,
      "step": 9030
    },
    {
      "epoch": 1.444604259059491,
      "grad_norm": 1.640625,
      "learning_rate": 5.556088207094918e-05,
      "loss": 1.3051,
      "step": 9040
    },
    {
      "epoch": 1.4462024052099565,
      "grad_norm": 4.21875,
      "learning_rate": 5.5401086609140304e-05,
      "loss": 1.1391,
      "step": 9050
    },
    {
      "epoch": 1.447800551360422,
      "grad_norm": 2.46875,
      "learning_rate": 5.524129114733142e-05,
      "loss": 1.4817,
      "step": 9060
    },
    {
      "epoch": 1.4493986975108872,
      "grad_norm": 2.53125,
      "learning_rate": 5.508149568552253e-05,
      "loss": 1.1117,
      "step": 9070
    },
    {
      "epoch": 1.4509968436613527,
      "grad_norm": 2.515625,
      "learning_rate": 5.4921700223713646e-05,
      "loss": 1.5724,
      "step": 9080
    },
    {
      "epoch": 1.4525949898118182,
      "grad_norm": 1.3125,
      "learning_rate": 5.4761904761904766e-05,
      "loss": 1.2384,
      "step": 9090
    },
    {
      "epoch": 1.4541931359622837,
      "grad_norm": 1.890625,
      "learning_rate": 5.460210930009588e-05,
      "loss": 1.4268,
      "step": 9100
    },
    {
      "epoch": 1.4557912821127492,
      "grad_norm": 1.2578125,
      "learning_rate": 5.4442313838286994e-05,
      "loss": 1.2202,
      "step": 9110
    },
    {
      "epoch": 1.4573894282632147,
      "grad_norm": 1.90625,
      "learning_rate": 5.428251837647811e-05,
      "loss": 1.323,
      "step": 9120
    },
    {
      "epoch": 1.4589875744136802,
      "grad_norm": 1.984375,
      "learning_rate": 5.412272291466922e-05,
      "loss": 1.1507,
      "step": 9130
    },
    {
      "epoch": 1.4605857205641457,
      "grad_norm": 2.03125,
      "learning_rate": 5.396292745286034e-05,
      "loss": 1.3952,
      "step": 9140
    },
    {
      "epoch": 1.462183866714611,
      "grad_norm": 2.78125,
      "learning_rate": 5.380313199105146e-05,
      "loss": 1.2552,
      "step": 9150
    },
    {
      "epoch": 1.4637820128650765,
      "grad_norm": 2.078125,
      "learning_rate": 5.364333652924257e-05,
      "loss": 1.3238,
      "step": 9160
    },
    {
      "epoch": 1.465380159015542,
      "grad_norm": 2.78125,
      "learning_rate": 5.3483541067433685e-05,
      "loss": 1.2185,
      "step": 9170
    },
    {
      "epoch": 1.4669783051660075,
      "grad_norm": 3.140625,
      "learning_rate": 5.33237456056248e-05,
      "loss": 1.2851,
      "step": 9180
    },
    {
      "epoch": 1.468576451316473,
      "grad_norm": 3.265625,
      "learning_rate": 5.316395014381592e-05,
      "loss": 1.4027,
      "step": 9190
    },
    {
      "epoch": 1.4701745974669382,
      "grad_norm": 1.21875,
      "learning_rate": 5.300415468200703e-05,
      "loss": 1.062,
      "step": 9200
    },
    {
      "epoch": 1.4717727436174037,
      "grad_norm": 2.96875,
      "learning_rate": 5.284435922019815e-05,
      "loss": 1.1951,
      "step": 9210
    },
    {
      "epoch": 1.4733708897678692,
      "grad_norm": 1.390625,
      "learning_rate": 5.268456375838926e-05,
      "loss": 0.9536,
      "step": 9220
    },
    {
      "epoch": 1.4749690359183347,
      "grad_norm": 2.53125,
      "learning_rate": 5.252476829658038e-05,
      "loss": 1.6967,
      "step": 9230
    },
    {
      "epoch": 1.4765671820688002,
      "grad_norm": 2.875,
      "learning_rate": 5.2364972834771496e-05,
      "loss": 1.3391,
      "step": 9240
    },
    {
      "epoch": 1.4781653282192657,
      "grad_norm": 3.015625,
      "learning_rate": 5.220517737296261e-05,
      "loss": 1.195,
      "step": 9250
    },
    {
      "epoch": 1.4797634743697312,
      "grad_norm": 5.625,
      "learning_rate": 5.2045381911153724e-05,
      "loss": 1.3403,
      "step": 9260
    },
    {
      "epoch": 1.4813616205201967,
      "grad_norm": 2.359375,
      "learning_rate": 5.188558644934484e-05,
      "loss": 1.2023,
      "step": 9270
    },
    {
      "epoch": 1.482959766670662,
      "grad_norm": 2.546875,
      "learning_rate": 5.172579098753596e-05,
      "loss": 1.7741,
      "step": 9280
    },
    {
      "epoch": 1.4845579128211275,
      "grad_norm": 1.6953125,
      "learning_rate": 5.156599552572707e-05,
      "loss": 1.2219,
      "step": 9290
    },
    {
      "epoch": 1.486156058971593,
      "grad_norm": 2.25,
      "learning_rate": 5.1406200063918186e-05,
      "loss": 1.68,
      "step": 9300
    },
    {
      "epoch": 1.4877542051220585,
      "grad_norm": 3.15625,
      "learning_rate": 5.12464046021093e-05,
      "loss": 1.0464,
      "step": 9310
    },
    {
      "epoch": 1.4893523512725237,
      "grad_norm": 2.765625,
      "learning_rate": 5.108660914030042e-05,
      "loss": 0.8564,
      "step": 9320
    },
    {
      "epoch": 1.4909504974229892,
      "grad_norm": 3.046875,
      "learning_rate": 5.0926813678491535e-05,
      "loss": 1.2797,
      "step": 9330
    },
    {
      "epoch": 1.4925486435734547,
      "grad_norm": 1.171875,
      "learning_rate": 5.076701821668265e-05,
      "loss": 1.2467,
      "step": 9340
    },
    {
      "epoch": 1.4941467897239202,
      "grad_norm": 2.53125,
      "learning_rate": 5.060722275487376e-05,
      "loss": 1.4596,
      "step": 9350
    },
    {
      "epoch": 1.4957449358743857,
      "grad_norm": 2.75,
      "learning_rate": 5.0447427293064877e-05,
      "loss": 1.2554,
      "step": 9360
    },
    {
      "epoch": 1.4973430820248512,
      "grad_norm": 2.34375,
      "learning_rate": 5.0287631831256e-05,
      "loss": 1.2734,
      "step": 9370
    },
    {
      "epoch": 1.4989412281753167,
      "grad_norm": 2.53125,
      "learning_rate": 5.012783636944711e-05,
      "loss": 1.5193,
      "step": 9380
    },
    {
      "epoch": 1.5005393743257822,
      "grad_norm": 1.2109375,
      "learning_rate": 4.9968040907638225e-05,
      "loss": 1.2191,
      "step": 9390
    },
    {
      "epoch": 1.5021375204762477,
      "grad_norm": 0.232421875,
      "learning_rate": 4.9808245445829346e-05,
      "loss": 0.9023,
      "step": 9400
    },
    {
      "epoch": 1.503735666626713,
      "grad_norm": 1.7421875,
      "learning_rate": 4.964844998402045e-05,
      "loss": 1.4496,
      "step": 9410
    },
    {
      "epoch": 1.5053338127771785,
      "grad_norm": 0.000522613525390625,
      "learning_rate": 4.948865452221157e-05,
      "loss": 1.0345,
      "step": 9420
    },
    {
      "epoch": 1.506931958927644,
      "grad_norm": 4.1875,
      "learning_rate": 4.932885906040269e-05,
      "loss": 1.2271,
      "step": 9430
    },
    {
      "epoch": 1.5085301050781093,
      "grad_norm": 2.109375,
      "learning_rate": 4.91690635985938e-05,
      "loss": 1.3536,
      "step": 9440
    },
    {
      "epoch": 1.5101282512285747,
      "grad_norm": 6.1875,
      "learning_rate": 4.900926813678492e-05,
      "loss": 1.1391,
      "step": 9450
    },
    {
      "epoch": 1.5117263973790402,
      "grad_norm": 2.296875,
      "learning_rate": 4.884947267497603e-05,
      "loss": 1.1154,
      "step": 9460
    },
    {
      "epoch": 1.5133245435295057,
      "grad_norm": 1.8671875,
      "learning_rate": 4.868967721316715e-05,
      "loss": 1.3362,
      "step": 9470
    },
    {
      "epoch": 1.5149226896799712,
      "grad_norm": 2.359375,
      "learning_rate": 4.8529881751358264e-05,
      "loss": 1.3425,
      "step": 9480
    },
    {
      "epoch": 1.5165208358304367,
      "grad_norm": 2.828125,
      "learning_rate": 4.837008628954938e-05,
      "loss": 1.5281,
      "step": 9490
    },
    {
      "epoch": 1.5181189819809022,
      "grad_norm": 2.5625,
      "learning_rate": 4.821029082774049e-05,
      "loss": 0.9772,
      "step": 9500
    },
    {
      "epoch": 1.5197171281313677,
      "grad_norm": 3.078125,
      "learning_rate": 4.8050495365931606e-05,
      "loss": 0.9256,
      "step": 9510
    },
    {
      "epoch": 1.5213152742818332,
      "grad_norm": 2.484375,
      "learning_rate": 4.789069990412273e-05,
      "loss": 1.1046,
      "step": 9520
    },
    {
      "epoch": 1.5229134204322985,
      "grad_norm": 2.15625,
      "learning_rate": 4.773090444231384e-05,
      "loss": 1.3927,
      "step": 9530
    },
    {
      "epoch": 1.524511566582764,
      "grad_norm": 2.0625,
      "learning_rate": 4.757110898050496e-05,
      "loss": 1.3408,
      "step": 9540
    },
    {
      "epoch": 1.5261097127332295,
      "grad_norm": 1.3125,
      "learning_rate": 4.741131351869607e-05,
      "loss": 1.0653,
      "step": 9550
    },
    {
      "epoch": 1.5277078588836948,
      "grad_norm": 1.9375,
      "learning_rate": 4.725151805688718e-05,
      "loss": 1.2195,
      "step": 9560
    },
    {
      "epoch": 1.5293060050341603,
      "grad_norm": 1.6796875,
      "learning_rate": 4.70917225950783e-05,
      "loss": 1.3237,
      "step": 9570
    },
    {
      "epoch": 1.5309041511846257,
      "grad_norm": 2.90625,
      "learning_rate": 4.693192713326942e-05,
      "loss": 1.537,
      "step": 9580
    },
    {
      "epoch": 1.5325022973350912,
      "grad_norm": 2.390625,
      "learning_rate": 4.677213167146053e-05,
      "loss": 1.5774,
      "step": 9590
    },
    {
      "epoch": 1.5341004434855567,
      "grad_norm": 3.734375,
      "learning_rate": 4.6612336209651645e-05,
      "loss": 1.1309,
      "step": 9600
    },
    {
      "epoch": 1.5356985896360222,
      "grad_norm": 6.71875,
      "learning_rate": 4.6452540747842766e-05,
      "loss": 1.5329,
      "step": 9610
    },
    {
      "epoch": 1.5372967357864877,
      "grad_norm": 3.515625,
      "learning_rate": 4.629274528603388e-05,
      "loss": 1.1672,
      "step": 9620
    },
    {
      "epoch": 1.5388948819369532,
      "grad_norm": 2.65625,
      "learning_rate": 4.6132949824224994e-05,
      "loss": 1.1239,
      "step": 9630
    },
    {
      "epoch": 1.5404930280874187,
      "grad_norm": 0.1494140625,
      "learning_rate": 4.597315436241611e-05,
      "loss": 0.9058,
      "step": 9640
    },
    {
      "epoch": 1.542091174237884,
      "grad_norm": 1.9609375,
      "learning_rate": 4.581335890060722e-05,
      "loss": 1.1258,
      "step": 9650
    },
    {
      "epoch": 1.5436893203883495,
      "grad_norm": 1.7265625,
      "learning_rate": 4.565356343879834e-05,
      "loss": 1.0534,
      "step": 9660
    },
    {
      "epoch": 1.545287466538815,
      "grad_norm": 0.2421875,
      "learning_rate": 4.5493767976989456e-05,
      "loss": 1.1949,
      "step": 9670
    },
    {
      "epoch": 1.5468856126892805,
      "grad_norm": 1.8828125,
      "learning_rate": 4.533397251518057e-05,
      "loss": 1.3991,
      "step": 9680
    },
    {
      "epoch": 1.5484837588397458,
      "grad_norm": 2.578125,
      "learning_rate": 4.5174177053371684e-05,
      "loss": 1.3883,
      "step": 9690
    },
    {
      "epoch": 1.5500819049902113,
      "grad_norm": 2.296875,
      "learning_rate": 4.5014381591562805e-05,
      "loss": 1.2998,
      "step": 9700
    },
    {
      "epoch": 1.5516800511406768,
      "grad_norm": 2.203125,
      "learning_rate": 4.485458612975392e-05,
      "loss": 1.2405,
      "step": 9710
    },
    {
      "epoch": 1.5532781972911422,
      "grad_norm": 2.140625,
      "learning_rate": 4.469479066794503e-05,
      "loss": 1.1904,
      "step": 9720
    },
    {
      "epoch": 1.5548763434416077,
      "grad_norm": 2.9375,
      "learning_rate": 4.4534995206136146e-05,
      "loss": 1.3481,
      "step": 9730
    },
    {
      "epoch": 1.5564744895920732,
      "grad_norm": 1.6953125,
      "learning_rate": 4.437519974432726e-05,
      "loss": 1.4791,
      "step": 9740
    },
    {
      "epoch": 1.5580726357425387,
      "grad_norm": 1.9140625,
      "learning_rate": 4.421540428251838e-05,
      "loss": 1.1366,
      "step": 9750
    },
    {
      "epoch": 1.5596707818930042,
      "grad_norm": 2.140625,
      "learning_rate": 4.4055608820709495e-05,
      "loss": 1.0828,
      "step": 9760
    },
    {
      "epoch": 1.5612689280434697,
      "grad_norm": 2.3125,
      "learning_rate": 4.389581335890061e-05,
      "loss": 1.1994,
      "step": 9770
    },
    {
      "epoch": 1.562867074193935,
      "grad_norm": 2.109375,
      "learning_rate": 4.373601789709172e-05,
      "loss": 1.2215,
      "step": 9780
    },
    {
      "epoch": 1.5644652203444005,
      "grad_norm": 2.40625,
      "learning_rate": 4.357622243528284e-05,
      "loss": 1.1581,
      "step": 9790
    },
    {
      "epoch": 1.566063366494866,
      "grad_norm": 3.296875,
      "learning_rate": 4.341642697347396e-05,
      "loss": 1.1657,
      "step": 9800
    },
    {
      "epoch": 1.5676615126453313,
      "grad_norm": 2.296875,
      "learning_rate": 4.325663151166507e-05,
      "loss": 0.9629,
      "step": 9810
    },
    {
      "epoch": 1.5692596587957968,
      "grad_norm": 5.0,
      "learning_rate": 4.3096836049856185e-05,
      "loss": 1.3874,
      "step": 9820
    },
    {
      "epoch": 1.5708578049462623,
      "grad_norm": 3.796875,
      "learning_rate": 4.29370405880473e-05,
      "loss": 1.4611,
      "step": 9830
    },
    {
      "epoch": 1.5724559510967278,
      "grad_norm": 1.7890625,
      "learning_rate": 4.277724512623842e-05,
      "loss": 0.8176,
      "step": 9840
    },
    {
      "epoch": 1.5740540972471933,
      "grad_norm": 2.421875,
      "learning_rate": 4.2617449664429534e-05,
      "loss": 1.2337,
      "step": 9850
    },
    {
      "epoch": 1.5756522433976587,
      "grad_norm": 3.53125,
      "learning_rate": 4.245765420262065e-05,
      "loss": 1.3409,
      "step": 9860
    },
    {
      "epoch": 1.5772503895481242,
      "grad_norm": 0.0003147125244140625,
      "learning_rate": 4.229785874081176e-05,
      "loss": 1.191,
      "step": 9870
    },
    {
      "epoch": 1.5788485356985897,
      "grad_norm": 1.1875,
      "learning_rate": 4.2138063279002876e-05,
      "loss": 1.5337,
      "step": 9880
    },
    {
      "epoch": 1.5804466818490552,
      "grad_norm": 1.875,
      "learning_rate": 4.1978267817193996e-05,
      "loss": 1.2284,
      "step": 9890
    },
    {
      "epoch": 1.5820448279995205,
      "grad_norm": 1.6953125,
      "learning_rate": 4.181847235538511e-05,
      "loss": 1.3786,
      "step": 9900
    },
    {
      "epoch": 1.583642974149986,
      "grad_norm": 7.375,
      "learning_rate": 4.1658676893576224e-05,
      "loss": 1.5886,
      "step": 9910
    },
    {
      "epoch": 1.5852411203004515,
      "grad_norm": 1.2578125,
      "learning_rate": 4.149888143176734e-05,
      "loss": 0.9968,
      "step": 9920
    },
    {
      "epoch": 1.5868392664509168,
      "grad_norm": 0.349609375,
      "learning_rate": 4.133908596995846e-05,
      "loss": 1.4466,
      "step": 9930
    },
    {
      "epoch": 1.5884374126013823,
      "grad_norm": 1.7578125,
      "learning_rate": 4.117929050814957e-05,
      "loss": 1.3049,
      "step": 9940
    },
    {
      "epoch": 1.5900355587518478,
      "grad_norm": 3.9375,
      "learning_rate": 4.101949504634069e-05,
      "loss": 1.0769,
      "step": 9950
    },
    {
      "epoch": 1.5916337049023133,
      "grad_norm": 1.96875,
      "learning_rate": 4.08596995845318e-05,
      "loss": 1.3806,
      "step": 9960
    },
    {
      "epoch": 1.5932318510527788,
      "grad_norm": 1.453125,
      "learning_rate": 4.0699904122722915e-05,
      "loss": 1.4107,
      "step": 9970
    },
    {
      "epoch": 1.5948299972032443,
      "grad_norm": 1.859375,
      "learning_rate": 4.0540108660914035e-05,
      "loss": 1.2353,
      "step": 9980
    },
    {
      "epoch": 1.5964281433537097,
      "grad_norm": 1.875,
      "learning_rate": 4.038031319910515e-05,
      "loss": 1.3055,
      "step": 9990
    },
    {
      "epoch": 1.5980262895041752,
      "grad_norm": 1.953125,
      "learning_rate": 4.022051773729626e-05,
      "loss": 1.3757,
      "step": 10000
    },
    {
      "epoch": 1.5996244356546407,
      "grad_norm": 5.46875,
      "learning_rate": 4.006072227548738e-05,
      "loss": 1.2116,
      "step": 10010
    },
    {
      "epoch": 1.6012225818051062,
      "grad_norm": 2.484375,
      "learning_rate": 3.990092681367849e-05,
      "loss": 1.1388,
      "step": 10020
    },
    {
      "epoch": 1.6028207279555715,
      "grad_norm": 1.2265625,
      "learning_rate": 3.974113135186961e-05,
      "loss": 1.4506,
      "step": 10030
    },
    {
      "epoch": 1.604418874106037,
      "grad_norm": 2.640625,
      "learning_rate": 3.958133589006072e-05,
      "loss": 1.5131,
      "step": 10040
    },
    {
      "epoch": 1.6060170202565025,
      "grad_norm": 3.65625,
      "learning_rate": 3.942154042825184e-05,
      "loss": 0.9481,
      "step": 10050
    },
    {
      "epoch": 1.6076151664069678,
      "grad_norm": 2.015625,
      "learning_rate": 3.9261744966442954e-05,
      "loss": 1.3636,
      "step": 10060
    },
    {
      "epoch": 1.6092133125574333,
      "grad_norm": 1.8828125,
      "learning_rate": 3.9101949504634074e-05,
      "loss": 0.9864,
      "step": 10070
    },
    {
      "epoch": 1.6108114587078988,
      "grad_norm": 4.15625,
      "learning_rate": 3.894215404282519e-05,
      "loss": 1.5113,
      "step": 10080
    },
    {
      "epoch": 1.6124096048583643,
      "grad_norm": 9.8125,
      "learning_rate": 3.87823585810163e-05,
      "loss": 1.3726,
      "step": 10090
    },
    {
      "epoch": 1.6140077510088298,
      "grad_norm": 1.859375,
      "learning_rate": 3.8622563119207416e-05,
      "loss": 1.228,
      "step": 10100
    },
    {
      "epoch": 1.6156058971592953,
      "grad_norm": 2.5625,
      "learning_rate": 3.846276765739853e-05,
      "loss": 1.2358,
      "step": 10110
    },
    {
      "epoch": 1.6172040433097608,
      "grad_norm": 1.5,
      "learning_rate": 3.830297219558965e-05,
      "loss": 1.6425,
      "step": 10120
    },
    {
      "epoch": 1.6188021894602262,
      "grad_norm": 2.0625,
      "learning_rate": 3.814317673378076e-05,
      "loss": 1.1236,
      "step": 10130
    },
    {
      "epoch": 1.6204003356106917,
      "grad_norm": 3.71875,
      "learning_rate": 3.798338127197188e-05,
      "loss": 1.007,
      "step": 10140
    },
    {
      "epoch": 1.621998481761157,
      "grad_norm": 2.640625,
      "learning_rate": 3.782358581016299e-05,
      "loss": 1.2844,
      "step": 10150
    },
    {
      "epoch": 1.6235966279116225,
      "grad_norm": 1.6171875,
      "learning_rate": 3.7663790348354113e-05,
      "loss": 1.5217,
      "step": 10160
    },
    {
      "epoch": 1.625194774062088,
      "grad_norm": 1.953125,
      "learning_rate": 3.750399488654523e-05,
      "loss": 1.1156,
      "step": 10170
    },
    {
      "epoch": 1.6267929202125533,
      "grad_norm": 2.171875,
      "learning_rate": 3.7344199424736334e-05,
      "loss": 1.4233,
      "step": 10180
    },
    {
      "epoch": 1.6283910663630188,
      "grad_norm": 2.65625,
      "learning_rate": 3.7184403962927455e-05,
      "loss": 1.2693,
      "step": 10190
    },
    {
      "epoch": 1.6299892125134843,
      "grad_norm": 2.109375,
      "learning_rate": 3.702460850111857e-05,
      "loss": 1.3342,
      "step": 10200
    },
    {
      "epoch": 1.6315873586639498,
      "grad_norm": 2.875,
      "learning_rate": 3.686481303930969e-05,
      "loss": 1.4448,
      "step": 10210
    },
    {
      "epoch": 1.6331855048144153,
      "grad_norm": 2.671875,
      "learning_rate": 3.67050175775008e-05,
      "loss": 1.3873,
      "step": 10220
    },
    {
      "epoch": 1.6347836509648808,
      "grad_norm": 1.8359375,
      "learning_rate": 3.654522211569192e-05,
      "loss": 1.311,
      "step": 10230
    },
    {
      "epoch": 1.6363817971153463,
      "grad_norm": 3.15625,
      "learning_rate": 3.638542665388303e-05,
      "loss": 1.5095,
      "step": 10240
    },
    {
      "epoch": 1.6379799432658118,
      "grad_norm": 3.6875,
      "learning_rate": 3.6225631192074146e-05,
      "loss": 1.2896,
      "step": 10250
    },
    {
      "epoch": 1.6395780894162773,
      "grad_norm": 1.9921875,
      "learning_rate": 3.606583573026526e-05,
      "loss": 1.245,
      "step": 10260
    },
    {
      "epoch": 1.6411762355667425,
      "grad_norm": 1.875,
      "learning_rate": 3.5906040268456373e-05,
      "loss": 1.3933,
      "step": 10270
    },
    {
      "epoch": 1.642774381717208,
      "grad_norm": 2.546875,
      "learning_rate": 3.5746244806647494e-05,
      "loss": 1.3776,
      "step": 10280
    },
    {
      "epoch": 1.6443725278676735,
      "grad_norm": 1.1640625,
      "learning_rate": 3.558644934483861e-05,
      "loss": 1.2239,
      "step": 10290
    },
    {
      "epoch": 1.645970674018139,
      "grad_norm": 2.078125,
      "learning_rate": 3.542665388302973e-05,
      "loss": 1.4168,
      "step": 10300
    },
    {
      "epoch": 1.6475688201686043,
      "grad_norm": 2.21875,
      "learning_rate": 3.5266858421220836e-05,
      "loss": 1.2524,
      "step": 10310
    },
    {
      "epoch": 1.6491669663190698,
      "grad_norm": 4.8125,
      "learning_rate": 3.510706295941196e-05,
      "loss": 1.101,
      "step": 10320
    },
    {
      "epoch": 1.6507651124695353,
      "grad_norm": 1.4140625,
      "learning_rate": 3.494726749760307e-05,
      "loss": 1.0732,
      "step": 10330
    },
    {
      "epoch": 1.6523632586200008,
      "grad_norm": 2.609375,
      "learning_rate": 3.4787472035794185e-05,
      "loss": 1.6432,
      "step": 10340
    },
    {
      "epoch": 1.6539614047704663,
      "grad_norm": 9.441375732421875e-05,
      "learning_rate": 3.46276765739853e-05,
      "loss": 1.1726,
      "step": 10350
    },
    {
      "epoch": 1.6555595509209318,
      "grad_norm": 1.8515625,
      "learning_rate": 3.446788111217641e-05,
      "loss": 1.4694,
      "step": 10360
    },
    {
      "epoch": 1.6571576970713973,
      "grad_norm": 2.453125,
      "learning_rate": 3.430808565036753e-05,
      "loss": 0.9163,
      "step": 10370
    },
    {
      "epoch": 1.6587558432218628,
      "grad_norm": 2.1875,
      "learning_rate": 3.414829018855865e-05,
      "loss": 1.7272,
      "step": 10380
    },
    {
      "epoch": 1.6603539893723283,
      "grad_norm": 3.140625,
      "learning_rate": 3.398849472674977e-05,
      "loss": 1.4244,
      "step": 10390
    },
    {
      "epoch": 1.6619521355227935,
      "grad_norm": 1.7578125,
      "learning_rate": 3.3828699264940875e-05,
      "loss": 1.1878,
      "step": 10400
    },
    {
      "epoch": 1.663550281673259,
      "grad_norm": 2.125,
      "learning_rate": 3.366890380313199e-05,
      "loss": 1.0944,
      "step": 10410
    },
    {
      "epoch": 1.6651484278237245,
      "grad_norm": 2.1875,
      "learning_rate": 3.350910834132311e-05,
      "loss": 1.197,
      "step": 10420
    },
    {
      "epoch": 1.6667465739741898,
      "grad_norm": 1.6796875,
      "learning_rate": 3.3349312879514224e-05,
      "loss": 0.9888,
      "step": 10430
    },
    {
      "epoch": 1.6683447201246553,
      "grad_norm": 7.25,
      "learning_rate": 3.318951741770534e-05,
      "loss": 1.1141,
      "step": 10440
    },
    {
      "epoch": 1.6699428662751208,
      "grad_norm": 1.625,
      "learning_rate": 3.302972195589645e-05,
      "loss": 1.3019,
      "step": 10450
    },
    {
      "epoch": 1.6715410124255863,
      "grad_norm": 1.6875,
      "learning_rate": 3.286992649408757e-05,
      "loss": 1.103,
      "step": 10460
    },
    {
      "epoch": 1.6731391585760518,
      "grad_norm": 2.15625,
      "learning_rate": 3.2710131032278686e-05,
      "loss": 1.5392,
      "step": 10470
    },
    {
      "epoch": 1.6747373047265173,
      "grad_norm": 1.5703125,
      "learning_rate": 3.25503355704698e-05,
      "loss": 1.4865,
      "step": 10480
    },
    {
      "epoch": 1.6763354508769828,
      "grad_norm": 6.15625,
      "learning_rate": 3.2390540108660914e-05,
      "loss": 1.1712,
      "step": 10490
    },
    {
      "epoch": 1.6779335970274483,
      "grad_norm": 1.0546875,
      "learning_rate": 3.223074464685203e-05,
      "loss": 1.2178,
      "step": 10500
    },
    {
      "epoch": 1.6795317431779138,
      "grad_norm": 1.3828125,
      "learning_rate": 3.207094918504315e-05,
      "loss": 1.2446,
      "step": 10510
    },
    {
      "epoch": 1.681129889328379,
      "grad_norm": 1.65625,
      "learning_rate": 3.191115372323426e-05,
      "loss": 1.1532,
      "step": 10520
    },
    {
      "epoch": 1.6827280354788445,
      "grad_norm": 1.1640625,
      "learning_rate": 3.1751358261425376e-05,
      "loss": 1.6389,
      "step": 10530
    },
    {
      "epoch": 1.68432618162931,
      "grad_norm": 2.296875,
      "learning_rate": 3.159156279961649e-05,
      "loss": 1.229,
      "step": 10540
    },
    {
      "epoch": 1.6859243277797753,
      "grad_norm": 2.40625,
      "learning_rate": 3.143176733780761e-05,
      "loss": 1.4964,
      "step": 10550
    },
    {
      "epoch": 1.6875224739302408,
      "grad_norm": 1.515625,
      "learning_rate": 3.1271971875998725e-05,
      "loss": 1.1419,
      "step": 10560
    },
    {
      "epoch": 1.6891206200807063,
      "grad_norm": 1.9765625,
      "learning_rate": 3.111217641418984e-05,
      "loss": 1.1819,
      "step": 10570
    },
    {
      "epoch": 1.6907187662311718,
      "grad_norm": 1.328125,
      "learning_rate": 3.095238095238095e-05,
      "loss": 0.8475,
      "step": 10580
    },
    {
      "epoch": 1.6923169123816373,
      "grad_norm": 1.3125,
      "learning_rate": 3.079258549057207e-05,
      "loss": 1.4528,
      "step": 10590
    },
    {
      "epoch": 1.6939150585321028,
      "grad_norm": 1.4609375,
      "learning_rate": 3.063279002876319e-05,
      "loss": 0.9576,
      "step": 10600
    },
    {
      "epoch": 1.6955132046825683,
      "grad_norm": 2.578125,
      "learning_rate": 3.0472994566954298e-05,
      "loss": 1.2461,
      "step": 10610
    },
    {
      "epoch": 1.6971113508330338,
      "grad_norm": 4.09375,
      "learning_rate": 3.031319910514542e-05,
      "loss": 1.4144,
      "step": 10620
    },
    {
      "epoch": 1.6987094969834993,
      "grad_norm": 1.09375,
      "learning_rate": 3.015340364333653e-05,
      "loss": 0.9315,
      "step": 10630
    },
    {
      "epoch": 1.7003076431339648,
      "grad_norm": 1.7109375,
      "learning_rate": 2.9993608181527643e-05,
      "loss": 1.1243,
      "step": 10640
    },
    {
      "epoch": 1.70190578928443,
      "grad_norm": 2.59375,
      "learning_rate": 2.983381271971876e-05,
      "loss": 1.1395,
      "step": 10650
    },
    {
      "epoch": 1.7035039354348955,
      "grad_norm": 1.3046875,
      "learning_rate": 2.9674017257909875e-05,
      "loss": 1.1562,
      "step": 10660
    },
    {
      "epoch": 1.705102081585361,
      "grad_norm": 4.625,
      "learning_rate": 2.9514221796100995e-05,
      "loss": 1.5681,
      "step": 10670
    },
    {
      "epoch": 1.7067002277358263,
      "grad_norm": 4.3125,
      "learning_rate": 2.9354426334292106e-05,
      "loss": 1.1852,
      "step": 10680
    },
    {
      "epoch": 1.7082983738862918,
      "grad_norm": 1.2578125,
      "learning_rate": 2.9194630872483227e-05,
      "loss": 1.0175,
      "step": 10690
    },
    {
      "epoch": 1.7098965200367573,
      "grad_norm": 0.20703125,
      "learning_rate": 2.9034835410674337e-05,
      "loss": 1.0576,
      "step": 10700
    },
    {
      "epoch": 1.7114946661872228,
      "grad_norm": 2.875,
      "learning_rate": 2.887503994886545e-05,
      "loss": 1.3383,
      "step": 10710
    },
    {
      "epoch": 1.7130928123376883,
      "grad_norm": 1.5859375,
      "learning_rate": 2.871524448705657e-05,
      "loss": 1.287,
      "step": 10720
    },
    {
      "epoch": 1.7146909584881538,
      "grad_norm": 1.203125,
      "learning_rate": 2.8555449025247682e-05,
      "loss": 1.1722,
      "step": 10730
    },
    {
      "epoch": 1.7162891046386193,
      "grad_norm": 2.921875,
      "learning_rate": 2.83956535634388e-05,
      "loss": 1.1195,
      "step": 10740
    },
    {
      "epoch": 1.7178872507890848,
      "grad_norm": 3.015625,
      "learning_rate": 2.8235858101629914e-05,
      "loss": 1.4291,
      "step": 10750
    },
    {
      "epoch": 1.7194853969395503,
      "grad_norm": 1.3984375,
      "learning_rate": 2.807606263982103e-05,
      "loss": 1.1331,
      "step": 10760
    },
    {
      "epoch": 1.7210835430900155,
      "grad_norm": 1.5,
      "learning_rate": 2.7916267178012145e-05,
      "loss": 1.4759,
      "step": 10770
    },
    {
      "epoch": 1.722681689240481,
      "grad_norm": 2.53125,
      "learning_rate": 2.7756471716203265e-05,
      "loss": 1.1911,
      "step": 10780
    },
    {
      "epoch": 1.7242798353909465,
      "grad_norm": 1.078125,
      "learning_rate": 2.7596676254394376e-05,
      "loss": 1.2858,
      "step": 10790
    },
    {
      "epoch": 1.7258779815414118,
      "grad_norm": 2.40625,
      "learning_rate": 2.743688079258549e-05,
      "loss": 1.7479,
      "step": 10800
    },
    {
      "epoch": 1.7274761276918773,
      "grad_norm": 1.7421875,
      "learning_rate": 2.7277085330776607e-05,
      "loss": 1.383,
      "step": 10810
    },
    {
      "epoch": 1.7290742738423428,
      "grad_norm": 6.771087646484375e-05,
      "learning_rate": 2.711728986896772e-05,
      "loss": 1.3411,
      "step": 10820
    },
    {
      "epoch": 1.7306724199928083,
      "grad_norm": 5.28125,
      "learning_rate": 2.695749440715884e-05,
      "loss": 1.0842,
      "step": 10830
    },
    {
      "epoch": 1.7322705661432738,
      "grad_norm": 2.296875,
      "learning_rate": 2.6797698945349952e-05,
      "loss": 0.8516,
      "step": 10840
    },
    {
      "epoch": 1.7338687122937393,
      "grad_norm": 2.46875,
      "learning_rate": 2.663790348354107e-05,
      "loss": 1.2575,
      "step": 10850
    },
    {
      "epoch": 1.7354668584442048,
      "grad_norm": 1.84375,
      "learning_rate": 2.6478108021732184e-05,
      "loss": 0.9714,
      "step": 10860
    },
    {
      "epoch": 1.7370650045946703,
      "grad_norm": 1.9453125,
      "learning_rate": 2.6318312559923298e-05,
      "loss": 1.4211,
      "step": 10870
    },
    {
      "epoch": 1.7386631507451358,
      "grad_norm": 2.703125,
      "learning_rate": 2.6158517098114415e-05,
      "loss": 1.1553,
      "step": 10880
    },
    {
      "epoch": 1.740261296895601,
      "grad_norm": 1.40625,
      "learning_rate": 2.599872163630553e-05,
      "loss": 1.036,
      "step": 10890
    },
    {
      "epoch": 1.7418594430460665,
      "grad_norm": 2.5625,
      "learning_rate": 2.5838926174496646e-05,
      "loss": 1.326,
      "step": 10900
    },
    {
      "epoch": 1.743457589196532,
      "grad_norm": 1.1875,
      "learning_rate": 2.567913071268776e-05,
      "loss": 1.2744,
      "step": 10910
    },
    {
      "epoch": 1.7450557353469973,
      "grad_norm": 3.265625,
      "learning_rate": 2.5519335250878878e-05,
      "loss": 1.3712,
      "step": 10920
    },
    {
      "epoch": 1.7466538814974628,
      "grad_norm": 1.765625,
      "learning_rate": 2.535953978906999e-05,
      "loss": 1.1836,
      "step": 10930
    },
    {
      "epoch": 1.7482520276479283,
      "grad_norm": 2.4375,
      "learning_rate": 2.5199744327261105e-05,
      "loss": 1.1237,
      "step": 10940
    },
    {
      "epoch": 1.7498501737983938,
      "grad_norm": 1.78125,
      "learning_rate": 2.5039948865452223e-05,
      "loss": 1.291,
      "step": 10950
    },
    {
      "epoch": 1.7514483199488593,
      "grad_norm": 3.4375,
      "learning_rate": 2.488015340364334e-05,
      "loss": 1.3941,
      "step": 10960
    },
    {
      "epoch": 1.7530464660993248,
      "grad_norm": 2.59375,
      "learning_rate": 2.4720357941834454e-05,
      "loss": 1.371,
      "step": 10970
    },
    {
      "epoch": 1.7546446122497903,
      "grad_norm": 1.6015625,
      "learning_rate": 2.4560562480025568e-05,
      "loss": 1.1686,
      "step": 10980
    },
    {
      "epoch": 1.7562427584002558,
      "grad_norm": 2.5,
      "learning_rate": 2.4400767018216682e-05,
      "loss": 1.0457,
      "step": 10990
    },
    {
      "epoch": 1.7578409045507213,
      "grad_norm": 3.671875,
      "learning_rate": 2.42409715564078e-05,
      "loss": 1.6821,
      "step": 11000
    },
    {
      "epoch": 1.7594390507011868,
      "grad_norm": 3.4375,
      "learning_rate": 2.4081176094598913e-05,
      "loss": 1.3349,
      "step": 11010
    },
    {
      "epoch": 1.761037196851652,
      "grad_norm": 1.890625,
      "learning_rate": 2.392138063279003e-05,
      "loss": 1.4008,
      "step": 11020
    },
    {
      "epoch": 1.7626353430021175,
      "grad_norm": 2.09375,
      "learning_rate": 2.3761585170981144e-05,
      "loss": 0.8364,
      "step": 11030
    },
    {
      "epoch": 1.764233489152583,
      "grad_norm": 3.15625,
      "learning_rate": 2.360178970917226e-05,
      "loss": 1.2621,
      "step": 11040
    },
    {
      "epoch": 1.7658316353030483,
      "grad_norm": 2.28125,
      "learning_rate": 2.3441994247363376e-05,
      "loss": 1.1315,
      "step": 11050
    },
    {
      "epoch": 1.7674297814535138,
      "grad_norm": 2.515625,
      "learning_rate": 2.328219878555449e-05,
      "loss": 1.2068,
      "step": 11060
    },
    {
      "epoch": 1.7690279276039793,
      "grad_norm": 3.734375,
      "learning_rate": 2.3122403323745607e-05,
      "loss": 1.3733,
      "step": 11070
    },
    {
      "epoch": 1.7706260737544448,
      "grad_norm": 1.625,
      "learning_rate": 2.296260786193672e-05,
      "loss": 1.4314,
      "step": 11080
    },
    {
      "epoch": 1.7722242199049103,
      "grad_norm": 2.75,
      "learning_rate": 2.2802812400127838e-05,
      "loss": 1.0119,
      "step": 11090
    },
    {
      "epoch": 1.7738223660553758,
      "grad_norm": 2.78125,
      "learning_rate": 2.2643016938318952e-05,
      "loss": 1.3018,
      "step": 11100
    },
    {
      "epoch": 1.7754205122058413,
      "grad_norm": 2.03125,
      "learning_rate": 2.248322147651007e-05,
      "loss": 1.1651,
      "step": 11110
    },
    {
      "epoch": 1.7770186583563068,
      "grad_norm": 1.8359375,
      "learning_rate": 2.2323426014701183e-05,
      "loss": 1.1732,
      "step": 11120
    },
    {
      "epoch": 1.7786168045067723,
      "grad_norm": 3.703125,
      "learning_rate": 2.2163630552892297e-05,
      "loss": 1.3934,
      "step": 11130
    },
    {
      "epoch": 1.7802149506572376,
      "grad_norm": 0.0001544952392578125,
      "learning_rate": 2.2003835091083415e-05,
      "loss": 1.3142,
      "step": 11140
    },
    {
      "epoch": 1.781813096807703,
      "grad_norm": 2.515625,
      "learning_rate": 2.184403962927453e-05,
      "loss": 1.4394,
      "step": 11150
    },
    {
      "epoch": 1.7834112429581686,
      "grad_norm": 1.703125,
      "learning_rate": 2.1684244167465646e-05,
      "loss": 1.2533,
      "step": 11160
    },
    {
      "epoch": 1.7850093891086338,
      "grad_norm": 2.4375,
      "learning_rate": 2.152444870565676e-05,
      "loss": 1.3737,
      "step": 11170
    },
    {
      "epoch": 1.7866075352590993,
      "grad_norm": 2.96875,
      "learning_rate": 2.1364653243847877e-05,
      "loss": 1.286,
      "step": 11180
    },
    {
      "epoch": 1.7882056814095648,
      "grad_norm": 2.25,
      "learning_rate": 2.120485778203899e-05,
      "loss": 1.5029,
      "step": 11190
    },
    {
      "epoch": 1.7898038275600303,
      "grad_norm": 1.4609375,
      "learning_rate": 2.104506232023011e-05,
      "loss": 1.1162,
      "step": 11200
    },
    {
      "epoch": 1.7914019737104958,
      "grad_norm": 2.984375,
      "learning_rate": 2.0885266858421222e-05,
      "loss": 1.2405,
      "step": 11210
    },
    {
      "epoch": 1.7930001198609613,
      "grad_norm": 1.8046875,
      "learning_rate": 2.0725471396612336e-05,
      "loss": 0.9812,
      "step": 11220
    },
    {
      "epoch": 1.7945982660114268,
      "grad_norm": 1.9453125,
      "learning_rate": 2.0565675934803454e-05,
      "loss": 1.4062,
      "step": 11230
    },
    {
      "epoch": 1.7961964121618923,
      "grad_norm": 1.953125,
      "learning_rate": 2.0405880472994567e-05,
      "loss": 1.3596,
      "step": 11240
    },
    {
      "epoch": 1.7977945583123578,
      "grad_norm": 1.75,
      "learning_rate": 2.0246085011185685e-05,
      "loss": 1.2806,
      "step": 11250
    },
    {
      "epoch": 1.7993927044628233,
      "grad_norm": 2.28125,
      "learning_rate": 2.00862895493768e-05,
      "loss": 1.436,
      "step": 11260
    },
    {
      "epoch": 1.8009908506132886,
      "grad_norm": 2.828125,
      "learning_rate": 1.9926494087567916e-05,
      "loss": 1.4267,
      "step": 11270
    },
    {
      "epoch": 1.802588996763754,
      "grad_norm": 3.078125,
      "learning_rate": 1.9766698625759027e-05,
      "loss": 1.3684,
      "step": 11280
    },
    {
      "epoch": 1.8041871429142196,
      "grad_norm": 1.5078125,
      "learning_rate": 1.9606903163950144e-05,
      "loss": 1.4075,
      "step": 11290
    },
    {
      "epoch": 1.8057852890646848,
      "grad_norm": 2.671875,
      "learning_rate": 1.9447107702141258e-05,
      "loss": 1.19,
      "step": 11300
    },
    {
      "epoch": 1.8073834352151503,
      "grad_norm": 1.3984375,
      "learning_rate": 1.9287312240332375e-05,
      "loss": 1.3933,
      "step": 11310
    },
    {
      "epoch": 1.8089815813656158,
      "grad_norm": 4.28125,
      "learning_rate": 1.9127516778523493e-05,
      "loss": 1.5171,
      "step": 11320
    },
    {
      "epoch": 1.8105797275160813,
      "grad_norm": 2.71875,
      "learning_rate": 1.8967721316714606e-05,
      "loss": 1.3336,
      "step": 11330
    },
    {
      "epoch": 1.8121778736665468,
      "grad_norm": 2.375,
      "learning_rate": 1.8807925854905724e-05,
      "loss": 0.9726,
      "step": 11340
    },
    {
      "epoch": 1.8137760198170123,
      "grad_norm": 2.25,
      "learning_rate": 1.8648130393096838e-05,
      "loss": 1.1984,
      "step": 11350
    },
    {
      "epoch": 1.8153741659674778,
      "grad_norm": 3.4375,
      "learning_rate": 1.848833493128795e-05,
      "loss": 0.8828,
      "step": 11360
    },
    {
      "epoch": 1.8169723121179433,
      "grad_norm": 2.578125,
      "learning_rate": 1.8328539469479066e-05,
      "loss": 1.2955,
      "step": 11370
    },
    {
      "epoch": 1.8185704582684088,
      "grad_norm": 1.421875,
      "learning_rate": 1.8168744007670183e-05,
      "loss": 1.2178,
      "step": 11380
    },
    {
      "epoch": 1.820168604418874,
      "grad_norm": 3.09375,
      "learning_rate": 1.8008948545861297e-05,
      "loss": 1.0556,
      "step": 11390
    },
    {
      "epoch": 1.8217667505693396,
      "grad_norm": 2.078125,
      "learning_rate": 1.7849153084052414e-05,
      "loss": 1.2247,
      "step": 11400
    },
    {
      "epoch": 1.823364896719805,
      "grad_norm": 1.5546875,
      "learning_rate": 1.7689357622243528e-05,
      "loss": 1.6733,
      "step": 11410
    },
    {
      "epoch": 1.8249630428702703,
      "grad_norm": 2.1875,
      "learning_rate": 1.7529562160434645e-05,
      "loss": 1.1223,
      "step": 11420
    },
    {
      "epoch": 1.8265611890207358,
      "grad_norm": 2.21875,
      "learning_rate": 1.7369766698625763e-05,
      "loss": 1.4123,
      "step": 11430
    },
    {
      "epoch": 1.8281593351712013,
      "grad_norm": 1.8046875,
      "learning_rate": 1.7209971236816873e-05,
      "loss": 1.2626,
      "step": 11440
    },
    {
      "epoch": 1.8297574813216668,
      "grad_norm": 1.3359375,
      "learning_rate": 1.705017577500799e-05,
      "loss": 1.1679,
      "step": 11450
    },
    {
      "epoch": 1.8313556274721323,
      "grad_norm": 2.078125,
      "learning_rate": 1.6890380313199105e-05,
      "loss": 1.0571,
      "step": 11460
    },
    {
      "epoch": 1.8329537736225978,
      "grad_norm": 1.1875,
      "learning_rate": 1.6730584851390222e-05,
      "loss": 1.1913,
      "step": 11470
    },
    {
      "epoch": 1.8345519197730633,
      "grad_norm": 1.7421875,
      "learning_rate": 1.6570789389581336e-05,
      "loss": 1.3623,
      "step": 11480
    },
    {
      "epoch": 1.8361500659235288,
      "grad_norm": 3.953125,
      "learning_rate": 1.6410993927772453e-05,
      "loss": 1.053,
      "step": 11490
    },
    {
      "epoch": 1.8377482120739943,
      "grad_norm": 4.15625,
      "learning_rate": 1.6251198465963567e-05,
      "loss": 1.2803,
      "step": 11500
    },
    {
      "epoch": 1.8393463582244596,
      "grad_norm": 2.296875,
      "learning_rate": 1.609140300415468e-05,
      "loss": 1.2347,
      "step": 11510
    },
    {
      "epoch": 1.840944504374925,
      "grad_norm": 2.109375,
      "learning_rate": 1.59316075423458e-05,
      "loss": 1.3274,
      "step": 11520
    },
    {
      "epoch": 1.8425426505253906,
      "grad_norm": 3.421875,
      "learning_rate": 1.5771812080536912e-05,
      "loss": 1.1374,
      "step": 11530
    },
    {
      "epoch": 1.8441407966758558,
      "grad_norm": 3.578125,
      "learning_rate": 1.561201661872803e-05,
      "loss": 1.2611,
      "step": 11540
    },
    {
      "epoch": 1.8457389428263213,
      "grad_norm": 1.640625,
      "learning_rate": 1.5452221156919144e-05,
      "loss": 1.2128,
      "step": 11550
    },
    {
      "epoch": 1.8473370889767868,
      "grad_norm": 1.390625,
      "learning_rate": 1.529242569511026e-05,
      "loss": 1.3822,
      "step": 11560
    },
    {
      "epoch": 1.8489352351272523,
      "grad_norm": 1.3828125,
      "learning_rate": 1.5132630233301376e-05,
      "loss": 1.1159,
      "step": 11570
    },
    {
      "epoch": 1.8505333812777178,
      "grad_norm": 2.546875,
      "learning_rate": 1.4972834771492492e-05,
      "loss": 1.1961,
      "step": 11580
    },
    {
      "epoch": 1.8521315274281833,
      "grad_norm": 3.140625,
      "learning_rate": 1.4813039309683604e-05,
      "loss": 0.9773,
      "step": 11590
    },
    {
      "epoch": 1.8537296735786488,
      "grad_norm": 3.609375,
      "learning_rate": 1.465324384787472e-05,
      "loss": 1.2019,
      "step": 11600
    },
    {
      "epoch": 1.8553278197291143,
      "grad_norm": 1.6796875,
      "learning_rate": 1.4493448386065836e-05,
      "loss": 1.5835,
      "step": 11610
    },
    {
      "epoch": 1.8569259658795798,
      "grad_norm": 1.8515625,
      "learning_rate": 1.4333652924256951e-05,
      "loss": 1.0124,
      "step": 11620
    },
    {
      "epoch": 1.8585241120300453,
      "grad_norm": 3.15625,
      "learning_rate": 1.4173857462448067e-05,
      "loss": 1.3098,
      "step": 11630
    },
    {
      "epoch": 1.8601222581805106,
      "grad_norm": 2.1875,
      "learning_rate": 1.4014062000639183e-05,
      "loss": 1.5541,
      "step": 11640
    },
    {
      "epoch": 1.861720404330976,
      "grad_norm": 1.46875,
      "learning_rate": 1.3854266538830298e-05,
      "loss": 1.0982,
      "step": 11650
    },
    {
      "epoch": 1.8633185504814416,
      "grad_norm": 2.640625,
      "learning_rate": 1.3694471077021415e-05,
      "loss": 0.8694,
      "step": 11660
    },
    {
      "epoch": 1.8649166966319068,
      "grad_norm": 1.7109375,
      "learning_rate": 1.3534675615212528e-05,
      "loss": 1.1647,
      "step": 11670
    },
    {
      "epoch": 1.8665148427823723,
      "grad_norm": 1.4296875,
      "learning_rate": 1.3374880153403643e-05,
      "loss": 1.3806,
      "step": 11680
    },
    {
      "epoch": 1.8681129889328378,
      "grad_norm": 1.78125,
      "learning_rate": 1.3215084691594759e-05,
      "loss": 1.3327,
      "step": 11690
    },
    {
      "epoch": 1.8697111350833033,
      "grad_norm": 1.6875,
      "learning_rate": 1.3055289229785875e-05,
      "loss": 1.3073,
      "step": 11700
    },
    {
      "epoch": 1.8713092812337688,
      "grad_norm": 3.53125,
      "learning_rate": 1.289549376797699e-05,
      "loss": 1.3956,
      "step": 11710
    },
    {
      "epoch": 1.8729074273842343,
      "grad_norm": 3.8125,
      "learning_rate": 1.2735698306168106e-05,
      "loss": 1.0507,
      "step": 11720
    },
    {
      "epoch": 1.8745055735346998,
      "grad_norm": 0.07177734375,
      "learning_rate": 1.2575902844359221e-05,
      "loss": 1.1772,
      "step": 11730
    },
    {
      "epoch": 1.8761037196851653,
      "grad_norm": 2.75,
      "learning_rate": 1.2416107382550337e-05,
      "loss": 1.2004,
      "step": 11740
    },
    {
      "epoch": 1.8777018658356308,
      "grad_norm": 0.0001468658447265625,
      "learning_rate": 1.2256311920741453e-05,
      "loss": 1.211,
      "step": 11750
    },
    {
      "epoch": 1.879300011986096,
      "grad_norm": 3.015625,
      "learning_rate": 1.2096516458932567e-05,
      "loss": 1.2556,
      "step": 11760
    },
    {
      "epoch": 1.8808981581365616,
      "grad_norm": 1.9140625,
      "learning_rate": 1.1936720997123682e-05,
      "loss": 1.2654,
      "step": 11770
    },
    {
      "epoch": 1.882496304287027,
      "grad_norm": 2.6875,
      "learning_rate": 1.1776925535314798e-05,
      "loss": 1.2279,
      "step": 11780
    },
    {
      "epoch": 1.8840944504374924,
      "grad_norm": 2.8125,
      "learning_rate": 1.1617130073505914e-05,
      "loss": 1.2493,
      "step": 11790
    },
    {
      "epoch": 1.8856925965879578,
      "grad_norm": 1.140625,
      "learning_rate": 1.1457334611697027e-05,
      "loss": 1.1918,
      "step": 11800
    },
    {
      "epoch": 1.8872907427384233,
      "grad_norm": 1.28125,
      "learning_rate": 1.1297539149888143e-05,
      "loss": 1.3786,
      "step": 11810
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 1.953125,
      "learning_rate": 1.1137743688079259e-05,
      "loss": 1.3839,
      "step": 11820
    },
    {
      "epoch": 1.8904870350393543,
      "grad_norm": 1.2109375,
      "learning_rate": 1.0977948226270374e-05,
      "loss": 1.3747,
      "step": 11830
    },
    {
      "epoch": 1.8920851811898198,
      "grad_norm": 1.203125,
      "learning_rate": 1.081815276446149e-05,
      "loss": 1.3482,
      "step": 11840
    },
    {
      "epoch": 1.8936833273402853,
      "grad_norm": 2.609375,
      "learning_rate": 1.0658357302652606e-05,
      "loss": 1.5123,
      "step": 11850
    },
    {
      "epoch": 1.8952814734907508,
      "grad_norm": 1.75,
      "learning_rate": 1.0498561840843721e-05,
      "loss": 1.5579,
      "step": 11860
    },
    {
      "epoch": 1.8968796196412163,
      "grad_norm": 2.984375,
      "learning_rate": 1.0338766379034837e-05,
      "loss": 1.5572,
      "step": 11870
    },
    {
      "epoch": 1.8984777657916818,
      "grad_norm": 1.75,
      "learning_rate": 1.017897091722595e-05,
      "loss": 1.1052,
      "step": 11880
    },
    {
      "epoch": 1.900075911942147,
      "grad_norm": 4.1484832763671875e-05,
      "learning_rate": 1.0019175455417066e-05,
      "loss": 1.5038,
      "step": 11890
    },
    {
      "epoch": 1.9016740580926126,
      "grad_norm": 1.8203125,
      "learning_rate": 9.859379993608182e-06,
      "loss": 1.3166,
      "step": 11900
    },
    {
      "epoch": 1.903272204243078,
      "grad_norm": 2.140625,
      "learning_rate": 9.699584531799296e-06,
      "loss": 1.3546,
      "step": 11910
    },
    {
      "epoch": 1.9048703503935434,
      "grad_norm": 1.8125,
      "learning_rate": 9.539789069990412e-06,
      "loss": 1.1047,
      "step": 11920
    },
    {
      "epoch": 1.9064684965440089,
      "grad_norm": 1.25,
      "learning_rate": 9.379993608181529e-06,
      "loss": 1.1289,
      "step": 11930
    },
    {
      "epoch": 1.9080666426944743,
      "grad_norm": 2.140625,
      "learning_rate": 9.220198146372645e-06,
      "loss": 1.1071,
      "step": 11940
    },
    {
      "epoch": 1.9096647888449398,
      "grad_norm": 1.5703125,
      "learning_rate": 9.060402684563759e-06,
      "loss": 1.3128,
      "step": 11950
    },
    {
      "epoch": 1.9112629349954053,
      "grad_norm": 1.6484375,
      "learning_rate": 8.900607222754874e-06,
      "loss": 1.2154,
      "step": 11960
    },
    {
      "epoch": 1.9128610811458708,
      "grad_norm": 1.046875,
      "learning_rate": 8.74081176094599e-06,
      "loss": 0.9509,
      "step": 11970
    },
    {
      "epoch": 1.9144592272963363,
      "grad_norm": 1.3828125,
      "learning_rate": 8.581016299137105e-06,
      "loss": 1.1789,
      "step": 11980
    },
    {
      "epoch": 1.9160573734468018,
      "grad_norm": 5.21875,
      "learning_rate": 8.42122083732822e-06,
      "loss": 1.4626,
      "step": 11990
    },
    {
      "epoch": 1.9176555195972673,
      "grad_norm": 2.09375,
      "learning_rate": 8.261425375519335e-06,
      "loss": 1.4971,
      "step": 12000
    },
    {
      "epoch": 1.9192536657477326,
      "grad_norm": 1.1328125,
      "learning_rate": 8.10162991371045e-06,
      "loss": 0.9313,
      "step": 12010
    },
    {
      "epoch": 1.920851811898198,
      "grad_norm": 2.609375,
      "learning_rate": 7.941834451901566e-06,
      "loss": 1.444,
      "step": 12020
    },
    {
      "epoch": 1.9224499580486636,
      "grad_norm": 1.140625,
      "learning_rate": 7.782038990092682e-06,
      "loss": 1.0516,
      "step": 12030
    },
    {
      "epoch": 1.9240481041991289,
      "grad_norm": 3.46875,
      "learning_rate": 7.622243528283797e-06,
      "loss": 1.34,
      "step": 12040
    },
    {
      "epoch": 1.9256462503495944,
      "grad_norm": 3.90625,
      "learning_rate": 7.462448066474913e-06,
      "loss": 1.3533,
      "step": 12050
    },
    {
      "epoch": 1.9272443965000599,
      "grad_norm": 1.9453125,
      "learning_rate": 7.302652604666029e-06,
      "loss": 1.0006,
      "step": 12060
    },
    {
      "epoch": 1.9288425426505253,
      "grad_norm": 2.546875,
      "learning_rate": 7.142857142857143e-06,
      "loss": 1.3067,
      "step": 12070
    },
    {
      "epoch": 1.9304406888009908,
      "grad_norm": 2.5,
      "learning_rate": 6.983061681048258e-06,
      "loss": 1.3986,
      "step": 12080
    },
    {
      "epoch": 1.9320388349514563,
      "grad_norm": 2.953125,
      "learning_rate": 6.823266219239374e-06,
      "loss": 1.1347,
      "step": 12090
    },
    {
      "epoch": 1.9336369811019218,
      "grad_norm": 2.359375,
      "learning_rate": 6.66347075743049e-06,
      "loss": 1.2546,
      "step": 12100
    },
    {
      "epoch": 1.9352351272523873,
      "grad_norm": 3.9375,
      "learning_rate": 6.503675295621604e-06,
      "loss": 1.482,
      "step": 12110
    },
    {
      "epoch": 1.9368332734028528,
      "grad_norm": 3.046875,
      "learning_rate": 6.34387983381272e-06,
      "loss": 0.9145,
      "step": 12120
    },
    {
      "epoch": 1.938431419553318,
      "grad_norm": 2.6875,
      "learning_rate": 6.184084372003835e-06,
      "loss": 1.2927,
      "step": 12130
    },
    {
      "epoch": 1.9400295657037836,
      "grad_norm": 1.71875,
      "learning_rate": 6.0242889101949504e-06,
      "loss": 1.5266,
      "step": 12140
    },
    {
      "epoch": 1.941627711854249,
      "grad_norm": 1.890625,
      "learning_rate": 5.864493448386066e-06,
      "loss": 0.8232,
      "step": 12150
    },
    {
      "epoch": 1.9432258580047144,
      "grad_norm": 1.7734375,
      "learning_rate": 5.704697986577182e-06,
      "loss": 1.5479,
      "step": 12160
    },
    {
      "epoch": 1.9448240041551799,
      "grad_norm": 3.504753112792969e-05,
      "learning_rate": 5.5449025247682965e-06,
      "loss": 1.0777,
      "step": 12170
    },
    {
      "epoch": 1.9464221503056454,
      "grad_norm": 1.15625,
      "learning_rate": 5.385107062959412e-06,
      "loss": 0.9321,
      "step": 12180
    },
    {
      "epoch": 1.9480202964561109,
      "grad_norm": 1.4296875,
      "learning_rate": 5.225311601150528e-06,
      "loss": 1.3025,
      "step": 12190
    },
    {
      "epoch": 1.9496184426065764,
      "grad_norm": 1.0625,
      "learning_rate": 5.065516139341643e-06,
      "loss": 1.1636,
      "step": 12200
    },
    {
      "epoch": 1.9512165887570418,
      "grad_norm": 3.4375,
      "learning_rate": 4.905720677532758e-06,
      "loss": 1.1652,
      "step": 12210
    },
    {
      "epoch": 1.9528147349075073,
      "grad_norm": 2.59375,
      "learning_rate": 4.745925215723874e-06,
      "loss": 1.54,
      "step": 12220
    },
    {
      "epoch": 1.9544128810579728,
      "grad_norm": 1.3125,
      "learning_rate": 4.5861297539149886e-06,
      "loss": 1.2639,
      "step": 12230
    },
    {
      "epoch": 1.9560110272084383,
      "grad_norm": 1.8359375,
      "learning_rate": 4.426334292106105e-06,
      "loss": 1.2979,
      "step": 12240
    },
    {
      "epoch": 1.9576091733589038,
      "grad_norm": 6.4375,
      "learning_rate": 4.26653883029722e-06,
      "loss": 1.5002,
      "step": 12250
    },
    {
      "epoch": 1.959207319509369,
      "grad_norm": 5.03125,
      "learning_rate": 4.1067433684883354e-06,
      "loss": 1.2916,
      "step": 12260
    },
    {
      "epoch": 1.9608054656598346,
      "grad_norm": 2.1875,
      "learning_rate": 3.94694790667945e-06,
      "loss": 1.0228,
      "step": 12270
    },
    {
      "epoch": 1.9624036118103,
      "grad_norm": 2.265625,
      "learning_rate": 3.7871524448705663e-06,
      "loss": 1.1321,
      "step": 12280
    },
    {
      "epoch": 1.9640017579607654,
      "grad_norm": 1.765625,
      "learning_rate": 3.627356983061681e-06,
      "loss": 1.0916,
      "step": 12290
    },
    {
      "epoch": 1.9655999041112309,
      "grad_norm": 1.6796875,
      "learning_rate": 3.467561521252797e-06,
      "loss": 1.3617,
      "step": 12300
    },
    {
      "epoch": 1.9671980502616964,
      "grad_norm": 1.234375,
      "learning_rate": 3.307766059443912e-06,
      "loss": 1.641,
      "step": 12310
    },
    {
      "epoch": 1.9687961964121619,
      "grad_norm": 3.578125,
      "learning_rate": 3.1479705976350275e-06,
      "loss": 1.1123,
      "step": 12320
    },
    {
      "epoch": 1.9703943425626274,
      "grad_norm": 3.671875,
      "learning_rate": 2.9881751358261427e-06,
      "loss": 1.5086,
      "step": 12330
    },
    {
      "epoch": 1.9719924887130929,
      "grad_norm": 3.53125,
      "learning_rate": 2.828379674017258e-06,
      "loss": 0.9645,
      "step": 12340
    },
    {
      "epoch": 1.9735906348635583,
      "grad_norm": 1.1328125,
      "learning_rate": 2.6685842122083736e-06,
      "loss": 1.0802,
      "step": 12350
    },
    {
      "epoch": 1.9751887810140238,
      "grad_norm": 2.71875,
      "learning_rate": 2.5087887503994888e-06,
      "loss": 1.481,
      "step": 12360
    },
    {
      "epoch": 1.9767869271644893,
      "grad_norm": 2.765625,
      "learning_rate": 2.3489932885906044e-06,
      "loss": 1.3875,
      "step": 12370
    },
    {
      "epoch": 1.9783850733149546,
      "grad_norm": 2.4375,
      "learning_rate": 2.1891978267817196e-06,
      "loss": 0.9654,
      "step": 12380
    },
    {
      "epoch": 1.97998321946542,
      "grad_norm": 2.984375,
      "learning_rate": 2.029402364972835e-06,
      "loss": 0.9501,
      "step": 12390
    },
    {
      "epoch": 1.9815813656158856,
      "grad_norm": 1.8984375,
      "learning_rate": 1.8696069031639504e-06,
      "loss": 1.0709,
      "step": 12400
    },
    {
      "epoch": 1.9831795117663509,
      "grad_norm": 2.78125,
      "learning_rate": 1.7098114413550656e-06,
      "loss": 1.4577,
      "step": 12410
    },
    {
      "epoch": 1.9847776579168164,
      "grad_norm": 2.625,
      "learning_rate": 1.5500159795461809e-06,
      "loss": 1.543,
      "step": 12420
    },
    {
      "epoch": 1.9863758040672819,
      "grad_norm": 1.765625,
      "learning_rate": 1.3902205177372963e-06,
      "loss": 1.1816,
      "step": 12430
    },
    {
      "epoch": 1.9879739502177474,
      "grad_norm": 2.0,
      "learning_rate": 1.2304250559284117e-06,
      "loss": 1.1529,
      "step": 12440
    },
    {
      "epoch": 1.9895720963682129,
      "grad_norm": 2.046875,
      "learning_rate": 1.070629594119527e-06,
      "loss": 1.2873,
      "step": 12450
    },
    {
      "epoch": 1.9911702425186784,
      "grad_norm": 1.21875,
      "learning_rate": 9.108341323106425e-07,
      "loss": 1.3247,
      "step": 12460
    },
    {
      "epoch": 1.9927683886691439,
      "grad_norm": 1.5078125,
      "learning_rate": 7.510386705017577e-07,
      "loss": 0.8451,
      "step": 12470
    },
    {
      "epoch": 1.9943665348196093,
      "grad_norm": 1.8671875,
      "learning_rate": 5.912432086928731e-07,
      "loss": 1.2629,
      "step": 12480
    },
    {
      "epoch": 1.9959646809700748,
      "grad_norm": 3.671875,
      "learning_rate": 4.314477468839885e-07,
      "loss": 1.2921,
      "step": 12490
    },
    {
      "epoch": 1.9975628271205401,
      "grad_norm": 2.3125,
      "learning_rate": 2.716522850751039e-07,
      "loss": 1.3469,
      "step": 12500
    },
    {
      "epoch": 1.9991609732710056,
      "grad_norm": 0.140625,
      "learning_rate": 1.1185682326621924e-07,
      "loss": 1.1359,
      "step": 12510
    }
  ],
  "logging_steps": 10,
  "max_steps": 12516,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.0948488278474143e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
