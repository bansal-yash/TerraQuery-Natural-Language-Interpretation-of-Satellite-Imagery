nohup: ignoring input
Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu126 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
/15tb_scratch_data/spandan/interiit/qwen/bbscore_all.py:19: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.

Please restructure your imports with 'import unsloth' at the top of your file.
  from unsloth import FastVisionModel
ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
   Loading finetuned model on GPU 0: /home/spandan/scratch/interiit/qwen/models--unsloth--Qwen3-VL-8B-Instruct/snapshots/11d38e30f7b6dec7545b704d119dc6fbecbbd639
==((====))==  Unsloth 2025.11.4: Fast Qwen3_Vl patching. Transformers: 4.57.3.
   \\   /|    Tesla V100-SXM2-32GB. Num GPUs = 2. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.0+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.0
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:01<00:05,  1.76s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:03<00:03,  1.66s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:04<00:01,  1.63s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:05<00:00,  1.35s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:05<00:00,  1.47s/it]
   Loading LoRA adapters from: /home/spandan/scratch/interiit/qwen/benchmarking_checkpoints/VRS_20000_trained/final
   Merging LoRA adapters into base model...
   Loading base model on GPU 1
==((====))==  Unsloth 2025.11.4: Fast Qwen3_Vl patching. Transformers: 4.57.3.
   \\   /|    Tesla V100-SXM2-32GB. Num GPUs = 2. Max memory: 31.733 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.0+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.0
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:01<00:03,  1.12s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:02<00:02,  1.06s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:03<00:01,  1.03s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.14it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.06it/s]

ðŸ“„ Loading test data: /home/spandan/scratch/interiit/data/VRSBench/VRSBench_EVAL_Cap.json
âœ… Total samples: 9350

ðŸŽ¨ Generating answers in batches...

  0%|          | 0/293 [00:00<?, ?it/s]  0%|          | 1/293 [00:01<05:24,  1.11s/it]  1%|          | 3/293 [00:46<1:23:30, 17.28s/it]  1%|â–         | 4/293 [01:13<1:38:12, 20.39s/it]  5%|â–         | 14/293 [01:38<25:54,  5.57s/it]   9%|â–Š         | 25/293 [02:06<17:13,  3.86s/it] 10%|â–‰         | 28/293 [02:32<20:51,  4.72s/it] 11%|â–ˆ         | 32/293 [02:58<22:24,  5.15s/it] 14%|â–ˆâ–        | 41/293 [03:24<17:15,  4.11s/it] 20%|â–ˆâ–‰        | 58/293 [03:24<07:19,  1.87s/it] 20%|â–ˆâ–‰        | 58/293 [03:42<07:19,  1.87s/it] 20%|â–ˆâ–ˆ        | 59/293 [03:50<11:30,  2.95s/it] 26%|â–ˆâ–ˆâ–Œ       | 75/293 [04:16<08:15,  2.27s/it] 26%|â–ˆâ–ˆâ–Œ       | 76/293 [04:41<11:47,  3.26s/it] 31%|â–ˆâ–ˆâ–ˆ       | 90/293 [05:07<08:50,  2.61s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 92/293 [05:33<11:39,  3.48s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 93/293 [05:59<16:00,  4.80s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 108/293 [06:25<09:35,  3.11s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 112/293 [06:51<11:08,  3.70s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 129/293 [06:52<04:57,  1.82s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 132/293 [06:52<04:19,  1.61s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 137/293 [06:52<03:19,  1.28s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 139/293 [06:52<02:56,  1.15s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 145/293 [06:53<01:57,  1.26it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 148/293 [06:53<01:36,  1.50it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 148/293 [07:12<01:36,  1.50it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 154/293 [07:19<04:25,  1.91s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 164/293 [07:44<04:46,  2.22s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 165/293 [08:10<07:45,  3.64s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 170/293 [08:35<08:21,  4.07s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 187/293 [08:36<02:57,  1.67s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 187/293 [08:53<02:57,  1.67s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 190/293 [09:02<04:24,  2.57s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 207/293 [09:02<01:48,  1.27s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 207/293 [09:13<01:48,  1.27s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 208/293 [09:28<03:13,  2.27s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 220/293 [09:53<02:41,  2.21s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 230/293 [10:18<02:25,  2.30s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 237/293 [10:43<02:25,  2.61s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 238/293 [11:08<03:22,  3.69s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 250/293 [11:33<02:08,  2.98s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 267/293 [11:34<00:41,  1.58s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 273/293 [11:34<00:25,  1.28s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 277/293 [11:35<00:17,  1.09s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 283/293 [11:35<00:08,  1.20it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 286/293 [11:35<00:04,  1.40it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 286/293 [11:53<00:04,  1.40it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 288/293 [12:00<00:10,  2.19s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 293/293 [12:00<00:00,  2.46s/it]

ðŸ“Š Computing metrics (BERTScore / BERT-BLEU)...

â„¹ï¸ bert-score library not available or failed (No module named 'bert_score'). Only computing BERT-BLEU.
â„¹ï¸ bert-score library not available or failed (No module named 'bert_score'). Only computing BERT-BLEU.

ðŸ”¹ Fine-tuned BERT-BLEU: 0.9329990780130903
ðŸ”¸ Base       BERT-BLEU: 0.886226471431518

ðŸŽ‰ Done!

